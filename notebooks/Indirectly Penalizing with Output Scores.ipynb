{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39003,"status":"ok","timestamp":1651018196125,"user":{"displayName":"Neehal Tumma","userId":"16200375369847542065"},"user_tz":240},"id":"Ny9HWwoxSM6i","outputId":"9b182a02-60b9-4a89-9f77-01f17ae5e145"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"4ajyvrD7R6Aj","executionInfo":{"status":"ok","timestamp":1651018230382,"user_tz":240,"elapsed":309,"user":{"displayName":"Neehal Tumma","userId":"16200375369847542065"}}},"outputs":[],"source":["import os\n","os.environ['HOME_DIR'] = 'drive/MyDrive/hidden-networks'\n","# !pip install -r $HOME_DIR/requirements.txt\n","\n","import sys\n","sys.path.append(os.path.join('/content', os.environ['HOME_DIR']))"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"pkngkPuoVh7E","executionInfo":{"status":"ok","timestamp":1651018239936,"user_tz":240,"elapsed":5904,"user":{"displayName":"Neehal Tumma","userId":"16200375369847542065"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","import torch.autograd as autograd\n","import collections\n","\n","from supermask_pruning import GetSubnet, SupermaskConv, SupermaskLinear\n","from supermask_pruning import train, test\n","\n","class ArgClass:\n","    def __init__(self, args):\n","        self.setattrs(**args)\n","        \n","    def setattrs(self, **kwargs):\n","        for name, val in kwargs.items():\n","            setattr(self, name, val)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"e6xNej7l34ov","executionInfo":{"status":"ok","timestamp":1651018316922,"user_tz":240,"elapsed":370,"user":{"displayName":"Neehal Tumma","userId":"16200375369847542065"}}},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self, args, input_channels, image_size, num_labels):\n","        super().__init__()\n","        \n","        sparsities = getattr(args, \"sparsity\", [{\"sparsity\": 1.0}, {\"sparsity\": 1.0}, {\"sparsity\": 1.0}, {\"sparsity\": 1.0}, {\"sparsity\": 1.0}])\n","        self.conv1 = SupermaskConv(input_channels, 64, 3, 1, bias=args.bias, init=args.init, **sparsities[0])\n","        self.conv2 = SupermaskConv(64, 64, 3, 1, bias=args.bias, init=args.init, **sparsities[1])\n","        s = (image_size - 4) * (image_size - 4) * 64 // 4\n","        self.fc1 = SupermaskLinear(s, 256, bias=args.bias, init=args.init, **sparsities[2])\n","        self.fc2 = SupermaskLinear(256, 256, bias=args.bias, init=args.init, **sparsities[3])\n","        self.fc3 = SupermaskLinear(256, num_labels, bias=args.bias, init=args.init, **sparsities[4])\n","        self.fc1.calculate_subscores = True\n","        self.args = args\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.conv2(x)\n","        x = F.max_pool2d(x, 2)\n","        x = torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x = self.fc2(x)\n","        x = F.relu(x)\n","        x = self.fc3(x)\n","        output = F.log_softmax(x, dim=1)\n","        return output\n","    \n","    def get_extra_state(self):\n","        return self.args\n","      \n","    def set_extra_state(self, state):\n","        self.args = state"]},{"cell_type":"code","source":["args = {\"init\": \"signed_constant\", \"bias\": False}\n","for i in Net(ArgClass(args), 3, 32, 10).children():\n","  print(i.weight)"],"metadata":{"id":"heFZ5JdfOLdZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651018320180,"user_tz":240,"elapsed":6,"user":{"displayName":"Neehal Tumma","userId":"16200375369847542065"}},"outputId":"21697ba8-d5e1-45de-fc2b-88ee9a59f7f7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[[[-0.1925,  0.1925, -0.1925],\n","          [ 0.1925,  0.1925,  0.1925],\n","          [-0.1925, -0.1925, -0.1925]],\n","\n","         [[-0.1925, -0.1925, -0.1925],\n","          [-0.1925, -0.1925,  0.1925],\n","          [-0.1925, -0.1925, -0.1925]],\n","\n","         [[-0.1925,  0.1925,  0.1925],\n","          [ 0.1925,  0.1925,  0.1925],\n","          [ 0.1925,  0.1925, -0.1925]]],\n","\n","\n","        [[[ 0.1925,  0.1925, -0.1925],\n","          [ 0.1925,  0.1925,  0.1925],\n","          [-0.1925, -0.1925, -0.1925]],\n","\n","         [[-0.1925,  0.1925, -0.1925],\n","          [-0.1925,  0.1925,  0.1925],\n","          [-0.1925,  0.1925,  0.1925]],\n","\n","         [[-0.1925, -0.1925, -0.1925],\n","          [-0.1925, -0.1925, -0.1925],\n","          [ 0.1925, -0.1925,  0.1925]]],\n","\n","\n","        [[[-0.1925,  0.1925, -0.1925],\n","          [-0.1925, -0.1925, -0.1925],\n","          [ 0.1925,  0.1925,  0.1925]],\n","\n","         [[-0.1925, -0.1925,  0.1925],\n","          [-0.1925,  0.1925,  0.1925],\n","          [ 0.1925, -0.1925, -0.1925]],\n","\n","         [[ 0.1925, -0.1925,  0.1925],\n","          [ 0.1925,  0.1925,  0.1925],\n","          [ 0.1925, -0.1925,  0.1925]]],\n","\n","\n","        ...,\n","\n","\n","        [[[-0.1925, -0.1925, -0.1925],\n","          [-0.1925, -0.1925, -0.1925],\n","          [ 0.1925,  0.1925,  0.1925]],\n","\n","         [[ 0.1925,  0.1925, -0.1925],\n","          [-0.1925,  0.1925,  0.1925],\n","          [-0.1925, -0.1925,  0.1925]],\n","\n","         [[ 0.1925, -0.1925,  0.1925],\n","          [ 0.1925, -0.1925, -0.1925],\n","          [-0.1925,  0.1925, -0.1925]]],\n","\n","\n","        [[[ 0.1925,  0.1925,  0.1925],\n","          [ 0.1925,  0.1925,  0.1925],\n","          [-0.1925,  0.1925,  0.1925]],\n","\n","         [[ 0.1925,  0.1925,  0.1925],\n","          [ 0.1925,  0.1925,  0.1925],\n","          [ 0.1925,  0.1925,  0.1925]],\n","\n","         [[-0.1925, -0.1925, -0.1925],\n","          [ 0.1925,  0.1925, -0.1925],\n","          [-0.1925,  0.1925, -0.1925]]],\n","\n","\n","        [[[ 0.1925,  0.1925,  0.1925],\n","          [-0.1925,  0.1925,  0.1925],\n","          [ 0.1925,  0.1925,  0.1925]],\n","\n","         [[-0.1925, -0.1925, -0.1925],\n","          [-0.1925,  0.1925,  0.1925],\n","          [-0.1925, -0.1925,  0.1925]],\n","\n","         [[ 0.1925, -0.1925,  0.1925],\n","          [ 0.1925,  0.1925, -0.1925],\n","          [-0.1925, -0.1925, -0.1925]]]])\n","Parameter containing:\n","tensor([[[[-0.0417,  0.0417, -0.0417],\n","          [-0.0417,  0.0417, -0.0417],\n","          [ 0.0417, -0.0417, -0.0417]],\n","\n","         [[-0.0417,  0.0417, -0.0417],\n","          [-0.0417,  0.0417,  0.0417],\n","          [-0.0417,  0.0417,  0.0417]],\n","\n","         [[-0.0417,  0.0417,  0.0417],\n","          [ 0.0417,  0.0417, -0.0417],\n","          [ 0.0417, -0.0417,  0.0417]],\n","\n","         ...,\n","\n","         [[ 0.0417, -0.0417, -0.0417],\n","          [-0.0417, -0.0417,  0.0417],\n","          [-0.0417, -0.0417, -0.0417]],\n","\n","         [[-0.0417, -0.0417,  0.0417],\n","          [-0.0417,  0.0417,  0.0417],\n","          [ 0.0417,  0.0417,  0.0417]],\n","\n","         [[-0.0417,  0.0417, -0.0417],\n","          [-0.0417,  0.0417,  0.0417],\n","          [-0.0417, -0.0417,  0.0417]]],\n","\n","\n","        [[[ 0.0417, -0.0417,  0.0417],\n","          [-0.0417, -0.0417, -0.0417],\n","          [-0.0417,  0.0417,  0.0417]],\n","\n","         [[ 0.0417, -0.0417,  0.0417],\n","          [ 0.0417,  0.0417,  0.0417],\n","          [-0.0417,  0.0417,  0.0417]],\n","\n","         [[ 0.0417,  0.0417,  0.0417],\n","          [-0.0417, -0.0417,  0.0417],\n","          [ 0.0417,  0.0417, -0.0417]],\n","\n","         ...,\n","\n","         [[ 0.0417, -0.0417, -0.0417],\n","          [ 0.0417,  0.0417, -0.0417],\n","          [-0.0417, -0.0417, -0.0417]],\n","\n","         [[-0.0417, -0.0417,  0.0417],\n","          [ 0.0417, -0.0417,  0.0417],\n","          [ 0.0417,  0.0417, -0.0417]],\n","\n","         [[-0.0417,  0.0417, -0.0417],\n","          [-0.0417,  0.0417,  0.0417],\n","          [ 0.0417,  0.0417, -0.0417]]],\n","\n","\n","        [[[ 0.0417,  0.0417, -0.0417],\n","          [-0.0417, -0.0417,  0.0417],\n","          [-0.0417,  0.0417,  0.0417]],\n","\n","         [[-0.0417,  0.0417, -0.0417],\n","          [ 0.0417,  0.0417, -0.0417],\n","          [ 0.0417, -0.0417,  0.0417]],\n","\n","         [[ 0.0417,  0.0417,  0.0417],\n","          [ 0.0417, -0.0417,  0.0417],\n","          [-0.0417, -0.0417, -0.0417]],\n","\n","         ...,\n","\n","         [[ 0.0417,  0.0417, -0.0417],\n","          [-0.0417, -0.0417, -0.0417],\n","          [-0.0417,  0.0417,  0.0417]],\n","\n","         [[ 0.0417,  0.0417, -0.0417],\n","          [ 0.0417, -0.0417, -0.0417],\n","          [-0.0417,  0.0417,  0.0417]],\n","\n","         [[ 0.0417,  0.0417,  0.0417],\n","          [ 0.0417, -0.0417, -0.0417],\n","          [ 0.0417,  0.0417, -0.0417]]],\n","\n","\n","        ...,\n","\n","\n","        [[[-0.0417,  0.0417,  0.0417],\n","          [ 0.0417,  0.0417, -0.0417],\n","          [ 0.0417, -0.0417,  0.0417]],\n","\n","         [[-0.0417,  0.0417, -0.0417],\n","          [-0.0417, -0.0417,  0.0417],\n","          [ 0.0417,  0.0417, -0.0417]],\n","\n","         [[-0.0417,  0.0417,  0.0417],\n","          [ 0.0417,  0.0417,  0.0417],\n","          [ 0.0417,  0.0417, -0.0417]],\n","\n","         ...,\n","\n","         [[ 0.0417,  0.0417, -0.0417],\n","          [-0.0417,  0.0417,  0.0417],\n","          [-0.0417, -0.0417, -0.0417]],\n","\n","         [[ 0.0417, -0.0417,  0.0417],\n","          [-0.0417,  0.0417, -0.0417],\n","          [ 0.0417, -0.0417, -0.0417]],\n","\n","         [[-0.0417, -0.0417,  0.0417],\n","          [-0.0417,  0.0417,  0.0417],\n","          [ 0.0417,  0.0417, -0.0417]]],\n","\n","\n","        [[[ 0.0417,  0.0417,  0.0417],\n","          [-0.0417, -0.0417, -0.0417],\n","          [-0.0417, -0.0417, -0.0417]],\n","\n","         [[-0.0417,  0.0417,  0.0417],\n","          [-0.0417,  0.0417, -0.0417],\n","          [-0.0417,  0.0417, -0.0417]],\n","\n","         [[ 0.0417, -0.0417, -0.0417],\n","          [-0.0417,  0.0417,  0.0417],\n","          [-0.0417,  0.0417, -0.0417]],\n","\n","         ...,\n","\n","         [[-0.0417, -0.0417,  0.0417],\n","          [ 0.0417, -0.0417, -0.0417],\n","          [-0.0417,  0.0417,  0.0417]],\n","\n","         [[ 0.0417,  0.0417, -0.0417],\n","          [ 0.0417, -0.0417,  0.0417],\n","          [ 0.0417, -0.0417, -0.0417]],\n","\n","         [[ 0.0417,  0.0417, -0.0417],\n","          [ 0.0417,  0.0417,  0.0417],\n","          [-0.0417,  0.0417, -0.0417]]],\n","\n","\n","        [[[-0.0417, -0.0417, -0.0417],\n","          [ 0.0417,  0.0417, -0.0417],\n","          [ 0.0417,  0.0417, -0.0417]],\n","\n","         [[-0.0417,  0.0417,  0.0417],\n","          [ 0.0417,  0.0417,  0.0417],\n","          [ 0.0417, -0.0417,  0.0417]],\n","\n","         [[ 0.0417,  0.0417, -0.0417],\n","          [ 0.0417, -0.0417,  0.0417],\n","          [-0.0417, -0.0417,  0.0417]],\n","\n","         ...,\n","\n","         [[-0.0417,  0.0417,  0.0417],\n","          [ 0.0417,  0.0417,  0.0417],\n","          [ 0.0417,  0.0417,  0.0417]],\n","\n","         [[-0.0417, -0.0417,  0.0417],\n","          [ 0.0417, -0.0417,  0.0417],\n","          [ 0.0417, -0.0417, -0.0417]],\n","\n","         [[ 0.0417,  0.0417,  0.0417],\n","          [ 0.0417,  0.0417,  0.0417],\n","          [ 0.0417,  0.0417, -0.0417]]]])\n","Parameter containing:\n","tensor([[-0.0089,  0.0089,  0.0089,  ...,  0.0089, -0.0089,  0.0089],\n","        [ 0.0089,  0.0089, -0.0089,  ..., -0.0089, -0.0089,  0.0089],\n","        [ 0.0089,  0.0089,  0.0089,  ...,  0.0089,  0.0089, -0.0089],\n","        ...,\n","        [ 0.0089, -0.0089, -0.0089,  ...,  0.0089, -0.0089,  0.0089],\n","        [ 0.0089, -0.0089, -0.0089,  ...,  0.0089,  0.0089,  0.0089],\n","        [ 0.0089, -0.0089,  0.0089,  ..., -0.0089, -0.0089, -0.0089]])\n","Parameter containing:\n","tensor([[-0.0625,  0.0625, -0.0625,  ..., -0.0625,  0.0625, -0.0625],\n","        [-0.0625,  0.0625,  0.0625,  ..., -0.0625,  0.0625, -0.0625],\n","        [ 0.0625, -0.0625,  0.0625,  ...,  0.0625,  0.0625,  0.0625],\n","        ...,\n","        [ 0.0625, -0.0625,  0.0625,  ..., -0.0625,  0.0625,  0.0625],\n","        [ 0.0625,  0.0625, -0.0625,  ..., -0.0625,  0.0625,  0.0625],\n","        [ 0.0625,  0.0625, -0.0625,  ...,  0.0625,  0.0625, -0.0625]])\n","Parameter containing:\n","tensor([[-0.0625,  0.0625,  0.0625,  ...,  0.0625,  0.0625,  0.0625],\n","        [ 0.0625, -0.0625,  0.0625,  ...,  0.0625,  0.0625, -0.0625],\n","        [ 0.0625,  0.0625,  0.0625,  ...,  0.0625, -0.0625, -0.0625],\n","        ...,\n","        [ 0.0625, -0.0625,  0.0625,  ..., -0.0625, -0.0625,  0.0625],\n","        [-0.0625, -0.0625, -0.0625,  ...,  0.0625, -0.0625, -0.0625],\n","        [ 0.0625,  0.0625, -0.0625,  ..., -0.0625,  0.0625,  0.0625]])\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"id":"YyHqlILxWd2m","executionInfo":{"status":"ok","timestamp":1651018322511,"user_tz":240,"elapsed":282,"user":{"displayName":"Neehal Tumma","userId":"16200375369847542065"}}},"outputs":[],"source":["# The main function runs the full training loop on a dataset of your choice\n","def main(model_args, train_args, base_model=None, trial=None):\n","    args = ArgClass(model_args)\n","    train_args = ArgClass(train_args)\n","    dataset = args.dataset\n","\n","    use_cuda = not args.no_cuda and torch.cuda.is_available()\n","\n","    torch.manual_seed(args.seed)\n","\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","    print(f\"Using device {device}\")\n","\n","    transform = None\n","    if dataset == \"MNIST\":\n","        transform = transforms.Compose([transforms.ToTensor(), \n","                                        transforms.Normalize((0.1307,), (0.3081,))\n","                                        ])\n","        train_transform = transform\n","        input_channels, image_size, num_labels = 1, 28, 10\n","    elif dataset == \"CIFAR10\":\n","        train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n","                                              transforms.RandomHorizontalFlip(),\n","                                              transforms.ToTensor(),\n","                                              transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","                                              ])\n","        transform = transforms.Compose([transforms.ToTensor(),\n","                                        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","                                        ])\n","        input_channels, image_size, num_labels = 3, 32, 10\n","    else:\n","        raise ValueError(\"Only supported datasets are CIFAR10 and MNIST currently.\")\n","\n","    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n","    train_loader = torch.utils.data.DataLoader(\n","        getattr(datasets, dataset)(os.path.join(train_args.data, dataset), \n","                                   train=True, download=True, transform=transform),\n","        batch_size=args.batch_size, shuffle=True, **kwargs)\n","    train_augmented_loader = torch.utils.data.DataLoader(\n","        getattr(datasets, dataset)(os.path.join(train_args.data, dataset), \n","                                   train=True, transform=train_transform),\n","        batch_size=args.batch_size, shuffle=True, **kwargs)\n","    test_loader = torch.utils.data.DataLoader(\n","        getattr(datasets, dataset)(os.path.join(train_args.data, dataset), \n","                                   train=False, transform=transform),\n","        batch_size=train_args.test_batch_size, shuffle=True, **kwargs)\n","\n","    model = Net(args, input_channels, image_size, num_labels).to(device)\n","\n","    if getattr(args, \"copy_layers\", None) is not None:\n","        if (bool(args.copy_layers) ^ (base_model is not None)):\n","            raise ValueError(\"copy_layers arg must be None or [] if base_model is not specified\")\n","        if base_model is not None and args.copy_layers:\n","            for layer in args.copy_layers:\n","                model.load_state_dict(getattr(base_model, layer).state_dict(prefix=f\"{layer}.\"), strict=False)\n","            \n","    # NOTE: only pass the parameters where p.requires_grad == True to the optimizer! Important!\n","    optimizer = getattr(optim, args.optimizer)(\n","        [p for p in model.parameters() if p.requires_grad],\n","        **args.optim_kwargs,\n","    )\n","    assert isinstance(args.epochs, list) or isinstance(args.epochs, int)\n","    num_epochs, check_freeze = (args.epochs, False) if isinstance(args.epochs, int) else (max(args.epochs), True)\n","    criterion = nn.CrossEntropyLoss().to(device)\n","    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs) if args.scheduler else None \n","\n","    for epoch in range(1, num_epochs + 1):\n","        if check_freeze:\n","            for freeze_at_epoch, child in zip(args.epochs, model.children()):\n","                if freeze_at_epoch == epoch - 1:\n","                    child.freeze()\n","                    print(f\"Freezing {child} before epoch {epoch}\")\n","   \n","        train(model, train_args.log_interval, device, train_augmented_loader, optimizer, criterion, epoch, penalty=model_args['score_penalty'])\n","        if (train_args.train_eval_interval and epoch % train_args.train_eval_interval == 0) or (train_args.eval_on_last and epoch == args.epochs):\n","            train_acc, train_loss = test(model, device, criterion, train_loader, name=\"Train\")\n","            if trial:\n","                trial.set_user_attr('train_acc', {**trial.user_attrs.get('train_acc', {}), **{epoch: train_acc}})\n","                trial.set_user_attr('train_loss', {**trial.user_attrs.get('train_loss', {}), **{epoch: train_loss}})\n","        if (train_args.test_eval_interval and epoch % train_args.test_eval_interval == 0) or (train_args.eval_on_last and epoch == args.epochs):\n","            test_acc, test_loss = test(model, device, criterion, test_loader, name=\"Test\")\n","            if trial:\n","                trial.set_user_attr('test_acc', {**trial.user_attrs.get('test_acc', {}), **{epoch: test_acc}})\n","                trial.set_user_attr('test_loss', {**trial.user_attrs.get('test_loss', {}), **{epoch: test_loss}})\n","                trial.report(test_acc, epoch-1)\n","                if trial.should_prune():\n","                    raise optuna.exceptions.TrialPruned()\n","\n","        if scheduler:\n","            scheduler.step()\n","\n","    if args.save_name is not None:\n","        torch.save(model.state_dict(), os.path.join(os.environ['HOME_DIR'], \\\n","                                                    \"trained_networks\", args.save_name))\n","    \n","    return model, device, train_loader, test_loader, criterion\n","\n","def get_prune_mask(layer, sparsity):\n","    with torch.no_grad():\n","        return GetSubnet.apply(layer.scores.abs(), sparsity)"]},{"cell_type":"code","source":["from updated_penalized_supermask_pruning import GetSubnet, SupermaskConv, SupermaskLinear\n","from updated_penalized_supermask_pruning import train, test\n","\n","# # Arguments that do not affect model at all\n","train_args = {\n","    \"test_batch_size\": 1000, # input batch size for testing (default: 1000)\n","    'data': '../data', # Location to store data (e.g. MNIST)\n","    'log_interval': 500, # how many batches to wait before logging training status\n","    'train_eval_interval': 10, # epoch interval at which to print training accuracy\n","    'test_eval_interval': 2, # epoch interval at which to print test accuracy\n","    'eval_on_last': True\n","}\n","\n","args = {\n","  \"dataset\": \"CIFAR10\",\n","  \"init\": \"signed_constant\",\n","  \"batch_size\": 16, # input batch size for training (default: 64)\n","  \"epochs\": [80, 100, 160, 140, 120], # number of epochs to train (default: 14)\n","  \"optimizer\": \"SGD\",\n","  \"optim_kwargs\": {\"lr\": 0.1, \"momentum\": 0.9, \"weight_decay\": 0.0001},\n","  \"scheduler\": True, # False for Adam, True for SGD, does CosineAnnealing\n","  'no_cuda': False, # disables CUDA training\n","  'seed': 1000, # random seed (default: 1)\n","  'save_name': None, #\"conv2_frozen_sp50_rs1000\", # \"simple20_rs2\", # For Saving the current Model, None if not saving\n","  'sparsity': [{\"sparsity\": 0.5}, {\"sparsity\": 0.5}, {\"sparsity\": 0.5}, {\"sparsity\": 0.5}, {\"sparsity\": 0.5}], # 'how sparse is each layer'\n","  'copy_layers': [], # ['conv1', 'conv2', 'fc2'],\n","  'bias': False, \n","  'score_penalty': 50\n","}\n","\n","trained_model, device, train_loader, test_loader, criterion = main(args, train_args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["6e0fcd12e3c7406fab5c94e8e16fe3f1","1f41d16d87ad44b19380725263868741","e66d728c4ab44177a28422d4cfdb950f","004074ae9b8441ad9242238e7c6fa9e7","801e604b329740049688bb77041d98ca","05bebc7b6cc444d99ee2976c530f0b39","83b6a3f0d5a045e4a4606d6fff88984f","40326802d5074a04b106b71e7d5bd956","22460777b3024231ac41759fa001e8b5","e0bcbbbe19f141dfb4ffa59bb397216b","41bdb3a5882f4a40b0268b594cee3127"]},"id":"SjiKxtEyEF3v","executionInfo":{"status":"ok","timestamp":1651023747449,"user_tz":240,"elapsed":5419370,"user":{"displayName":"Neehal Tumma","userId":"16200375369847542065"}},"outputId":"7681a400-01e9-4718-ad1d-0b9f8bbd5d54"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device cuda\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/CIFAR10/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e0fcd12e3c7406fab5c94e8e16fe3f1"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ../data/CIFAR10/cifar-10-python.tar.gz to ../data/CIFAR10\n","Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.295848\n","Train Epoch: 1 [8000/50000 (16%)]\tLoss: 1.975683\n","Train Epoch: 1 [16000/50000 (32%)]\tLoss: 1.955054\n","Train Epoch: 1 [24000/50000 (48%)]\tLoss: 1.636424\n","Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.634288\n","Train Epoch: 1 [40000/50000 (80%)]\tLoss: 1.550059\n","Train Epoch: 1 [48000/50000 (96%)]\tLoss: 1.147443\n","Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.338509\n","Train Epoch: 2 [8000/50000 (16%)]\tLoss: 1.039059\n","Train Epoch: 2 [16000/50000 (32%)]\tLoss: 1.431777\n","Train Epoch: 2 [24000/50000 (48%)]\tLoss: 1.306947\n","Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.402120\n","Train Epoch: 2 [40000/50000 (80%)]\tLoss: 1.447774\n","Train Epoch: 2 [48000/50000 (96%)]\tLoss: 1.827856\n","\n","Test set: Average loss: 0.0014, Accuracy: 5077/10000 (51%)\n","\n","Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.390622\n","Train Epoch: 3 [8000/50000 (16%)]\tLoss: 1.836106\n","Train Epoch: 3 [16000/50000 (32%)]\tLoss: 1.231730\n","Train Epoch: 3 [24000/50000 (48%)]\tLoss: 1.398725\n","Train Epoch: 3 [32000/50000 (64%)]\tLoss: 2.121230\n","Train Epoch: 3 [40000/50000 (80%)]\tLoss: 1.464935\n","Train Epoch: 3 [48000/50000 (96%)]\tLoss: 1.443257\n","Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.241855\n","Train Epoch: 4 [8000/50000 (16%)]\tLoss: 1.391883\n","Train Epoch: 4 [16000/50000 (32%)]\tLoss: 1.270690\n","Train Epoch: 4 [24000/50000 (48%)]\tLoss: 1.672654\n","Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1.139640\n","Train Epoch: 4 [40000/50000 (80%)]\tLoss: 1.797227\n","Train Epoch: 4 [48000/50000 (96%)]\tLoss: 1.478565\n","\n","Test set: Average loss: 0.0013, Accuracy: 5218/10000 (52%)\n","\n","Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.213873\n","Train Epoch: 5 [8000/50000 (16%)]\tLoss: 1.177782\n","Train Epoch: 5 [16000/50000 (32%)]\tLoss: 1.761026\n","Train Epoch: 5 [24000/50000 (48%)]\tLoss: 0.912246\n","Train Epoch: 5 [32000/50000 (64%)]\tLoss: 1.508153\n","Train Epoch: 5 [40000/50000 (80%)]\tLoss: 1.692925\n","Train Epoch: 5 [48000/50000 (96%)]\tLoss: 0.946915\n","Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.698298\n","Train Epoch: 6 [8000/50000 (16%)]\tLoss: 1.260043\n","Train Epoch: 6 [16000/50000 (32%)]\tLoss: 0.785882\n","Train Epoch: 6 [24000/50000 (48%)]\tLoss: 1.692097\n","Train Epoch: 6 [32000/50000 (64%)]\tLoss: 1.232509\n","Train Epoch: 6 [40000/50000 (80%)]\tLoss: 1.072366\n","Train Epoch: 6 [48000/50000 (96%)]\tLoss: 1.239127\n","\n","Test set: Average loss: 0.0012, Accuracy: 5891/10000 (59%)\n","\n","Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.865922\n","Train Epoch: 7 [8000/50000 (16%)]\tLoss: 1.018846\n","Train Epoch: 7 [16000/50000 (32%)]\tLoss: 1.049647\n","Train Epoch: 7 [24000/50000 (48%)]\tLoss: 1.351314\n","Train Epoch: 7 [32000/50000 (64%)]\tLoss: 1.059710\n","Train Epoch: 7 [40000/50000 (80%)]\tLoss: 1.311032\n","Train Epoch: 7 [48000/50000 (96%)]\tLoss: 0.918858\n","Train Epoch: 8 [0/50000 (0%)]\tLoss: 1.174440\n","Train Epoch: 8 [8000/50000 (16%)]\tLoss: 1.331227\n","Train Epoch: 8 [16000/50000 (32%)]\tLoss: 1.115188\n","Train Epoch: 8 [24000/50000 (48%)]\tLoss: 1.465578\n","Train Epoch: 8 [32000/50000 (64%)]\tLoss: 1.372842\n","Train Epoch: 8 [40000/50000 (80%)]\tLoss: 1.132851\n","Train Epoch: 8 [48000/50000 (96%)]\tLoss: 1.599317\n","\n","Test set: Average loss: 0.0012, Accuracy: 5796/10000 (58%)\n","\n","Train Epoch: 9 [0/50000 (0%)]\tLoss: 1.302686\n","Train Epoch: 9 [8000/50000 (16%)]\tLoss: 1.403914\n","Train Epoch: 9 [16000/50000 (32%)]\tLoss: 1.325061\n","Train Epoch: 9 [24000/50000 (48%)]\tLoss: 1.347865\n","Train Epoch: 9 [32000/50000 (64%)]\tLoss: 1.189363\n","Train Epoch: 9 [40000/50000 (80%)]\tLoss: 1.768802\n","Train Epoch: 9 [48000/50000 (96%)]\tLoss: 1.327208\n","Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.936178\n","Train Epoch: 10 [8000/50000 (16%)]\tLoss: 1.297600\n","Train Epoch: 10 [16000/50000 (32%)]\tLoss: 1.563115\n","Train Epoch: 10 [24000/50000 (48%)]\tLoss: 0.877680\n","Train Epoch: 10 [32000/50000 (64%)]\tLoss: 1.309158\n","Train Epoch: 10 [40000/50000 (80%)]\tLoss: 1.206814\n","Train Epoch: 10 [48000/50000 (96%)]\tLoss: 1.497980\n","\n","Train set: Average loss: 0.0759, Accuracy: 28887/50000 (58%)\n","\n","\n","Test set: Average loss: 0.0012, Accuracy: 5712/10000 (57%)\n","\n","Train Epoch: 11 [0/50000 (0%)]\tLoss: 1.232005\n","Train Epoch: 11 [8000/50000 (16%)]\tLoss: 1.507769\n","Train Epoch: 11 [16000/50000 (32%)]\tLoss: 0.974697\n","Train Epoch: 11 [24000/50000 (48%)]\tLoss: 1.378659\n","Train Epoch: 11 [32000/50000 (64%)]\tLoss: 1.603639\n","Train Epoch: 11 [40000/50000 (80%)]\tLoss: 1.467621\n","Train Epoch: 11 [48000/50000 (96%)]\tLoss: 0.923305\n","Train Epoch: 12 [0/50000 (0%)]\tLoss: 1.391238\n","Train Epoch: 12 [8000/50000 (16%)]\tLoss: 1.218742\n","Train Epoch: 12 [16000/50000 (32%)]\tLoss: 1.625601\n","Train Epoch: 12 [24000/50000 (48%)]\tLoss: 1.018958\n","Train Epoch: 12 [32000/50000 (64%)]\tLoss: 1.961470\n","Train Epoch: 12 [40000/50000 (80%)]\tLoss: 1.103118\n","Train Epoch: 12 [48000/50000 (96%)]\tLoss: 1.333014\n","\n","Test set: Average loss: 0.0013, Accuracy: 5317/10000 (53%)\n","\n","Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.842750\n","Train Epoch: 13 [8000/50000 (16%)]\tLoss: 0.904393\n","Train Epoch: 13 [16000/50000 (32%)]\tLoss: 1.482054\n","Train Epoch: 13 [24000/50000 (48%)]\tLoss: 1.437236\n","Train Epoch: 13 [32000/50000 (64%)]\tLoss: 1.078964\n","Train Epoch: 13 [40000/50000 (80%)]\tLoss: 1.375037\n","Train Epoch: 13 [48000/50000 (96%)]\tLoss: 1.499052\n","Train Epoch: 14 [0/50000 (0%)]\tLoss: 1.121644\n","Train Epoch: 14 [8000/50000 (16%)]\tLoss: 0.985677\n","Train Epoch: 14 [16000/50000 (32%)]\tLoss: 1.218104\n","Train Epoch: 14 [24000/50000 (48%)]\tLoss: 0.988027\n","Train Epoch: 14 [32000/50000 (64%)]\tLoss: 1.100771\n","Train Epoch: 14 [40000/50000 (80%)]\tLoss: 1.096007\n","Train Epoch: 14 [48000/50000 (96%)]\tLoss: 1.081486\n","\n","Test set: Average loss: 0.0012, Accuracy: 6010/10000 (60%)\n","\n","Train Epoch: 15 [0/50000 (0%)]\tLoss: 1.140487\n","Train Epoch: 15 [8000/50000 (16%)]\tLoss: 0.735539\n","Train Epoch: 15 [16000/50000 (32%)]\tLoss: 1.350210\n","Train Epoch: 15 [24000/50000 (48%)]\tLoss: 1.304556\n","Train Epoch: 15 [32000/50000 (64%)]\tLoss: 1.189706\n","Train Epoch: 15 [40000/50000 (80%)]\tLoss: 1.574212\n","Train Epoch: 15 [48000/50000 (96%)]\tLoss: 1.194987\n","Train Epoch: 16 [0/50000 (0%)]\tLoss: 1.046192\n","Train Epoch: 16 [8000/50000 (16%)]\tLoss: 1.472990\n","Train Epoch: 16 [16000/50000 (32%)]\tLoss: 1.198516\n","Train Epoch: 16 [24000/50000 (48%)]\tLoss: 1.324267\n","Train Epoch: 16 [32000/50000 (64%)]\tLoss: 2.364318\n","Train Epoch: 16 [40000/50000 (80%)]\tLoss: 1.229762\n","Train Epoch: 16 [48000/50000 (96%)]\tLoss: 1.566231\n","\n","Test set: Average loss: 0.0012, Accuracy: 5754/10000 (58%)\n","\n","Train Epoch: 17 [0/50000 (0%)]\tLoss: 1.134578\n","Train Epoch: 17 [8000/50000 (16%)]\tLoss: 1.405845\n","Train Epoch: 17 [16000/50000 (32%)]\tLoss: 1.418115\n","Train Epoch: 17 [24000/50000 (48%)]\tLoss: 0.966618\n","Train Epoch: 17 [32000/50000 (64%)]\tLoss: 1.239645\n","Train Epoch: 17 [40000/50000 (80%)]\tLoss: 0.986423\n","Train Epoch: 17 [48000/50000 (96%)]\tLoss: 1.516239\n","Train Epoch: 18 [0/50000 (0%)]\tLoss: 1.472794\n","Train Epoch: 18 [8000/50000 (16%)]\tLoss: 0.890533\n","Train Epoch: 18 [16000/50000 (32%)]\tLoss: 0.736391\n","Train Epoch: 18 [24000/50000 (48%)]\tLoss: 1.157594\n","Train Epoch: 18 [32000/50000 (64%)]\tLoss: 1.176343\n","Train Epoch: 18 [40000/50000 (80%)]\tLoss: 1.164335\n","Train Epoch: 18 [48000/50000 (96%)]\tLoss: 1.511300\n","\n","Test set: Average loss: 0.0012, Accuracy: 5687/10000 (57%)\n","\n","Train Epoch: 19 [0/50000 (0%)]\tLoss: 1.582810\n","Train Epoch: 19 [8000/50000 (16%)]\tLoss: 1.261412\n","Train Epoch: 19 [16000/50000 (32%)]\tLoss: 1.156675\n","Train Epoch: 19 [24000/50000 (48%)]\tLoss: 1.536033\n","Train Epoch: 19 [32000/50000 (64%)]\tLoss: 1.410463\n","Train Epoch: 19 [40000/50000 (80%)]\tLoss: 1.331481\n","Train Epoch: 19 [48000/50000 (96%)]\tLoss: 1.063874\n","Train Epoch: 20 [0/50000 (0%)]\tLoss: 1.564632\n","Train Epoch: 20 [8000/50000 (16%)]\tLoss: 1.113781\n","Train Epoch: 20 [16000/50000 (32%)]\tLoss: 1.263770\n","Train Epoch: 20 [24000/50000 (48%)]\tLoss: 1.247446\n","Train Epoch: 20 [32000/50000 (64%)]\tLoss: 1.514051\n","Train Epoch: 20 [40000/50000 (80%)]\tLoss: 1.111676\n","Train Epoch: 20 [48000/50000 (96%)]\tLoss: 1.051584\n","\n","Train set: Average loss: 0.0703, Accuracy: 30235/50000 (60%)\n","\n","\n","Test set: Average loss: 0.0012, Accuracy: 5943/10000 (59%)\n","\n","Train Epoch: 21 [0/50000 (0%)]\tLoss: 0.674974\n","Train Epoch: 21 [8000/50000 (16%)]\tLoss: 1.206202\n","Train Epoch: 21 [16000/50000 (32%)]\tLoss: 1.212409\n","Train Epoch: 21 [24000/50000 (48%)]\tLoss: 1.060133\n","Train Epoch: 21 [32000/50000 (64%)]\tLoss: 1.109913\n","Train Epoch: 21 [40000/50000 (80%)]\tLoss: 1.128958\n","Train Epoch: 21 [48000/50000 (96%)]\tLoss: 1.482975\n","Train Epoch: 22 [0/50000 (0%)]\tLoss: 0.969003\n","Train Epoch: 22 [8000/50000 (16%)]\tLoss: 0.764214\n","Train Epoch: 22 [16000/50000 (32%)]\tLoss: 1.253099\n","Train Epoch: 22 [24000/50000 (48%)]\tLoss: 1.603305\n","Train Epoch: 22 [32000/50000 (64%)]\tLoss: 1.543441\n","Train Epoch: 22 [40000/50000 (80%)]\tLoss: 1.277307\n","Train Epoch: 22 [48000/50000 (96%)]\tLoss: 0.997532\n","\n","Test set: Average loss: 0.0012, Accuracy: 5895/10000 (59%)\n","\n","Train Epoch: 23 [0/50000 (0%)]\tLoss: 1.189395\n","Train Epoch: 23 [8000/50000 (16%)]\tLoss: 1.270277\n","Train Epoch: 23 [16000/50000 (32%)]\tLoss: 1.333661\n","Train Epoch: 23 [24000/50000 (48%)]\tLoss: 1.004020\n","Train Epoch: 23 [32000/50000 (64%)]\tLoss: 1.975749\n","Train Epoch: 23 [40000/50000 (80%)]\tLoss: 1.361987\n","Train Epoch: 23 [48000/50000 (96%)]\tLoss: 1.274737\n","Train Epoch: 24 [0/50000 (0%)]\tLoss: 1.212975\n","Train Epoch: 24 [8000/50000 (16%)]\tLoss: 1.153415\n","Train Epoch: 24 [16000/50000 (32%)]\tLoss: 0.970992\n","Train Epoch: 24 [24000/50000 (48%)]\tLoss: 1.096118\n","Train Epoch: 24 [32000/50000 (64%)]\tLoss: 1.569931\n","Train Epoch: 24 [40000/50000 (80%)]\tLoss: 1.130532\n","Train Epoch: 24 [48000/50000 (96%)]\tLoss: 1.416397\n","\n","Test set: Average loss: 0.0011, Accuracy: 6118/10000 (61%)\n","\n","Train Epoch: 25 [0/50000 (0%)]\tLoss: 0.907508\n","Train Epoch: 25 [8000/50000 (16%)]\tLoss: 1.690780\n","Train Epoch: 25 [16000/50000 (32%)]\tLoss: 1.169611\n","Train Epoch: 25 [24000/50000 (48%)]\tLoss: 1.198062\n","Train Epoch: 25 [32000/50000 (64%)]\tLoss: 1.686397\n","Train Epoch: 25 [40000/50000 (80%)]\tLoss: 1.078626\n","Train Epoch: 25 [48000/50000 (96%)]\tLoss: 0.910783\n","Train Epoch: 26 [0/50000 (0%)]\tLoss: 1.292548\n","Train Epoch: 26 [8000/50000 (16%)]\tLoss: 1.139894\n","Train Epoch: 26 [16000/50000 (32%)]\tLoss: 1.322995\n","Train Epoch: 26 [24000/50000 (48%)]\tLoss: 1.731134\n","Train Epoch: 26 [32000/50000 (64%)]\tLoss: 1.000255\n","Train Epoch: 26 [40000/50000 (80%)]\tLoss: 1.571252\n","Train Epoch: 26 [48000/50000 (96%)]\tLoss: 0.940827\n","\n","Test set: Average loss: 0.0011, Accuracy: 6176/10000 (62%)\n","\n","Train Epoch: 27 [0/50000 (0%)]\tLoss: 1.497462\n","Train Epoch: 27 [8000/50000 (16%)]\tLoss: 0.894457\n","Train Epoch: 27 [16000/50000 (32%)]\tLoss: 1.240028\n","Train Epoch: 27 [24000/50000 (48%)]\tLoss: 0.953602\n","Train Epoch: 27 [32000/50000 (64%)]\tLoss: 1.765993\n","Train Epoch: 27 [40000/50000 (80%)]\tLoss: 1.167279\n","Train Epoch: 27 [48000/50000 (96%)]\tLoss: 0.698019\n","Train Epoch: 28 [0/50000 (0%)]\tLoss: 1.587544\n","Train Epoch: 28 [8000/50000 (16%)]\tLoss: 0.939041\n","Train Epoch: 28 [16000/50000 (32%)]\tLoss: 0.965208\n","Train Epoch: 28 [24000/50000 (48%)]\tLoss: 1.377162\n","Train Epoch: 28 [32000/50000 (64%)]\tLoss: 1.796041\n","Train Epoch: 28 [40000/50000 (80%)]\tLoss: 1.007553\n","Train Epoch: 28 [48000/50000 (96%)]\tLoss: 1.321931\n","\n","Test set: Average loss: 0.0013, Accuracy: 5509/10000 (55%)\n","\n","Train Epoch: 29 [0/50000 (0%)]\tLoss: 1.325314\n","Train Epoch: 29 [8000/50000 (16%)]\tLoss: 1.425121\n","Train Epoch: 29 [16000/50000 (32%)]\tLoss: 1.320329\n","Train Epoch: 29 [24000/50000 (48%)]\tLoss: 1.383711\n","Train Epoch: 29 [32000/50000 (64%)]\tLoss: 1.377710\n","Train Epoch: 29 [40000/50000 (80%)]\tLoss: 1.622947\n","Train Epoch: 29 [48000/50000 (96%)]\tLoss: 1.510673\n","Train Epoch: 30 [0/50000 (0%)]\tLoss: 1.139369\n","Train Epoch: 30 [8000/50000 (16%)]\tLoss: 1.080923\n","Train Epoch: 30 [16000/50000 (32%)]\tLoss: 1.434229\n","Train Epoch: 30 [24000/50000 (48%)]\tLoss: 1.392313\n","Train Epoch: 30 [32000/50000 (64%)]\tLoss: 1.281363\n","Train Epoch: 30 [40000/50000 (80%)]\tLoss: 1.226055\n","Train Epoch: 30 [48000/50000 (96%)]\tLoss: 1.281923\n","\n","Train set: Average loss: 0.0725, Accuracy: 29622/50000 (59%)\n","\n","\n","Test set: Average loss: 0.0012, Accuracy: 5912/10000 (59%)\n","\n","Train Epoch: 31 [0/50000 (0%)]\tLoss: 0.742505\n","Train Epoch: 31 [8000/50000 (16%)]\tLoss: 0.921462\n","Train Epoch: 31 [16000/50000 (32%)]\tLoss: 1.271243\n","Train Epoch: 31 [24000/50000 (48%)]\tLoss: 1.148704\n","Train Epoch: 31 [32000/50000 (64%)]\tLoss: 1.222538\n","Train Epoch: 31 [40000/50000 (80%)]\tLoss: 1.039707\n","Train Epoch: 31 [48000/50000 (96%)]\tLoss: 1.178772\n","Train Epoch: 32 [0/50000 (0%)]\tLoss: 0.686387\n","Train Epoch: 32 [8000/50000 (16%)]\tLoss: 1.357942\n","Train Epoch: 32 [16000/50000 (32%)]\tLoss: 0.882280\n","Train Epoch: 32 [24000/50000 (48%)]\tLoss: 1.105937\n","Train Epoch: 32 [32000/50000 (64%)]\tLoss: 0.972218\n","Train Epoch: 32 [40000/50000 (80%)]\tLoss: 1.411830\n","Train Epoch: 32 [48000/50000 (96%)]\tLoss: 1.454906\n","\n","Test set: Average loss: 0.0011, Accuracy: 6149/10000 (61%)\n","\n","Train Epoch: 33 [0/50000 (0%)]\tLoss: 1.204962\n","Train Epoch: 33 [8000/50000 (16%)]\tLoss: 1.762200\n","Train Epoch: 33 [16000/50000 (32%)]\tLoss: 1.712790\n","Train Epoch: 33 [24000/50000 (48%)]\tLoss: 1.445137\n","Train Epoch: 33 [32000/50000 (64%)]\tLoss: 1.207994\n","Train Epoch: 33 [40000/50000 (80%)]\tLoss: 1.136724\n","Train Epoch: 33 [48000/50000 (96%)]\tLoss: 1.127017\n","Train Epoch: 34 [0/50000 (0%)]\tLoss: 1.392017\n","Train Epoch: 34 [8000/50000 (16%)]\tLoss: 1.238667\n","Train Epoch: 34 [16000/50000 (32%)]\tLoss: 0.849132\n","Train Epoch: 34 [24000/50000 (48%)]\tLoss: 1.162164\n","Train Epoch: 34 [32000/50000 (64%)]\tLoss: 1.372132\n","Train Epoch: 34 [40000/50000 (80%)]\tLoss: 0.898865\n","Train Epoch: 34 [48000/50000 (96%)]\tLoss: 1.463261\n","\n","Test set: Average loss: 0.0012, Accuracy: 5979/10000 (60%)\n","\n","Train Epoch: 35 [0/50000 (0%)]\tLoss: 0.858966\n","Train Epoch: 35 [8000/50000 (16%)]\tLoss: 1.244367\n","Train Epoch: 35 [16000/50000 (32%)]\tLoss: 1.153873\n","Train Epoch: 35 [24000/50000 (48%)]\tLoss: 1.505779\n","Train Epoch: 35 [32000/50000 (64%)]\tLoss: 1.305508\n","Train Epoch: 35 [40000/50000 (80%)]\tLoss: 0.999147\n","Train Epoch: 35 [48000/50000 (96%)]\tLoss: 1.138538\n","Train Epoch: 36 [0/50000 (0%)]\tLoss: 1.106938\n","Train Epoch: 36 [8000/50000 (16%)]\tLoss: 0.954247\n","Train Epoch: 36 [16000/50000 (32%)]\tLoss: 1.299920\n","Train Epoch: 36 [24000/50000 (48%)]\tLoss: 0.977734\n","Train Epoch: 36 [32000/50000 (64%)]\tLoss: 1.107895\n","Train Epoch: 36 [40000/50000 (80%)]\tLoss: 1.163401\n","Train Epoch: 36 [48000/50000 (96%)]\tLoss: 1.130896\n","\n","Test set: Average loss: 0.0011, Accuracy: 6342/10000 (63%)\n","\n","Train Epoch: 37 [0/50000 (0%)]\tLoss: 0.981237\n","Train Epoch: 37 [8000/50000 (16%)]\tLoss: 1.038329\n","Train Epoch: 37 [16000/50000 (32%)]\tLoss: 1.457219\n","Train Epoch: 37 [24000/50000 (48%)]\tLoss: 1.587066\n","Train Epoch: 37 [32000/50000 (64%)]\tLoss: 1.058605\n","Train Epoch: 37 [40000/50000 (80%)]\tLoss: 0.866626\n","Train Epoch: 37 [48000/50000 (96%)]\tLoss: 1.089834\n","Train Epoch: 38 [0/50000 (0%)]\tLoss: 1.144921\n","Train Epoch: 38 [8000/50000 (16%)]\tLoss: 0.969007\n","Train Epoch: 38 [16000/50000 (32%)]\tLoss: 1.347941\n","Train Epoch: 38 [24000/50000 (48%)]\tLoss: 1.467262\n","Train Epoch: 38 [32000/50000 (64%)]\tLoss: 1.159026\n","Train Epoch: 38 [40000/50000 (80%)]\tLoss: 1.540494\n","Train Epoch: 38 [48000/50000 (96%)]\tLoss: 0.915171\n","\n","Test set: Average loss: 0.0012, Accuracy: 5986/10000 (60%)\n","\n","Train Epoch: 39 [0/50000 (0%)]\tLoss: 1.374433\n","Train Epoch: 39 [8000/50000 (16%)]\tLoss: 1.189823\n","Train Epoch: 39 [16000/50000 (32%)]\tLoss: 1.224567\n","Train Epoch: 39 [24000/50000 (48%)]\tLoss: 1.057421\n","Train Epoch: 39 [32000/50000 (64%)]\tLoss: 1.334910\n","Train Epoch: 39 [40000/50000 (80%)]\tLoss: 1.153786\n","Train Epoch: 39 [48000/50000 (96%)]\tLoss: 1.411380\n","Train Epoch: 40 [0/50000 (0%)]\tLoss: 0.881659\n","Train Epoch: 40 [8000/50000 (16%)]\tLoss: 1.492895\n","Train Epoch: 40 [16000/50000 (32%)]\tLoss: 1.560953\n","Train Epoch: 40 [24000/50000 (48%)]\tLoss: 1.302748\n","Train Epoch: 40 [32000/50000 (64%)]\tLoss: 1.317562\n","Train Epoch: 40 [40000/50000 (80%)]\tLoss: 1.143389\n","Train Epoch: 40 [48000/50000 (96%)]\tLoss: 1.291125\n","\n","Train set: Average loss: 0.0711, Accuracy: 29742/50000 (59%)\n","\n","\n","Test set: Average loss: 0.0012, Accuracy: 5849/10000 (58%)\n","\n","Train Epoch: 41 [0/50000 (0%)]\tLoss: 1.234586\n","Train Epoch: 41 [8000/50000 (16%)]\tLoss: 0.737042\n","Train Epoch: 41 [16000/50000 (32%)]\tLoss: 1.254429\n","Train Epoch: 41 [24000/50000 (48%)]\tLoss: 1.201520\n","Train Epoch: 41 [32000/50000 (64%)]\tLoss: 2.517154\n","Train Epoch: 41 [40000/50000 (80%)]\tLoss: 0.765743\n","Train Epoch: 41 [48000/50000 (96%)]\tLoss: 1.156833\n","Train Epoch: 42 [0/50000 (0%)]\tLoss: 0.994692\n","Train Epoch: 42 [8000/50000 (16%)]\tLoss: 1.664806\n","Train Epoch: 42 [16000/50000 (32%)]\tLoss: 1.308219\n","Train Epoch: 42 [24000/50000 (48%)]\tLoss: 1.367351\n","Train Epoch: 42 [32000/50000 (64%)]\tLoss: 1.793536\n","Train Epoch: 42 [40000/50000 (80%)]\tLoss: 1.237584\n","Train Epoch: 42 [48000/50000 (96%)]\tLoss: 1.077090\n","\n","Test set: Average loss: 0.0011, Accuracy: 6094/10000 (61%)\n","\n","Train Epoch: 43 [0/50000 (0%)]\tLoss: 0.886300\n","Train Epoch: 43 [8000/50000 (16%)]\tLoss: 1.399599\n","Train Epoch: 43 [16000/50000 (32%)]\tLoss: 1.227203\n","Train Epoch: 43 [24000/50000 (48%)]\tLoss: 1.222469\n","Train Epoch: 43 [32000/50000 (64%)]\tLoss: 1.499078\n","Train Epoch: 43 [40000/50000 (80%)]\tLoss: 1.557432\n","Train Epoch: 43 [48000/50000 (96%)]\tLoss: 1.179182\n","Train Epoch: 44 [0/50000 (0%)]\tLoss: 1.267369\n","Train Epoch: 44 [8000/50000 (16%)]\tLoss: 1.572388\n","Train Epoch: 44 [16000/50000 (32%)]\tLoss: 1.238052\n","Train Epoch: 44 [24000/50000 (48%)]\tLoss: 1.271028\n","Train Epoch: 44 [32000/50000 (64%)]\tLoss: 1.188550\n","Train Epoch: 44 [40000/50000 (80%)]\tLoss: 1.161294\n","Train Epoch: 44 [48000/50000 (96%)]\tLoss: 0.870063\n","\n","Test set: Average loss: 0.0012, Accuracy: 5958/10000 (60%)\n","\n","Train Epoch: 45 [0/50000 (0%)]\tLoss: 1.194581\n","Train Epoch: 45 [8000/50000 (16%)]\tLoss: 0.761361\n","Train Epoch: 45 [16000/50000 (32%)]\tLoss: 0.857748\n","Train Epoch: 45 [24000/50000 (48%)]\tLoss: 0.996378\n","Train Epoch: 45 [32000/50000 (64%)]\tLoss: 0.759691\n","Train Epoch: 45 [40000/50000 (80%)]\tLoss: 0.858895\n","Train Epoch: 45 [48000/50000 (96%)]\tLoss: 1.076513\n","Train Epoch: 46 [0/50000 (0%)]\tLoss: 0.827258\n","Train Epoch: 46 [8000/50000 (16%)]\tLoss: 1.284308\n","Train Epoch: 46 [16000/50000 (32%)]\tLoss: 1.201105\n","Train Epoch: 46 [24000/50000 (48%)]\tLoss: 1.313845\n","Train Epoch: 46 [32000/50000 (64%)]\tLoss: 1.420494\n","Train Epoch: 46 [40000/50000 (80%)]\tLoss: 1.395581\n","Train Epoch: 46 [48000/50000 (96%)]\tLoss: 1.361315\n","\n","Test set: Average loss: 0.0011, Accuracy: 6159/10000 (62%)\n","\n","Train Epoch: 47 [0/50000 (0%)]\tLoss: 0.634924\n","Train Epoch: 47 [8000/50000 (16%)]\tLoss: 1.242297\n","Train Epoch: 47 [16000/50000 (32%)]\tLoss: 1.018473\n","Train Epoch: 47 [24000/50000 (48%)]\tLoss: 1.188221\n","Train Epoch: 47 [32000/50000 (64%)]\tLoss: 1.208926\n","Train Epoch: 47 [40000/50000 (80%)]\tLoss: 1.253034\n","Train Epoch: 47 [48000/50000 (96%)]\tLoss: 0.837833\n","Train Epoch: 48 [0/50000 (0%)]\tLoss: 1.460604\n","Train Epoch: 48 [8000/50000 (16%)]\tLoss: 1.769686\n","Train Epoch: 48 [16000/50000 (32%)]\tLoss: 1.597350\n","Train Epoch: 48 [24000/50000 (48%)]\tLoss: 1.476165\n","Train Epoch: 48 [32000/50000 (64%)]\tLoss: 1.309410\n","Train Epoch: 48 [40000/50000 (80%)]\tLoss: 0.806626\n","Train Epoch: 48 [48000/50000 (96%)]\tLoss: 1.699629\n","\n","Test set: Average loss: 0.0011, Accuracy: 6077/10000 (61%)\n","\n","Train Epoch: 49 [0/50000 (0%)]\tLoss: 0.724005\n","Train Epoch: 49 [8000/50000 (16%)]\tLoss: 1.141163\n","Train Epoch: 49 [16000/50000 (32%)]\tLoss: 1.116983\n","Train Epoch: 49 [24000/50000 (48%)]\tLoss: 1.251984\n","Train Epoch: 49 [32000/50000 (64%)]\tLoss: 1.005633\n","Train Epoch: 49 [40000/50000 (80%)]\tLoss: 1.118237\n","Train Epoch: 49 [48000/50000 (96%)]\tLoss: 1.046083\n","Train Epoch: 50 [0/50000 (0%)]\tLoss: 1.434921\n","Train Epoch: 50 [8000/50000 (16%)]\tLoss: 0.983719\n","Train Epoch: 50 [16000/50000 (32%)]\tLoss: 1.124223\n","Train Epoch: 50 [24000/50000 (48%)]\tLoss: 1.047233\n","Train Epoch: 50 [32000/50000 (64%)]\tLoss: 1.328263\n","Train Epoch: 50 [40000/50000 (80%)]\tLoss: 1.377968\n","Train Epoch: 50 [48000/50000 (96%)]\tLoss: 1.156491\n","\n","Train set: Average loss: 0.0674, Accuracy: 31588/50000 (63%)\n","\n","\n","Test set: Average loss: 0.0011, Accuracy: 6237/10000 (62%)\n","\n","Train Epoch: 51 [0/50000 (0%)]\tLoss: 1.469757\n","Train Epoch: 51 [8000/50000 (16%)]\tLoss: 0.801830\n","Train Epoch: 51 [16000/50000 (32%)]\tLoss: 1.190547\n","Train Epoch: 51 [24000/50000 (48%)]\tLoss: 0.755903\n","Train Epoch: 51 [32000/50000 (64%)]\tLoss: 1.232770\n","Train Epoch: 51 [40000/50000 (80%)]\tLoss: 1.414814\n","Train Epoch: 51 [48000/50000 (96%)]\tLoss: 1.108609\n","Train Epoch: 52 [0/50000 (0%)]\tLoss: 1.159583\n","Train Epoch: 52 [8000/50000 (16%)]\tLoss: 1.380953\n","Train Epoch: 52 [16000/50000 (32%)]\tLoss: 0.889235\n","Train Epoch: 52 [24000/50000 (48%)]\tLoss: 1.094725\n","Train Epoch: 52 [32000/50000 (64%)]\tLoss: 1.186152\n","Train Epoch: 52 [40000/50000 (80%)]\tLoss: 0.740614\n","Train Epoch: 52 [48000/50000 (96%)]\tLoss: 1.137359\n","\n","Test set: Average loss: 0.0011, Accuracy: 6089/10000 (61%)\n","\n","Train Epoch: 53 [0/50000 (0%)]\tLoss: 1.150082\n","Train Epoch: 53 [8000/50000 (16%)]\tLoss: 1.331786\n","Train Epoch: 53 [16000/50000 (32%)]\tLoss: 1.219981\n","Train Epoch: 53 [24000/50000 (48%)]\tLoss: 1.471927\n","Train Epoch: 53 [32000/50000 (64%)]\tLoss: 1.523911\n","Train Epoch: 53 [40000/50000 (80%)]\tLoss: 1.411910\n","Train Epoch: 53 [48000/50000 (96%)]\tLoss: 0.881847\n","Train Epoch: 54 [0/50000 (0%)]\tLoss: 1.261233\n","Train Epoch: 54 [8000/50000 (16%)]\tLoss: 1.407686\n","Train Epoch: 54 [16000/50000 (32%)]\tLoss: 1.062831\n","Train Epoch: 54 [24000/50000 (48%)]\tLoss: 0.937846\n","Train Epoch: 54 [32000/50000 (64%)]\tLoss: 0.898343\n","Train Epoch: 54 [40000/50000 (80%)]\tLoss: 1.064854\n","Train Epoch: 54 [48000/50000 (96%)]\tLoss: 1.170510\n","\n","Test set: Average loss: 0.0011, Accuracy: 6398/10000 (64%)\n","\n","Train Epoch: 55 [0/50000 (0%)]\tLoss: 1.247055\n","Train Epoch: 55 [8000/50000 (16%)]\tLoss: 0.912748\n","Train Epoch: 55 [16000/50000 (32%)]\tLoss: 1.724576\n","Train Epoch: 55 [24000/50000 (48%)]\tLoss: 1.212129\n","Train Epoch: 55 [32000/50000 (64%)]\tLoss: 1.213860\n","Train Epoch: 55 [40000/50000 (80%)]\tLoss: 1.075695\n","Train Epoch: 55 [48000/50000 (96%)]\tLoss: 0.931524\n","Train Epoch: 56 [0/50000 (0%)]\tLoss: 1.702524\n","Train Epoch: 56 [8000/50000 (16%)]\tLoss: 1.706752\n","Train Epoch: 56 [16000/50000 (32%)]\tLoss: 0.838547\n","Train Epoch: 56 [24000/50000 (48%)]\tLoss: 1.075940\n","Train Epoch: 56 [32000/50000 (64%)]\tLoss: 1.340765\n","Train Epoch: 56 [40000/50000 (80%)]\tLoss: 1.063414\n","Train Epoch: 56 [48000/50000 (96%)]\tLoss: 1.090743\n","\n","Test set: Average loss: 0.0011, Accuracy: 6284/10000 (63%)\n","\n","Train Epoch: 57 [0/50000 (0%)]\tLoss: 1.160682\n","Train Epoch: 57 [8000/50000 (16%)]\tLoss: 1.526352\n","Train Epoch: 57 [16000/50000 (32%)]\tLoss: 0.980409\n","Train Epoch: 57 [24000/50000 (48%)]\tLoss: 0.942313\n","Train Epoch: 57 [32000/50000 (64%)]\tLoss: 0.805425\n","Train Epoch: 57 [40000/50000 (80%)]\tLoss: 0.812310\n","Train Epoch: 57 [48000/50000 (96%)]\tLoss: 1.306697\n","Train Epoch: 58 [0/50000 (0%)]\tLoss: 1.222961\n","Train Epoch: 58 [8000/50000 (16%)]\tLoss: 1.284460\n","Train Epoch: 58 [16000/50000 (32%)]\tLoss: 1.268648\n","Train Epoch: 58 [24000/50000 (48%)]\tLoss: 1.056776\n","Train Epoch: 58 [32000/50000 (64%)]\tLoss: 0.913379\n","Train Epoch: 58 [40000/50000 (80%)]\tLoss: 1.425134\n","Train Epoch: 58 [48000/50000 (96%)]\tLoss: 1.206692\n","\n","Test set: Average loss: 0.0011, Accuracy: 6064/10000 (61%)\n","\n","Train Epoch: 59 [0/50000 (0%)]\tLoss: 0.800259\n","Train Epoch: 59 [8000/50000 (16%)]\tLoss: 0.567821\n","Train Epoch: 59 [16000/50000 (32%)]\tLoss: 1.227072\n","Train Epoch: 59 [24000/50000 (48%)]\tLoss: 0.913562\n","Train Epoch: 59 [32000/50000 (64%)]\tLoss: 1.106574\n","Train Epoch: 59 [40000/50000 (80%)]\tLoss: 1.410051\n","Train Epoch: 59 [48000/50000 (96%)]\tLoss: 0.817480\n","Train Epoch: 60 [0/50000 (0%)]\tLoss: 1.797338\n","Train Epoch: 60 [8000/50000 (16%)]\tLoss: 1.134122\n","Train Epoch: 60 [16000/50000 (32%)]\tLoss: 1.167755\n","Train Epoch: 60 [24000/50000 (48%)]\tLoss: 1.111196\n","Train Epoch: 60 [32000/50000 (64%)]\tLoss: 1.018349\n","Train Epoch: 60 [40000/50000 (80%)]\tLoss: 1.355224\n","Train Epoch: 60 [48000/50000 (96%)]\tLoss: 1.157472\n","\n","Train set: Average loss: 0.0677, Accuracy: 31461/50000 (63%)\n","\n","\n","Test set: Average loss: 0.0011, Accuracy: 6224/10000 (62%)\n","\n","Train Epoch: 61 [0/50000 (0%)]\tLoss: 1.159851\n","Train Epoch: 61 [8000/50000 (16%)]\tLoss: 0.901547\n","Train Epoch: 61 [16000/50000 (32%)]\tLoss: 0.971469\n","Train Epoch: 61 [24000/50000 (48%)]\tLoss: 1.011134\n","Train Epoch: 61 [32000/50000 (64%)]\tLoss: 1.161220\n","Train Epoch: 61 [40000/50000 (80%)]\tLoss: 1.280224\n","Train Epoch: 61 [48000/50000 (96%)]\tLoss: 0.808653\n","Train Epoch: 62 [0/50000 (0%)]\tLoss: 1.445766\n","Train Epoch: 62 [8000/50000 (16%)]\tLoss: 1.735663\n","Train Epoch: 62 [16000/50000 (32%)]\tLoss: 1.244209\n","Train Epoch: 62 [24000/50000 (48%)]\tLoss: 0.902468\n","Train Epoch: 62 [32000/50000 (64%)]\tLoss: 1.116735\n","Train Epoch: 62 [40000/50000 (80%)]\tLoss: 0.838220\n","Train Epoch: 62 [48000/50000 (96%)]\tLoss: 0.908249\n","\n","Test set: Average loss: 0.0011, Accuracy: 6329/10000 (63%)\n","\n","Train Epoch: 63 [0/50000 (0%)]\tLoss: 1.084655\n","Train Epoch: 63 [8000/50000 (16%)]\tLoss: 1.318769\n","Train Epoch: 63 [16000/50000 (32%)]\tLoss: 1.127890\n","Train Epoch: 63 [24000/50000 (48%)]\tLoss: 1.425430\n","Train Epoch: 63 [32000/50000 (64%)]\tLoss: 0.785200\n","Train Epoch: 63 [40000/50000 (80%)]\tLoss: 0.965301\n","Train Epoch: 63 [48000/50000 (96%)]\tLoss: 0.883246\n","Train Epoch: 64 [0/50000 (0%)]\tLoss: 1.242586\n","Train Epoch: 64 [8000/50000 (16%)]\tLoss: 1.411382\n","Train Epoch: 64 [16000/50000 (32%)]\tLoss: 1.410119\n","Train Epoch: 64 [24000/50000 (48%)]\tLoss: 0.797262\n","Train Epoch: 64 [32000/50000 (64%)]\tLoss: 1.176758\n","Train Epoch: 64 [40000/50000 (80%)]\tLoss: 1.143092\n","Train Epoch: 64 [48000/50000 (96%)]\tLoss: 1.032865\n","\n","Test set: Average loss: 0.0011, Accuracy: 6057/10000 (61%)\n","\n","Train Epoch: 65 [0/50000 (0%)]\tLoss: 1.846467\n","Train Epoch: 65 [8000/50000 (16%)]\tLoss: 1.426742\n","Train Epoch: 65 [16000/50000 (32%)]\tLoss: 0.962421\n","Train Epoch: 65 [24000/50000 (48%)]\tLoss: 1.130103\n","Train Epoch: 65 [32000/50000 (64%)]\tLoss: 0.610139\n","Train Epoch: 65 [40000/50000 (80%)]\tLoss: 1.005128\n","Train Epoch: 65 [48000/50000 (96%)]\tLoss: 1.337515\n","Train Epoch: 66 [0/50000 (0%)]\tLoss: 0.892959\n","Train Epoch: 66 [8000/50000 (16%)]\tLoss: 0.950322\n","Train Epoch: 66 [16000/50000 (32%)]\tLoss: 1.335723\n","Train Epoch: 66 [24000/50000 (48%)]\tLoss: 0.739453\n","Train Epoch: 66 [32000/50000 (64%)]\tLoss: 1.416561\n","Train Epoch: 66 [40000/50000 (80%)]\tLoss: 1.309833\n","Train Epoch: 66 [48000/50000 (96%)]\tLoss: 1.387618\n","\n","Test set: Average loss: 0.0011, Accuracy: 6392/10000 (64%)\n","\n","Train Epoch: 67 [0/50000 (0%)]\tLoss: 1.365963\n","Train Epoch: 67 [8000/50000 (16%)]\tLoss: 1.082907\n","Train Epoch: 67 [16000/50000 (32%)]\tLoss: 1.304402\n","Train Epoch: 67 [24000/50000 (48%)]\tLoss: 1.350340\n","Train Epoch: 67 [32000/50000 (64%)]\tLoss: 1.426506\n","Train Epoch: 67 [40000/50000 (80%)]\tLoss: 0.958679\n","Train Epoch: 67 [48000/50000 (96%)]\tLoss: 0.961650\n","Train Epoch: 68 [0/50000 (0%)]\tLoss: 0.879985\n","Train Epoch: 68 [8000/50000 (16%)]\tLoss: 0.953784\n","Train Epoch: 68 [16000/50000 (32%)]\tLoss: 1.075976\n","Train Epoch: 68 [24000/50000 (48%)]\tLoss: 1.364927\n","Train Epoch: 68 [32000/50000 (64%)]\tLoss: 1.319736\n","Train Epoch: 68 [40000/50000 (80%)]\tLoss: 1.079880\n","Train Epoch: 68 [48000/50000 (96%)]\tLoss: 1.264458\n","\n","Test set: Average loss: 0.0012, Accuracy: 5807/10000 (58%)\n","\n","Train Epoch: 69 [0/50000 (0%)]\tLoss: 1.138264\n","Train Epoch: 69 [8000/50000 (16%)]\tLoss: 1.094446\n","Train Epoch: 69 [16000/50000 (32%)]\tLoss: 1.179743\n","Train Epoch: 69 [24000/50000 (48%)]\tLoss: 0.854499\n","Train Epoch: 69 [32000/50000 (64%)]\tLoss: 0.926515\n","Train Epoch: 69 [40000/50000 (80%)]\tLoss: 1.157217\n","Train Epoch: 69 [48000/50000 (96%)]\tLoss: 0.995860\n","Train Epoch: 70 [0/50000 (0%)]\tLoss: 0.978064\n","Train Epoch: 70 [8000/50000 (16%)]\tLoss: 1.393370\n","Train Epoch: 70 [16000/50000 (32%)]\tLoss: 1.210110\n","Train Epoch: 70 [24000/50000 (48%)]\tLoss: 1.080366\n","Train Epoch: 70 [32000/50000 (64%)]\tLoss: 0.806069\n","Train Epoch: 70 [40000/50000 (80%)]\tLoss: 1.078942\n","Train Epoch: 70 [48000/50000 (96%)]\tLoss: 1.458951\n","\n","Train set: Average loss: 0.0644, Accuracy: 32025/50000 (64%)\n","\n","\n","Test set: Average loss: 0.0011, Accuracy: 6330/10000 (63%)\n","\n","Train Epoch: 71 [0/50000 (0%)]\tLoss: 1.595445\n","Train Epoch: 71 [8000/50000 (16%)]\tLoss: 1.133688\n","Train Epoch: 71 [16000/50000 (32%)]\tLoss: 1.509877\n","Train Epoch: 71 [24000/50000 (48%)]\tLoss: 1.003782\n","Train Epoch: 71 [32000/50000 (64%)]\tLoss: 0.819559\n","Train Epoch: 71 [40000/50000 (80%)]\tLoss: 0.774242\n","Train Epoch: 71 [48000/50000 (96%)]\tLoss: 1.404735\n","Train Epoch: 72 [0/50000 (0%)]\tLoss: 1.247613\n","Train Epoch: 72 [8000/50000 (16%)]\tLoss: 0.863228\n","Train Epoch: 72 [16000/50000 (32%)]\tLoss: 1.112772\n","Train Epoch: 72 [24000/50000 (48%)]\tLoss: 0.950638\n","Train Epoch: 72 [32000/50000 (64%)]\tLoss: 1.080209\n","Train Epoch: 72 [40000/50000 (80%)]\tLoss: 1.325363\n","Train Epoch: 72 [48000/50000 (96%)]\tLoss: 1.835028\n","\n","Test set: Average loss: 0.0011, Accuracy: 6154/10000 (62%)\n","\n","Train Epoch: 73 [0/50000 (0%)]\tLoss: 1.121359\n","Train Epoch: 73 [8000/50000 (16%)]\tLoss: 0.796996\n","Train Epoch: 73 [16000/50000 (32%)]\tLoss: 0.967674\n","Train Epoch: 73 [24000/50000 (48%)]\tLoss: 1.513062\n","Train Epoch: 73 [32000/50000 (64%)]\tLoss: 1.157832\n","Train Epoch: 73 [40000/50000 (80%)]\tLoss: 0.792534\n","Train Epoch: 73 [48000/50000 (96%)]\tLoss: 1.844006\n","Train Epoch: 74 [0/50000 (0%)]\tLoss: 1.474732\n","Train Epoch: 74 [8000/50000 (16%)]\tLoss: 1.286528\n","Train Epoch: 74 [16000/50000 (32%)]\tLoss: 0.601130\n","Train Epoch: 74 [24000/50000 (48%)]\tLoss: 0.731406\n","Train Epoch: 74 [32000/50000 (64%)]\tLoss: 1.798197\n","Train Epoch: 74 [40000/50000 (80%)]\tLoss: 1.234543\n","Train Epoch: 74 [48000/50000 (96%)]\tLoss: 1.397794\n","\n","Test set: Average loss: 0.0011, Accuracy: 6366/10000 (64%)\n","\n","Train Epoch: 75 [0/50000 (0%)]\tLoss: 0.807083\n","Train Epoch: 75 [8000/50000 (16%)]\tLoss: 0.514794\n","Train Epoch: 75 [16000/50000 (32%)]\tLoss: 1.066812\n","Train Epoch: 75 [24000/50000 (48%)]\tLoss: 1.075388\n","Train Epoch: 75 [32000/50000 (64%)]\tLoss: 1.108335\n","Train Epoch: 75 [40000/50000 (80%)]\tLoss: 0.795414\n","Train Epoch: 75 [48000/50000 (96%)]\tLoss: 1.460238\n","Train Epoch: 76 [0/50000 (0%)]\tLoss: 1.087418\n","Train Epoch: 76 [8000/50000 (16%)]\tLoss: 0.960647\n","Train Epoch: 76 [16000/50000 (32%)]\tLoss: 1.172919\n","Train Epoch: 76 [24000/50000 (48%)]\tLoss: 0.745606\n","Train Epoch: 76 [32000/50000 (64%)]\tLoss: 0.987430\n","Train Epoch: 76 [40000/50000 (80%)]\tLoss: 0.571134\n","Train Epoch: 76 [48000/50000 (96%)]\tLoss: 1.164929\n","\n","Test set: Average loss: 0.0011, Accuracy: 6125/10000 (61%)\n","\n","Train Epoch: 77 [0/50000 (0%)]\tLoss: 1.447585\n","Train Epoch: 77 [8000/50000 (16%)]\tLoss: 1.203200\n","Train Epoch: 77 [16000/50000 (32%)]\tLoss: 1.427460\n","Train Epoch: 77 [24000/50000 (48%)]\tLoss: 1.697101\n","Train Epoch: 77 [32000/50000 (64%)]\tLoss: 1.341846\n","Train Epoch: 77 [40000/50000 (80%)]\tLoss: 1.280421\n","Train Epoch: 77 [48000/50000 (96%)]\tLoss: 1.307014\n","Train Epoch: 78 [0/50000 (0%)]\tLoss: 1.009212\n","Train Epoch: 78 [8000/50000 (16%)]\tLoss: 1.845175\n","Train Epoch: 78 [16000/50000 (32%)]\tLoss: 0.734701\n","Train Epoch: 78 [24000/50000 (48%)]\tLoss: 0.865122\n","Train Epoch: 78 [32000/50000 (64%)]\tLoss: 1.218145\n","Train Epoch: 78 [40000/50000 (80%)]\tLoss: 1.119983\n","Train Epoch: 78 [48000/50000 (96%)]\tLoss: 1.354375\n","\n","Test set: Average loss: 0.0010, Accuracy: 6564/10000 (66%)\n","\n","Train Epoch: 79 [0/50000 (0%)]\tLoss: 0.749404\n","Train Epoch: 79 [8000/50000 (16%)]\tLoss: 0.802197\n","Train Epoch: 79 [16000/50000 (32%)]\tLoss: 0.954388\n","Train Epoch: 79 [24000/50000 (48%)]\tLoss: 1.236080\n","Train Epoch: 79 [32000/50000 (64%)]\tLoss: 1.483848\n","Train Epoch: 79 [40000/50000 (80%)]\tLoss: 0.913340\n","Train Epoch: 79 [48000/50000 (96%)]\tLoss: 1.365544\n","Train Epoch: 80 [0/50000 (0%)]\tLoss: 1.101752\n","Train Epoch: 80 [8000/50000 (16%)]\tLoss: 1.164190\n","Train Epoch: 80 [16000/50000 (32%)]\tLoss: 1.460365\n","Train Epoch: 80 [24000/50000 (48%)]\tLoss: 1.293048\n","Train Epoch: 80 [32000/50000 (64%)]\tLoss: 1.177015\n","Train Epoch: 80 [40000/50000 (80%)]\tLoss: 1.207534\n","Train Epoch: 80 [48000/50000 (96%)]\tLoss: 0.732173\n","\n","Train set: Average loss: 0.0634, Accuracy: 32775/50000 (66%)\n","\n","\n","Test set: Average loss: 0.0010, Accuracy: 6427/10000 (64%)\n","\n","Freezing SupermaskConv(3, 64, kernel_size=(3, 3), stride=(1, 1), bias=False, sparsity=0.5) before epoch 81\n","Train Epoch: 81 [0/50000 (0%)]\tLoss: 1.379947\n","Train Epoch: 81 [8000/50000 (16%)]\tLoss: 1.098165\n","Train Epoch: 81 [16000/50000 (32%)]\tLoss: 1.070835\n","Train Epoch: 81 [24000/50000 (48%)]\tLoss: 0.759036\n","Train Epoch: 81 [32000/50000 (64%)]\tLoss: 1.154510\n","Train Epoch: 81 [40000/50000 (80%)]\tLoss: 1.191746\n","Train Epoch: 81 [48000/50000 (96%)]\tLoss: 1.372828\n","Train Epoch: 82 [0/50000 (0%)]\tLoss: 1.061286\n","Train Epoch: 82 [8000/50000 (16%)]\tLoss: 0.823409\n","Train Epoch: 82 [16000/50000 (32%)]\tLoss: 0.902416\n","Train Epoch: 82 [24000/50000 (48%)]\tLoss: 1.291407\n","Train Epoch: 82 [32000/50000 (64%)]\tLoss: 1.355633\n","Train Epoch: 82 [40000/50000 (80%)]\tLoss: 1.288082\n","Train Epoch: 82 [48000/50000 (96%)]\tLoss: 1.067943\n","\n","Test set: Average loss: 0.0010, Accuracy: 6562/10000 (66%)\n","\n","Train Epoch: 83 [0/50000 (0%)]\tLoss: 0.776450\n","Train Epoch: 83 [8000/50000 (16%)]\tLoss: 0.925150\n","Train Epoch: 83 [16000/50000 (32%)]\tLoss: 1.637498\n","Train Epoch: 83 [24000/50000 (48%)]\tLoss: 0.773422\n","Train Epoch: 83 [32000/50000 (64%)]\tLoss: 0.898674\n","Train Epoch: 83 [40000/50000 (80%)]\tLoss: 1.184572\n","Train Epoch: 83 [48000/50000 (96%)]\tLoss: 1.963348\n","Train Epoch: 84 [0/50000 (0%)]\tLoss: 0.920438\n","Train Epoch: 84 [8000/50000 (16%)]\tLoss: 0.805298\n","Train Epoch: 84 [16000/50000 (32%)]\tLoss: 1.305970\n","Train Epoch: 84 [24000/50000 (48%)]\tLoss: 0.885865\n","Train Epoch: 84 [32000/50000 (64%)]\tLoss: 0.943083\n","Train Epoch: 84 [40000/50000 (80%)]\tLoss: 1.320640\n","Train Epoch: 84 [48000/50000 (96%)]\tLoss: 1.035932\n","\n","Test set: Average loss: 0.0010, Accuracy: 6482/10000 (65%)\n","\n","Train Epoch: 85 [0/50000 (0%)]\tLoss: 1.198371\n","Train Epoch: 85 [8000/50000 (16%)]\tLoss: 1.348298\n","Train Epoch: 85 [16000/50000 (32%)]\tLoss: 1.027102\n","Train Epoch: 85 [24000/50000 (48%)]\tLoss: 1.228687\n","Train Epoch: 85 [32000/50000 (64%)]\tLoss: 1.012242\n","Train Epoch: 85 [40000/50000 (80%)]\tLoss: 1.105901\n","Train Epoch: 85 [48000/50000 (96%)]\tLoss: 1.123409\n","Train Epoch: 86 [0/50000 (0%)]\tLoss: 1.324551\n","Train Epoch: 86 [8000/50000 (16%)]\tLoss: 1.227791\n","Train Epoch: 86 [16000/50000 (32%)]\tLoss: 0.924117\n","Train Epoch: 86 [24000/50000 (48%)]\tLoss: 1.582575\n","Train Epoch: 86 [32000/50000 (64%)]\tLoss: 0.844315\n","Train Epoch: 86 [40000/50000 (80%)]\tLoss: 1.337767\n","Train Epoch: 86 [48000/50000 (96%)]\tLoss: 1.132385\n","\n","Test set: Average loss: 0.0010, Accuracy: 6526/10000 (65%)\n","\n","Train Epoch: 87 [0/50000 (0%)]\tLoss: 1.010977\n","Train Epoch: 87 [8000/50000 (16%)]\tLoss: 1.232194\n","Train Epoch: 87 [16000/50000 (32%)]\tLoss: 1.114736\n","Train Epoch: 87 [24000/50000 (48%)]\tLoss: 1.257387\n","Train Epoch: 87 [32000/50000 (64%)]\tLoss: 1.146920\n","Train Epoch: 87 [40000/50000 (80%)]\tLoss: 0.541370\n","Train Epoch: 87 [48000/50000 (96%)]\tLoss: 0.750760\n","Train Epoch: 88 [0/50000 (0%)]\tLoss: 1.011894\n","Train Epoch: 88 [8000/50000 (16%)]\tLoss: 0.725066\n","Train Epoch: 88 [16000/50000 (32%)]\tLoss: 1.113920\n","Train Epoch: 88 [24000/50000 (48%)]\tLoss: 0.904042\n","Train Epoch: 88 [32000/50000 (64%)]\tLoss: 1.211387\n","Train Epoch: 88 [40000/50000 (80%)]\tLoss: 0.736391\n","Train Epoch: 88 [48000/50000 (96%)]\tLoss: 0.641414\n","\n","Test set: Average loss: 0.0010, Accuracy: 6701/10000 (67%)\n","\n","Train Epoch: 89 [0/50000 (0%)]\tLoss: 0.725154\n","Train Epoch: 89 [8000/50000 (16%)]\tLoss: 0.687115\n","Train Epoch: 89 [16000/50000 (32%)]\tLoss: 1.347547\n","Train Epoch: 89 [24000/50000 (48%)]\tLoss: 1.271791\n","Train Epoch: 89 [32000/50000 (64%)]\tLoss: 1.318265\n","Train Epoch: 89 [40000/50000 (80%)]\tLoss: 0.810432\n","Train Epoch: 89 [48000/50000 (96%)]\tLoss: 0.896329\n","Train Epoch: 90 [0/50000 (0%)]\tLoss: 1.209830\n","Train Epoch: 90 [8000/50000 (16%)]\tLoss: 1.115854\n","Train Epoch: 90 [16000/50000 (32%)]\tLoss: 1.021300\n","Train Epoch: 90 [24000/50000 (48%)]\tLoss: 1.338449\n","Train Epoch: 90 [32000/50000 (64%)]\tLoss: 0.675937\n","Train Epoch: 90 [40000/50000 (80%)]\tLoss: 0.970392\n","Train Epoch: 90 [48000/50000 (96%)]\tLoss: 1.174160\n","\n","Train set: Average loss: 0.0594, Accuracy: 33493/50000 (67%)\n","\n","\n","Test set: Average loss: 0.0010, Accuracy: 6571/10000 (66%)\n","\n","Train Epoch: 91 [0/50000 (0%)]\tLoss: 1.205380\n","Train Epoch: 91 [8000/50000 (16%)]\tLoss: 1.103234\n","Train Epoch: 91 [16000/50000 (32%)]\tLoss: 0.909243\n","Train Epoch: 91 [24000/50000 (48%)]\tLoss: 0.670915\n","Train Epoch: 91 [32000/50000 (64%)]\tLoss: 0.665531\n","Train Epoch: 91 [40000/50000 (80%)]\tLoss: 0.800237\n","Train Epoch: 91 [48000/50000 (96%)]\tLoss: 0.951271\n","Train Epoch: 92 [0/50000 (0%)]\tLoss: 1.061898\n","Train Epoch: 92 [8000/50000 (16%)]\tLoss: 1.018521\n","Train Epoch: 92 [16000/50000 (32%)]\tLoss: 0.942703\n","Train Epoch: 92 [24000/50000 (48%)]\tLoss: 1.187983\n","Train Epoch: 92 [32000/50000 (64%)]\tLoss: 1.715568\n","Train Epoch: 92 [40000/50000 (80%)]\tLoss: 1.084598\n","Train Epoch: 92 [48000/50000 (96%)]\tLoss: 0.640837\n","\n","Test set: Average loss: 0.0010, Accuracy: 6769/10000 (68%)\n","\n","Train Epoch: 93 [0/50000 (0%)]\tLoss: 0.814521\n","Train Epoch: 93 [8000/50000 (16%)]\tLoss: 0.809663\n","Train Epoch: 93 [16000/50000 (32%)]\tLoss: 0.915940\n","Train Epoch: 93 [24000/50000 (48%)]\tLoss: 1.034151\n","Train Epoch: 93 [32000/50000 (64%)]\tLoss: 1.484189\n","Train Epoch: 93 [40000/50000 (80%)]\tLoss: 0.815397\n","Train Epoch: 93 [48000/50000 (96%)]\tLoss: 1.003246\n","Train Epoch: 94 [0/50000 (0%)]\tLoss: 1.670447\n","Train Epoch: 94 [8000/50000 (16%)]\tLoss: 1.049036\n","Train Epoch: 94 [16000/50000 (32%)]\tLoss: 0.656328\n","Train Epoch: 94 [24000/50000 (48%)]\tLoss: 0.882182\n","Train Epoch: 94 [32000/50000 (64%)]\tLoss: 0.584288\n","Train Epoch: 94 [40000/50000 (80%)]\tLoss: 1.041322\n","Train Epoch: 94 [48000/50000 (96%)]\tLoss: 1.030625\n","\n","Test set: Average loss: 0.0009, Accuracy: 6793/10000 (68%)\n","\n","Train Epoch: 95 [0/50000 (0%)]\tLoss: 1.238559\n","Train Epoch: 95 [8000/50000 (16%)]\tLoss: 1.332277\n","Train Epoch: 95 [16000/50000 (32%)]\tLoss: 0.937450\n","Train Epoch: 95 [24000/50000 (48%)]\tLoss: 1.149299\n","Train Epoch: 95 [32000/50000 (64%)]\tLoss: 1.070314\n","Train Epoch: 95 [40000/50000 (80%)]\tLoss: 1.304919\n","Train Epoch: 95 [48000/50000 (96%)]\tLoss: 1.076962\n","Train Epoch: 96 [0/50000 (0%)]\tLoss: 0.846086\n","Train Epoch: 96 [8000/50000 (16%)]\tLoss: 0.861628\n","Train Epoch: 96 [16000/50000 (32%)]\tLoss: 1.321350\n","Train Epoch: 96 [24000/50000 (48%)]\tLoss: 1.015668\n","Train Epoch: 96 [32000/50000 (64%)]\tLoss: 0.937633\n","Train Epoch: 96 [40000/50000 (80%)]\tLoss: 0.825186\n","Train Epoch: 96 [48000/50000 (96%)]\tLoss: 0.834075\n","\n","Test set: Average loss: 0.0009, Accuracy: 6856/10000 (69%)\n","\n","Train Epoch: 97 [0/50000 (0%)]\tLoss: 0.551562\n","Train Epoch: 97 [8000/50000 (16%)]\tLoss: 0.910550\n","Train Epoch: 97 [16000/50000 (32%)]\tLoss: 1.180269\n","Train Epoch: 97 [24000/50000 (48%)]\tLoss: 1.181126\n","Train Epoch: 97 [32000/50000 (64%)]\tLoss: 0.659515\n","Train Epoch: 97 [40000/50000 (80%)]\tLoss: 1.148205\n","Train Epoch: 97 [48000/50000 (96%)]\tLoss: 1.101112\n","Train Epoch: 98 [0/50000 (0%)]\tLoss: 1.467424\n","Train Epoch: 98 [8000/50000 (16%)]\tLoss: 1.266018\n","Train Epoch: 98 [16000/50000 (32%)]\tLoss: 0.748377\n","Train Epoch: 98 [24000/50000 (48%)]\tLoss: 0.433037\n","Train Epoch: 98 [32000/50000 (64%)]\tLoss: 0.766397\n","Train Epoch: 98 [40000/50000 (80%)]\tLoss: 1.071346\n","Train Epoch: 98 [48000/50000 (96%)]\tLoss: 0.942516\n","\n","Test set: Average loss: 0.0010, Accuracy: 6743/10000 (67%)\n","\n","Train Epoch: 99 [0/50000 (0%)]\tLoss: 1.223204\n","Train Epoch: 99 [8000/50000 (16%)]\tLoss: 1.285767\n","Train Epoch: 99 [16000/50000 (32%)]\tLoss: 1.119911\n","Train Epoch: 99 [24000/50000 (48%)]\tLoss: 0.779516\n","Train Epoch: 99 [32000/50000 (64%)]\tLoss: 1.094258\n","Train Epoch: 99 [40000/50000 (80%)]\tLoss: 0.831105\n","Train Epoch: 99 [48000/50000 (96%)]\tLoss: 1.313123\n","Train Epoch: 100 [0/50000 (0%)]\tLoss: 0.982262\n","Train Epoch: 100 [8000/50000 (16%)]\tLoss: 1.346966\n","Train Epoch: 100 [16000/50000 (32%)]\tLoss: 0.813622\n","Train Epoch: 100 [24000/50000 (48%)]\tLoss: 1.097133\n","Train Epoch: 100 [32000/50000 (64%)]\tLoss: 1.084156\n","Train Epoch: 100 [40000/50000 (80%)]\tLoss: 0.892795\n","Train Epoch: 100 [48000/50000 (96%)]\tLoss: 1.077026\n","\n","Train set: Average loss: 0.0530, Accuracy: 35628/50000 (71%)\n","\n","\n","Test set: Average loss: 0.0009, Accuracy: 6952/10000 (70%)\n","\n","Freezing SupermaskConv(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False, sparsity=0.5) before epoch 101\n","Train Epoch: 101 [0/50000 (0%)]\tLoss: 0.820953\n","Train Epoch: 101 [8000/50000 (16%)]\tLoss: 0.848084\n","Train Epoch: 101 [16000/50000 (32%)]\tLoss: 1.127133\n","Train Epoch: 101 [24000/50000 (48%)]\tLoss: 1.095696\n","Train Epoch: 101 [32000/50000 (64%)]\tLoss: 1.063633\n","Train Epoch: 101 [40000/50000 (80%)]\tLoss: 0.746476\n","Train Epoch: 101 [48000/50000 (96%)]\tLoss: 0.884030\n","Train Epoch: 102 [0/50000 (0%)]\tLoss: 0.800424\n","Train Epoch: 102 [8000/50000 (16%)]\tLoss: 0.644774\n","Train Epoch: 102 [16000/50000 (32%)]\tLoss: 1.031779\n","Train Epoch: 102 [24000/50000 (48%)]\tLoss: 0.880769\n","Train Epoch: 102 [32000/50000 (64%)]\tLoss: 0.992558\n","Train Epoch: 102 [40000/50000 (80%)]\tLoss: 0.573901\n","Train Epoch: 102 [48000/50000 (96%)]\tLoss: 1.002457\n","\n","Test set: Average loss: 0.0009, Accuracy: 6950/10000 (70%)\n","\n","Train Epoch: 103 [0/50000 (0%)]\tLoss: 0.738987\n","Train Epoch: 103 [8000/50000 (16%)]\tLoss: 0.663593\n","Train Epoch: 103 [16000/50000 (32%)]\tLoss: 0.867231\n","Train Epoch: 103 [24000/50000 (48%)]\tLoss: 0.810112\n","Train Epoch: 103 [32000/50000 (64%)]\tLoss: 0.953957\n","Train Epoch: 103 [40000/50000 (80%)]\tLoss: 0.910869\n","Train Epoch: 103 [48000/50000 (96%)]\tLoss: 0.903251\n","Train Epoch: 104 [0/50000 (0%)]\tLoss: 0.667566\n","Train Epoch: 104 [8000/50000 (16%)]\tLoss: 0.982011\n","Train Epoch: 104 [16000/50000 (32%)]\tLoss: 1.227546\n","Train Epoch: 104 [24000/50000 (48%)]\tLoss: 1.051580\n","Train Epoch: 104 [32000/50000 (64%)]\tLoss: 0.808036\n","Train Epoch: 104 [40000/50000 (80%)]\tLoss: 1.271635\n","Train Epoch: 104 [48000/50000 (96%)]\tLoss: 0.933549\n","\n","Test set: Average loss: 0.0009, Accuracy: 6810/10000 (68%)\n","\n","Train Epoch: 105 [0/50000 (0%)]\tLoss: 1.217977\n","Train Epoch: 105 [8000/50000 (16%)]\tLoss: 1.057706\n","Train Epoch: 105 [16000/50000 (32%)]\tLoss: 0.737434\n","Train Epoch: 105 [24000/50000 (48%)]\tLoss: 1.168154\n","Train Epoch: 105 [32000/50000 (64%)]\tLoss: 0.893371\n","Train Epoch: 105 [40000/50000 (80%)]\tLoss: 0.761130\n","Train Epoch: 105 [48000/50000 (96%)]\tLoss: 0.982609\n","Train Epoch: 106 [0/50000 (0%)]\tLoss: 0.544139\n","Train Epoch: 106 [8000/50000 (16%)]\tLoss: 1.008479\n","Train Epoch: 106 [16000/50000 (32%)]\tLoss: 0.849690\n","Train Epoch: 106 [24000/50000 (48%)]\tLoss: 1.112157\n","Train Epoch: 106 [32000/50000 (64%)]\tLoss: 1.056841\n","Train Epoch: 106 [40000/50000 (80%)]\tLoss: 0.619439\n","Train Epoch: 106 [48000/50000 (96%)]\tLoss: 0.810058\n","\n","Test set: Average loss: 0.0009, Accuracy: 7023/10000 (70%)\n","\n","Train Epoch: 107 [0/50000 (0%)]\tLoss: 1.149615\n","Train Epoch: 107 [8000/50000 (16%)]\tLoss: 0.478054\n","Train Epoch: 107 [16000/50000 (32%)]\tLoss: 1.531858\n","Train Epoch: 107 [24000/50000 (48%)]\tLoss: 0.831811\n","Train Epoch: 107 [32000/50000 (64%)]\tLoss: 0.580606\n","Train Epoch: 107 [40000/50000 (80%)]\tLoss: 0.324074\n","Train Epoch: 107 [48000/50000 (96%)]\tLoss: 1.041331\n","Train Epoch: 108 [0/50000 (0%)]\tLoss: 0.657492\n","Train Epoch: 108 [8000/50000 (16%)]\tLoss: 1.138375\n","Train Epoch: 108 [16000/50000 (32%)]\tLoss: 1.122902\n","Train Epoch: 108 [24000/50000 (48%)]\tLoss: 0.976320\n","Train Epoch: 108 [32000/50000 (64%)]\tLoss: 1.042148\n","Train Epoch: 108 [40000/50000 (80%)]\tLoss: 1.254737\n","Train Epoch: 108 [48000/50000 (96%)]\tLoss: 0.826062\n","\n","Test set: Average loss: 0.0009, Accuracy: 7045/10000 (70%)\n","\n","Train Epoch: 109 [0/50000 (0%)]\tLoss: 1.227716\n","Train Epoch: 109 [8000/50000 (16%)]\tLoss: 0.801791\n","Train Epoch: 109 [16000/50000 (32%)]\tLoss: 1.213126\n","Train Epoch: 109 [24000/50000 (48%)]\tLoss: 0.547720\n","Train Epoch: 109 [32000/50000 (64%)]\tLoss: 1.368911\n","Train Epoch: 109 [40000/50000 (80%)]\tLoss: 0.726092\n","Train Epoch: 109 [48000/50000 (96%)]\tLoss: 0.865114\n","Train Epoch: 110 [0/50000 (0%)]\tLoss: 0.736128\n","Train Epoch: 110 [8000/50000 (16%)]\tLoss: 0.931316\n","Train Epoch: 110 [16000/50000 (32%)]\tLoss: 0.799360\n","Train Epoch: 110 [24000/50000 (48%)]\tLoss: 0.598972\n","Train Epoch: 110 [32000/50000 (64%)]\tLoss: 0.451730\n","Train Epoch: 110 [40000/50000 (80%)]\tLoss: 0.906878\n","Train Epoch: 110 [48000/50000 (96%)]\tLoss: 0.669439\n","\n","Train set: Average loss: 0.0518, Accuracy: 35864/50000 (72%)\n","\n","\n","Test set: Average loss: 0.0009, Accuracy: 7041/10000 (70%)\n","\n","Train Epoch: 111 [0/50000 (0%)]\tLoss: 1.697620\n","Train Epoch: 111 [8000/50000 (16%)]\tLoss: 0.412534\n","Train Epoch: 111 [16000/50000 (32%)]\tLoss: 1.113616\n","Train Epoch: 111 [24000/50000 (48%)]\tLoss: 1.054451\n","Train Epoch: 111 [32000/50000 (64%)]\tLoss: 0.993240\n","Train Epoch: 111 [40000/50000 (80%)]\tLoss: 1.231228\n","Train Epoch: 111 [48000/50000 (96%)]\tLoss: 0.873534\n","Train Epoch: 112 [0/50000 (0%)]\tLoss: 1.456074\n","Train Epoch: 112 [8000/50000 (16%)]\tLoss: 1.518266\n","Train Epoch: 112 [16000/50000 (32%)]\tLoss: 1.030420\n","Train Epoch: 112 [24000/50000 (48%)]\tLoss: 1.232090\n","Train Epoch: 112 [32000/50000 (64%)]\tLoss: 0.666957\n","Train Epoch: 112 [40000/50000 (80%)]\tLoss: 0.658251\n","Train Epoch: 112 [48000/50000 (96%)]\tLoss: 0.961549\n","\n","Test set: Average loss: 0.0008, Accuracy: 7162/10000 (72%)\n","\n","Train Epoch: 113 [0/50000 (0%)]\tLoss: 0.750966\n","Train Epoch: 113 [8000/50000 (16%)]\tLoss: 0.933361\n","Train Epoch: 113 [16000/50000 (32%)]\tLoss: 0.534900\n","Train Epoch: 113 [24000/50000 (48%)]\tLoss: 0.996843\n","Train Epoch: 113 [32000/50000 (64%)]\tLoss: 1.145149\n","Train Epoch: 113 [40000/50000 (80%)]\tLoss: 0.642584\n","Train Epoch: 113 [48000/50000 (96%)]\tLoss: 0.884329\n","Train Epoch: 114 [0/50000 (0%)]\tLoss: 0.822056\n","Train Epoch: 114 [8000/50000 (16%)]\tLoss: 0.747834\n","Train Epoch: 114 [16000/50000 (32%)]\tLoss: 0.692752\n","Train Epoch: 114 [24000/50000 (48%)]\tLoss: 0.778994\n","Train Epoch: 114 [32000/50000 (64%)]\tLoss: 0.993988\n","Train Epoch: 114 [40000/50000 (80%)]\tLoss: 1.277779\n","Train Epoch: 114 [48000/50000 (96%)]\tLoss: 1.330083\n","\n","Test set: Average loss: 0.0008, Accuracy: 7205/10000 (72%)\n","\n","Train Epoch: 115 [0/50000 (0%)]\tLoss: 0.738272\n","Train Epoch: 115 [8000/50000 (16%)]\tLoss: 1.019422\n","Train Epoch: 115 [16000/50000 (32%)]\tLoss: 0.967631\n","Train Epoch: 115 [24000/50000 (48%)]\tLoss: 0.938345\n","Train Epoch: 115 [32000/50000 (64%)]\tLoss: 1.045713\n","Train Epoch: 115 [40000/50000 (80%)]\tLoss: 0.754921\n","Train Epoch: 115 [48000/50000 (96%)]\tLoss: 0.814137\n","Train Epoch: 116 [0/50000 (0%)]\tLoss: 0.580148\n","Train Epoch: 116 [8000/50000 (16%)]\tLoss: 0.633285\n","Train Epoch: 116 [16000/50000 (32%)]\tLoss: 0.802390\n","Train Epoch: 116 [24000/50000 (48%)]\tLoss: 0.753452\n","Train Epoch: 116 [32000/50000 (64%)]\tLoss: 0.561616\n","Train Epoch: 116 [40000/50000 (80%)]\tLoss: 1.084395\n","Train Epoch: 116 [48000/50000 (96%)]\tLoss: 0.424981\n","\n","Test set: Average loss: 0.0009, Accuracy: 6993/10000 (70%)\n","\n","Train Epoch: 117 [0/50000 (0%)]\tLoss: 1.277550\n","Train Epoch: 117 [8000/50000 (16%)]\tLoss: 0.907626\n","Train Epoch: 117 [16000/50000 (32%)]\tLoss: 1.050488\n","Train Epoch: 117 [24000/50000 (48%)]\tLoss: 1.240602\n","Train Epoch: 117 [32000/50000 (64%)]\tLoss: 0.631579\n","Train Epoch: 117 [40000/50000 (80%)]\tLoss: 0.994277\n","Train Epoch: 117 [48000/50000 (96%)]\tLoss: 0.752802\n","Train Epoch: 118 [0/50000 (0%)]\tLoss: 1.102955\n","Train Epoch: 118 [8000/50000 (16%)]\tLoss: 0.793398\n","Train Epoch: 118 [16000/50000 (32%)]\tLoss: 0.710725\n","Train Epoch: 118 [24000/50000 (48%)]\tLoss: 0.364289\n","Train Epoch: 118 [32000/50000 (64%)]\tLoss: 0.691427\n","Train Epoch: 118 [40000/50000 (80%)]\tLoss: 0.970695\n","Train Epoch: 118 [48000/50000 (96%)]\tLoss: 0.636677\n","\n","Test set: Average loss: 0.0008, Accuracy: 7168/10000 (72%)\n","\n","Train Epoch: 119 [0/50000 (0%)]\tLoss: 0.787639\n","Train Epoch: 119 [8000/50000 (16%)]\tLoss: 0.754025\n","Train Epoch: 119 [16000/50000 (32%)]\tLoss: 1.314152\n","Train Epoch: 119 [24000/50000 (48%)]\tLoss: 1.247104\n","Train Epoch: 119 [32000/50000 (64%)]\tLoss: 1.055471\n","Train Epoch: 119 [40000/50000 (80%)]\tLoss: 0.664405\n","Train Epoch: 119 [48000/50000 (96%)]\tLoss: 0.646433\n","Train Epoch: 120 [0/50000 (0%)]\tLoss: 1.354038\n","Train Epoch: 120 [8000/50000 (16%)]\tLoss: 0.592498\n","Train Epoch: 120 [16000/50000 (32%)]\tLoss: 0.802375\n","Train Epoch: 120 [24000/50000 (48%)]\tLoss: 1.080847\n","Train Epoch: 120 [32000/50000 (64%)]\tLoss: 0.634244\n","Train Epoch: 120 [40000/50000 (80%)]\tLoss: 0.449743\n","Train Epoch: 120 [48000/50000 (96%)]\tLoss: 1.044344\n","\n","Train set: Average loss: 0.0488, Accuracy: 36924/50000 (74%)\n","\n","\n","Test set: Average loss: 0.0008, Accuracy: 7185/10000 (72%)\n","\n","Freezing SupermaskLinear(in_features=256, out_features=10, bias=False, sparsity=0.5) before epoch 121\n","Train Epoch: 121 [0/50000 (0%)]\tLoss: 0.916462\n","Train Epoch: 121 [8000/50000 (16%)]\tLoss: 0.891431\n","Train Epoch: 121 [16000/50000 (32%)]\tLoss: 0.918562\n","Train Epoch: 121 [24000/50000 (48%)]\tLoss: 0.605069\n","Train Epoch: 121 [32000/50000 (64%)]\tLoss: 1.133221\n","Train Epoch: 121 [40000/50000 (80%)]\tLoss: 0.709551\n","Train Epoch: 121 [48000/50000 (96%)]\tLoss: 1.066253\n","Train Epoch: 122 [0/50000 (0%)]\tLoss: 0.855242\n","Train Epoch: 122 [8000/50000 (16%)]\tLoss: 0.908962\n","Train Epoch: 122 [16000/50000 (32%)]\tLoss: 0.969157\n","Train Epoch: 122 [24000/50000 (48%)]\tLoss: 0.466731\n","Train Epoch: 122 [32000/50000 (64%)]\tLoss: 0.621335\n","Train Epoch: 122 [40000/50000 (80%)]\tLoss: 0.899928\n","Train Epoch: 122 [48000/50000 (96%)]\tLoss: 0.729267\n","\n","Test set: Average loss: 0.0008, Accuracy: 7214/10000 (72%)\n","\n","Train Epoch: 123 [0/50000 (0%)]\tLoss: 1.013250\n","Train Epoch: 123 [8000/50000 (16%)]\tLoss: 0.645191\n","Train Epoch: 123 [16000/50000 (32%)]\tLoss: 0.588883\n","Train Epoch: 123 [24000/50000 (48%)]\tLoss: 0.931614\n","Train Epoch: 123 [32000/50000 (64%)]\tLoss: 0.766992\n","Train Epoch: 123 [40000/50000 (80%)]\tLoss: 0.771448\n","Train Epoch: 123 [48000/50000 (96%)]\tLoss: 0.774611\n","Train Epoch: 124 [0/50000 (0%)]\tLoss: 1.119952\n","Train Epoch: 124 [8000/50000 (16%)]\tLoss: 0.941981\n","Train Epoch: 124 [16000/50000 (32%)]\tLoss: 0.564202\n","Train Epoch: 124 [24000/50000 (48%)]\tLoss: 0.648013\n","Train Epoch: 124 [32000/50000 (64%)]\tLoss: 0.526918\n","Train Epoch: 124 [40000/50000 (80%)]\tLoss: 0.572464\n","Train Epoch: 124 [48000/50000 (96%)]\tLoss: 0.415148\n","\n","Test set: Average loss: 0.0008, Accuracy: 7341/10000 (73%)\n","\n","Train Epoch: 125 [0/50000 (0%)]\tLoss: 0.530757\n","Train Epoch: 125 [8000/50000 (16%)]\tLoss: 1.088770\n","Train Epoch: 125 [16000/50000 (32%)]\tLoss: 0.686875\n","Train Epoch: 125 [24000/50000 (48%)]\tLoss: 0.874429\n","Train Epoch: 125 [32000/50000 (64%)]\tLoss: 0.792666\n","Train Epoch: 125 [40000/50000 (80%)]\tLoss: 0.717894\n","Train Epoch: 125 [48000/50000 (96%)]\tLoss: 0.804293\n","Train Epoch: 126 [0/50000 (0%)]\tLoss: 1.502076\n","Train Epoch: 126 [8000/50000 (16%)]\tLoss: 0.939142\n","Train Epoch: 126 [16000/50000 (32%)]\tLoss: 1.280906\n","Train Epoch: 126 [24000/50000 (48%)]\tLoss: 0.659074\n","Train Epoch: 126 [32000/50000 (64%)]\tLoss: 0.587577\n","Train Epoch: 126 [40000/50000 (80%)]\tLoss: 0.748615\n","Train Epoch: 126 [48000/50000 (96%)]\tLoss: 0.388743\n","\n","Test set: Average loss: 0.0008, Accuracy: 7261/10000 (73%)\n","\n","Train Epoch: 127 [0/50000 (0%)]\tLoss: 1.086318\n","Train Epoch: 127 [8000/50000 (16%)]\tLoss: 0.728598\n","Train Epoch: 127 [16000/50000 (32%)]\tLoss: 0.937329\n","Train Epoch: 127 [24000/50000 (48%)]\tLoss: 1.082154\n","Train Epoch: 127 [32000/50000 (64%)]\tLoss: 0.772147\n","Train Epoch: 127 [40000/50000 (80%)]\tLoss: 0.677030\n","Train Epoch: 127 [48000/50000 (96%)]\tLoss: 0.628404\n","Train Epoch: 128 [0/50000 (0%)]\tLoss: 0.847254\n","Train Epoch: 128 [8000/50000 (16%)]\tLoss: 0.721189\n","Train Epoch: 128 [16000/50000 (32%)]\tLoss: 0.770373\n","Train Epoch: 128 [24000/50000 (48%)]\tLoss: 0.502237\n","Train Epoch: 128 [32000/50000 (64%)]\tLoss: 0.901777\n","Train Epoch: 128 [40000/50000 (80%)]\tLoss: 0.835090\n","Train Epoch: 128 [48000/50000 (96%)]\tLoss: 1.180260\n","\n","Test set: Average loss: 0.0008, Accuracy: 7204/10000 (72%)\n","\n","Train Epoch: 129 [0/50000 (0%)]\tLoss: 0.832610\n","Train Epoch: 129 [8000/50000 (16%)]\tLoss: 0.902188\n","Train Epoch: 129 [16000/50000 (32%)]\tLoss: 0.794275\n","Train Epoch: 129 [24000/50000 (48%)]\tLoss: 0.662772\n","Train Epoch: 129 [32000/50000 (64%)]\tLoss: 0.993836\n","Train Epoch: 129 [40000/50000 (80%)]\tLoss: 0.796517\n","Train Epoch: 129 [48000/50000 (96%)]\tLoss: 1.078408\n","Train Epoch: 130 [0/50000 (0%)]\tLoss: 0.994819\n","Train Epoch: 130 [8000/50000 (16%)]\tLoss: 0.788374\n","Train Epoch: 130 [16000/50000 (32%)]\tLoss: 0.863974\n","Train Epoch: 130 [24000/50000 (48%)]\tLoss: 1.114991\n","Train Epoch: 130 [32000/50000 (64%)]\tLoss: 0.764716\n","Train Epoch: 130 [40000/50000 (80%)]\tLoss: 0.748737\n","Train Epoch: 130 [48000/50000 (96%)]\tLoss: 0.944996\n","\n","Train set: Average loss: 0.0460, Accuracy: 37776/50000 (76%)\n","\n","\n","Test set: Average loss: 0.0008, Accuracy: 7310/10000 (73%)\n","\n","Train Epoch: 131 [0/50000 (0%)]\tLoss: 0.947519\n","Train Epoch: 131 [8000/50000 (16%)]\tLoss: 1.069362\n","Train Epoch: 131 [16000/50000 (32%)]\tLoss: 0.974814\n","Train Epoch: 131 [24000/50000 (48%)]\tLoss: 0.483189\n","Train Epoch: 131 [32000/50000 (64%)]\tLoss: 1.027070\n","Train Epoch: 131 [40000/50000 (80%)]\tLoss: 0.640775\n","Train Epoch: 131 [48000/50000 (96%)]\tLoss: 1.342342\n","Train Epoch: 132 [0/50000 (0%)]\tLoss: 0.681532\n","Train Epoch: 132 [8000/50000 (16%)]\tLoss: 0.966450\n","Train Epoch: 132 [16000/50000 (32%)]\tLoss: 1.215692\n","Train Epoch: 132 [24000/50000 (48%)]\tLoss: 0.860158\n","Train Epoch: 132 [32000/50000 (64%)]\tLoss: 0.587510\n","Train Epoch: 132 [40000/50000 (80%)]\tLoss: 0.887040\n","Train Epoch: 132 [48000/50000 (96%)]\tLoss: 0.923045\n","\n","Test set: Average loss: 0.0008, Accuracy: 7362/10000 (74%)\n","\n","Train Epoch: 133 [0/50000 (0%)]\tLoss: 0.511421\n","Train Epoch: 133 [8000/50000 (16%)]\tLoss: 1.058192\n","Train Epoch: 133 [16000/50000 (32%)]\tLoss: 0.627484\n","Train Epoch: 133 [24000/50000 (48%)]\tLoss: 0.686099\n","Train Epoch: 133 [32000/50000 (64%)]\tLoss: 1.390273\n","Train Epoch: 133 [40000/50000 (80%)]\tLoss: 0.884881\n","Train Epoch: 133 [48000/50000 (96%)]\tLoss: 1.098261\n","Train Epoch: 134 [0/50000 (0%)]\tLoss: 0.870931\n","Train Epoch: 134 [8000/50000 (16%)]\tLoss: 0.730033\n","Train Epoch: 134 [16000/50000 (32%)]\tLoss: 0.383394\n","Train Epoch: 134 [24000/50000 (48%)]\tLoss: 0.652171\n","Train Epoch: 134 [32000/50000 (64%)]\tLoss: 0.919973\n","Train Epoch: 134 [40000/50000 (80%)]\tLoss: 0.582317\n","Train Epoch: 134 [48000/50000 (96%)]\tLoss: 0.878199\n","\n","Test set: Average loss: 0.0008, Accuracy: 7381/10000 (74%)\n","\n","Train Epoch: 135 [0/50000 (0%)]\tLoss: 0.456032\n","Train Epoch: 135 [8000/50000 (16%)]\tLoss: 0.752002\n","Train Epoch: 135 [16000/50000 (32%)]\tLoss: 0.467874\n","Train Epoch: 135 [24000/50000 (48%)]\tLoss: 0.691626\n","Train Epoch: 135 [32000/50000 (64%)]\tLoss: 0.509780\n","Train Epoch: 135 [40000/50000 (80%)]\tLoss: 0.929681\n","Train Epoch: 135 [48000/50000 (96%)]\tLoss: 0.705625\n","Train Epoch: 136 [0/50000 (0%)]\tLoss: 0.727405\n","Train Epoch: 136 [8000/50000 (16%)]\tLoss: 0.680214\n","Train Epoch: 136 [16000/50000 (32%)]\tLoss: 0.471808\n","Train Epoch: 136 [24000/50000 (48%)]\tLoss: 0.544060\n","Train Epoch: 136 [32000/50000 (64%)]\tLoss: 1.017196\n","Train Epoch: 136 [40000/50000 (80%)]\tLoss: 0.852565\n","Train Epoch: 136 [48000/50000 (96%)]\tLoss: 1.068098\n","\n","Test set: Average loss: 0.0008, Accuracy: 7327/10000 (73%)\n","\n","Train Epoch: 137 [0/50000 (0%)]\tLoss: 0.824224\n","Train Epoch: 137 [8000/50000 (16%)]\tLoss: 1.178820\n","Train Epoch: 137 [16000/50000 (32%)]\tLoss: 0.752156\n","Train Epoch: 137 [24000/50000 (48%)]\tLoss: 1.474963\n","Train Epoch: 137 [32000/50000 (64%)]\tLoss: 0.576319\n","Train Epoch: 137 [40000/50000 (80%)]\tLoss: 0.826026\n","Train Epoch: 137 [48000/50000 (96%)]\tLoss: 0.973150\n","Train Epoch: 138 [0/50000 (0%)]\tLoss: 0.528640\n","Train Epoch: 138 [8000/50000 (16%)]\tLoss: 0.890492\n","Train Epoch: 138 [16000/50000 (32%)]\tLoss: 0.727228\n","Train Epoch: 138 [24000/50000 (48%)]\tLoss: 0.763183\n","Train Epoch: 138 [32000/50000 (64%)]\tLoss: 0.600069\n","Train Epoch: 138 [40000/50000 (80%)]\tLoss: 1.152232\n","Train Epoch: 138 [48000/50000 (96%)]\tLoss: 0.585128\n","\n","Test set: Average loss: 0.0008, Accuracy: 7348/10000 (73%)\n","\n","Train Epoch: 139 [0/50000 (0%)]\tLoss: 0.534744\n","Train Epoch: 139 [8000/50000 (16%)]\tLoss: 0.709190\n","Train Epoch: 139 [16000/50000 (32%)]\tLoss: 0.851979\n","Train Epoch: 139 [24000/50000 (48%)]\tLoss: 0.544953\n","Train Epoch: 139 [32000/50000 (64%)]\tLoss: 0.623304\n","Train Epoch: 139 [40000/50000 (80%)]\tLoss: 0.701727\n","Train Epoch: 139 [48000/50000 (96%)]\tLoss: 0.849631\n","Train Epoch: 140 [0/50000 (0%)]\tLoss: 0.542827\n","Train Epoch: 140 [8000/50000 (16%)]\tLoss: 0.551982\n","Train Epoch: 140 [16000/50000 (32%)]\tLoss: 1.177267\n","Train Epoch: 140 [24000/50000 (48%)]\tLoss: 0.245462\n","Train Epoch: 140 [32000/50000 (64%)]\tLoss: 0.425021\n","Train Epoch: 140 [40000/50000 (80%)]\tLoss: 0.404013\n","Train Epoch: 140 [48000/50000 (96%)]\tLoss: 0.374016\n","\n","Train set: Average loss: 0.0436, Accuracy: 38646/50000 (77%)\n","\n","\n","Test set: Average loss: 0.0008, Accuracy: 7450/10000 (74%)\n","\n","Freezing SupermaskLinear(in_features=256, out_features=256, bias=False, sparsity=0.5) before epoch 141\n","Train Epoch: 141 [0/50000 (0%)]\tLoss: 1.022713\n","Train Epoch: 141 [8000/50000 (16%)]\tLoss: 0.699401\n","Train Epoch: 141 [16000/50000 (32%)]\tLoss: 0.929954\n","Train Epoch: 141 [24000/50000 (48%)]\tLoss: 1.014631\n","Train Epoch: 141 [32000/50000 (64%)]\tLoss: 0.508088\n","Train Epoch: 141 [40000/50000 (80%)]\tLoss: 0.638759\n","Train Epoch: 141 [48000/50000 (96%)]\tLoss: 0.891266\n","Train Epoch: 142 [0/50000 (0%)]\tLoss: 0.725621\n","Train Epoch: 142 [8000/50000 (16%)]\tLoss: 0.913528\n","Train Epoch: 142 [16000/50000 (32%)]\tLoss: 0.719641\n","Train Epoch: 142 [24000/50000 (48%)]\tLoss: 0.701845\n","Train Epoch: 142 [32000/50000 (64%)]\tLoss: 0.767663\n","Train Epoch: 142 [40000/50000 (80%)]\tLoss: 0.811990\n","Train Epoch: 142 [48000/50000 (96%)]\tLoss: 0.835113\n","\n","Test set: Average loss: 0.0008, Accuracy: 7444/10000 (74%)\n","\n","Train Epoch: 143 [0/50000 (0%)]\tLoss: 0.732609\n","Train Epoch: 143 [8000/50000 (16%)]\tLoss: 0.666158\n","Train Epoch: 143 [16000/50000 (32%)]\tLoss: 1.258686\n","Train Epoch: 143 [24000/50000 (48%)]\tLoss: 0.793336\n","Train Epoch: 143 [32000/50000 (64%)]\tLoss: 1.346875\n","Train Epoch: 143 [40000/50000 (80%)]\tLoss: 1.026843\n","Train Epoch: 143 [48000/50000 (96%)]\tLoss: 0.879188\n","Train Epoch: 144 [0/50000 (0%)]\tLoss: 0.828169\n","Train Epoch: 144 [8000/50000 (16%)]\tLoss: 0.642884\n","Train Epoch: 144 [16000/50000 (32%)]\tLoss: 0.582968\n","Train Epoch: 144 [24000/50000 (48%)]\tLoss: 0.597514\n","Train Epoch: 144 [32000/50000 (64%)]\tLoss: 0.708577\n","Train Epoch: 144 [40000/50000 (80%)]\tLoss: 0.289476\n","Train Epoch: 144 [48000/50000 (96%)]\tLoss: 1.010798\n","\n","Test set: Average loss: 0.0008, Accuracy: 7398/10000 (74%)\n","\n","Train Epoch: 145 [0/50000 (0%)]\tLoss: 0.262377\n","Train Epoch: 145 [8000/50000 (16%)]\tLoss: 0.972315\n","Train Epoch: 145 [16000/50000 (32%)]\tLoss: 0.843164\n","Train Epoch: 145 [24000/50000 (48%)]\tLoss: 0.784165\n","Train Epoch: 145 [32000/50000 (64%)]\tLoss: 0.723184\n","Train Epoch: 145 [40000/50000 (80%)]\tLoss: 0.841766\n","Train Epoch: 145 [48000/50000 (96%)]\tLoss: 0.475753\n","Train Epoch: 146 [0/50000 (0%)]\tLoss: 1.024502\n","Train Epoch: 146 [8000/50000 (16%)]\tLoss: 0.742141\n","Train Epoch: 146 [16000/50000 (32%)]\tLoss: 0.751795\n","Train Epoch: 146 [24000/50000 (48%)]\tLoss: 0.585055\n","Train Epoch: 146 [32000/50000 (64%)]\tLoss: 0.893326\n","Train Epoch: 146 [40000/50000 (80%)]\tLoss: 1.015513\n","Train Epoch: 146 [48000/50000 (96%)]\tLoss: 0.343310\n","\n","Test set: Average loss: 0.0008, Accuracy: 7453/10000 (75%)\n","\n","Train Epoch: 147 [0/50000 (0%)]\tLoss: 0.691014\n","Train Epoch: 147 [8000/50000 (16%)]\tLoss: 1.225213\n","Train Epoch: 147 [16000/50000 (32%)]\tLoss: 0.885511\n","Train Epoch: 147 [24000/50000 (48%)]\tLoss: 0.629058\n","Train Epoch: 147 [32000/50000 (64%)]\tLoss: 0.696221\n","Train Epoch: 147 [40000/50000 (80%)]\tLoss: 0.695536\n","Train Epoch: 147 [48000/50000 (96%)]\tLoss: 1.113586\n","Train Epoch: 148 [0/50000 (0%)]\tLoss: 0.537212\n","Train Epoch: 148 [8000/50000 (16%)]\tLoss: 0.878110\n","Train Epoch: 148 [16000/50000 (32%)]\tLoss: 0.685205\n","Train Epoch: 148 [24000/50000 (48%)]\tLoss: 0.673773\n","Train Epoch: 148 [32000/50000 (64%)]\tLoss: 0.580025\n","Train Epoch: 148 [40000/50000 (80%)]\tLoss: 0.820117\n","Train Epoch: 148 [48000/50000 (96%)]\tLoss: 0.769342\n","\n","Test set: Average loss: 0.0008, Accuracy: 7450/10000 (74%)\n","\n","Train Epoch: 149 [0/50000 (0%)]\tLoss: 0.569613\n","Train Epoch: 149 [8000/50000 (16%)]\tLoss: 0.918313\n","Train Epoch: 149 [16000/50000 (32%)]\tLoss: 0.758219\n","Train Epoch: 149 [24000/50000 (48%)]\tLoss: 0.972468\n","Train Epoch: 149 [32000/50000 (64%)]\tLoss: 0.780045\n","Train Epoch: 149 [40000/50000 (80%)]\tLoss: 1.008336\n","Train Epoch: 149 [48000/50000 (96%)]\tLoss: 0.520446\n","Train Epoch: 150 [0/50000 (0%)]\tLoss: 0.572006\n","Train Epoch: 150 [8000/50000 (16%)]\tLoss: 0.848217\n","Train Epoch: 150 [16000/50000 (32%)]\tLoss: 0.845291\n","Train Epoch: 150 [24000/50000 (48%)]\tLoss: 0.811385\n","Train Epoch: 150 [32000/50000 (64%)]\tLoss: 0.953715\n","Train Epoch: 150 [40000/50000 (80%)]\tLoss: 1.067344\n","Train Epoch: 150 [48000/50000 (96%)]\tLoss: 0.583937\n","\n","Train set: Average loss: 0.0427, Accuracy: 38839/50000 (78%)\n","\n","\n","Test set: Average loss: 0.0008, Accuracy: 7467/10000 (75%)\n","\n","Train Epoch: 151 [0/50000 (0%)]\tLoss: 1.081350\n","Train Epoch: 151 [8000/50000 (16%)]\tLoss: 1.021466\n","Train Epoch: 151 [16000/50000 (32%)]\tLoss: 0.618871\n","Train Epoch: 151 [24000/50000 (48%)]\tLoss: 0.718810\n","Train Epoch: 151 [32000/50000 (64%)]\tLoss: 0.868701\n","Train Epoch: 151 [40000/50000 (80%)]\tLoss: 0.632211\n","Train Epoch: 151 [48000/50000 (96%)]\tLoss: 0.860479\n","Train Epoch: 152 [0/50000 (0%)]\tLoss: 0.984178\n","Train Epoch: 152 [8000/50000 (16%)]\tLoss: 0.539756\n","Train Epoch: 152 [16000/50000 (32%)]\tLoss: 0.440334\n","Train Epoch: 152 [24000/50000 (48%)]\tLoss: 0.662109\n","Train Epoch: 152 [32000/50000 (64%)]\tLoss: 1.108983\n","Train Epoch: 152 [40000/50000 (80%)]\tLoss: 0.733410\n","Train Epoch: 152 [48000/50000 (96%)]\tLoss: 0.718818\n","\n","Test set: Average loss: 0.0008, Accuracy: 7482/10000 (75%)\n","\n","Train Epoch: 153 [0/50000 (0%)]\tLoss: 0.654988\n","Train Epoch: 153 [8000/50000 (16%)]\tLoss: 0.663324\n","Train Epoch: 153 [16000/50000 (32%)]\tLoss: 0.897262\n","Train Epoch: 153 [24000/50000 (48%)]\tLoss: 0.566162\n","Train Epoch: 153 [32000/50000 (64%)]\tLoss: 0.836871\n","Train Epoch: 153 [40000/50000 (80%)]\tLoss: 0.430202\n","Train Epoch: 153 [48000/50000 (96%)]\tLoss: 0.948932\n","Train Epoch: 154 [0/50000 (0%)]\tLoss: 0.647572\n","Train Epoch: 154 [8000/50000 (16%)]\tLoss: 0.702274\n","Train Epoch: 154 [16000/50000 (32%)]\tLoss: 0.966421\n","Train Epoch: 154 [24000/50000 (48%)]\tLoss: 1.096006\n","Train Epoch: 154 [32000/50000 (64%)]\tLoss: 0.994497\n","Train Epoch: 154 [40000/50000 (80%)]\tLoss: 0.506739\n","Train Epoch: 154 [48000/50000 (96%)]\tLoss: 0.879648\n","\n","Test set: Average loss: 0.0008, Accuracy: 7484/10000 (75%)\n","\n","Train Epoch: 155 [0/50000 (0%)]\tLoss: 0.942480\n","Train Epoch: 155 [8000/50000 (16%)]\tLoss: 0.519463\n","Train Epoch: 155 [16000/50000 (32%)]\tLoss: 0.466175\n","Train Epoch: 155 [24000/50000 (48%)]\tLoss: 0.638174\n","Train Epoch: 155 [32000/50000 (64%)]\tLoss: 0.636415\n","Train Epoch: 155 [40000/50000 (80%)]\tLoss: 0.433036\n","Train Epoch: 155 [48000/50000 (96%)]\tLoss: 0.356839\n","Train Epoch: 156 [0/50000 (0%)]\tLoss: 0.858885\n","Train Epoch: 156 [8000/50000 (16%)]\tLoss: 0.527098\n","Train Epoch: 156 [16000/50000 (32%)]\tLoss: 0.581762\n","Train Epoch: 156 [24000/50000 (48%)]\tLoss: 0.480045\n","Train Epoch: 156 [32000/50000 (64%)]\tLoss: 0.613519\n","Train Epoch: 156 [40000/50000 (80%)]\tLoss: 0.555998\n","Train Epoch: 156 [48000/50000 (96%)]\tLoss: 0.732880\n","\n","Test set: Average loss: 0.0008, Accuracy: 7508/10000 (75%)\n","\n","Train Epoch: 157 [0/50000 (0%)]\tLoss: 1.063190\n","Train Epoch: 157 [8000/50000 (16%)]\tLoss: 0.645140\n","Train Epoch: 157 [16000/50000 (32%)]\tLoss: 0.910381\n","Train Epoch: 157 [24000/50000 (48%)]\tLoss: 0.616082\n","Train Epoch: 157 [32000/50000 (64%)]\tLoss: 0.332851\n","Train Epoch: 157 [40000/50000 (80%)]\tLoss: 0.856009\n","Train Epoch: 157 [48000/50000 (96%)]\tLoss: 0.538785\n","Train Epoch: 158 [0/50000 (0%)]\tLoss: 0.831970\n","Train Epoch: 158 [8000/50000 (16%)]\tLoss: 1.122530\n","Train Epoch: 158 [16000/50000 (32%)]\tLoss: 0.709682\n","Train Epoch: 158 [24000/50000 (48%)]\tLoss: 0.571162\n","Train Epoch: 158 [32000/50000 (64%)]\tLoss: 0.953075\n","Train Epoch: 158 [40000/50000 (80%)]\tLoss: 0.872753\n","Train Epoch: 158 [48000/50000 (96%)]\tLoss: 0.644655\n","\n","Test set: Average loss: 0.0008, Accuracy: 7508/10000 (75%)\n","\n","Train Epoch: 159 [0/50000 (0%)]\tLoss: 0.648657\n","Train Epoch: 159 [8000/50000 (16%)]\tLoss: 0.967826\n","Train Epoch: 159 [16000/50000 (32%)]\tLoss: 1.120144\n","Train Epoch: 159 [24000/50000 (48%)]\tLoss: 0.944476\n","Train Epoch: 159 [32000/50000 (64%)]\tLoss: 0.710940\n","Train Epoch: 159 [40000/50000 (80%)]\tLoss: 0.565326\n","Train Epoch: 159 [48000/50000 (96%)]\tLoss: 0.425692\n","Train Epoch: 160 [0/50000 (0%)]\tLoss: 0.830020\n","Train Epoch: 160 [8000/50000 (16%)]\tLoss: 0.836515\n","Train Epoch: 160 [16000/50000 (32%)]\tLoss: 0.821539\n","Train Epoch: 160 [24000/50000 (48%)]\tLoss: 0.711906\n","Train Epoch: 160 [32000/50000 (64%)]\tLoss: 0.432688\n","Train Epoch: 160 [40000/50000 (80%)]\tLoss: 0.371138\n","Train Epoch: 160 [48000/50000 (96%)]\tLoss: 0.982396\n","\n","Train set: Average loss: 0.0424, Accuracy: 38959/50000 (78%)\n","\n","\n","Test set: Average loss: 0.0008, Accuracy: 7507/10000 (75%)\n","\n"]}]},{"cell_type":"code","source":["from updated_penalized_supermask_pruning import GetSubnet, SupermaskConv, SupermaskLinear\n","from updated_penalized_supermask_pruning import train, test\n","\n","# # Arguments that do not affect model at all\n","train_args = {\n","    \"test_batch_size\": 1000, # input batch size for testing (default: 1000)\n","    'data': '../data', # Location to store data (e.g. MNIST)\n","    'log_interval': 500, # how many batches to wait before logging training status\n","    'train_eval_interval': 10, # epoch interval at which to print training accuracy\n","    'test_eval_interval': 2, # epoch interval at which to print test accuracy\n","    'eval_on_last': True\n","}\n","\n","args = {\n","  \"dataset\": \"CIFAR10\",\n","  \"init\": \"signed_constant\",\n","  \"batch_size\": 16, # input batch size for training (default: 64)\n","  \"epochs\": 160, # number of epochs to train (default: 14)\n","  \"optimizer\": \"SGD\",\n","  \"optim_kwargs\": {\"lr\": 0.1, \"momentum\": 0.9, \"weight_decay\": 0.0001},\n","  \"scheduler\": True, # False for Adam, True for SGD, does CosineAnnealing\n","  'no_cuda': False, # disables CUDA training\n","  'seed': 1000, # random seed (default: 1)\n","  'save_name': None, #\"conv2_frozen_sp50_rs1000\", # \"simple20_rs2\", # For Saving the current Model, None if not saving\n","  'sparsity': [{\"sparsity\": 0.5}, {\"sparsity\": 0.5}, {\"sparsity\": 0.5}, {\"sparsity\": 0.5}, {\"sparsity\": 0.5}], # 'how sparse is each layer'\n","  'copy_layers': [], # ['conv1', 'conv2', 'fc2'],\n","  'bias': False, \n","  'score_penalty': 50\n","}\n","\n","trained_model, device, train_loader, test_loader, criterion = main(args, train_args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["8ec8df6f527248dc8dda33e1902a5b86","c983ab896b7a44799ecb4b09bdbd2783","69495c28adf843f98c390cbd6fb772c8","17604f4770014a7f8e6f6c00ffe5b023","a40f7009acb04e7d9a537307190fae5d","98888f1c11c342e0938f832893b7421f","4fd2b913a63f4c4baf9e9f86c115c804","e8afc44401aa4fb6a8d08e221ae9cf94","a464caaf80f64b70b3c610a3c59ec479","c10738d574c94da186b76bd925b4c83c","38b2adb324394ad49280fe25a5b2f644"]},"id":"WE6TlzpsUSEV","executionInfo":{"status":"error","timestamp":1651012285549,"user_tz":240,"elapsed":3416012,"user":{"displayName":"Neehal Tumma","userId":"02469198182259025401"}},"outputId":"2070c8e8-2c38-45ee-dae1-1644af858616"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device cuda\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/CIFAR10/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ec8df6f527248dc8dda33e1902a5b86"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ../data/CIFAR10/cifar-10-python.tar.gz to ../data/CIFAR10\n","Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.295848\n","Train Epoch: 1 [8000/50000 (16%)]\tLoss: 2.002564\n","Train Epoch: 1 [16000/50000 (32%)]\tLoss: 1.931832\n","Train Epoch: 1 [24000/50000 (48%)]\tLoss: 1.517643\n","Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.714643\n","Train Epoch: 1 [40000/50000 (80%)]\tLoss: 1.641865\n","Train Epoch: 1 [48000/50000 (96%)]\tLoss: 1.103495\n","Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.284589\n","Train Epoch: 2 [8000/50000 (16%)]\tLoss: 1.196224\n","Train Epoch: 2 [16000/50000 (32%)]\tLoss: 1.352911\n","Train Epoch: 2 [24000/50000 (48%)]\tLoss: 1.327775\n","Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.523379\n","Train Epoch: 2 [40000/50000 (80%)]\tLoss: 1.588583\n","Train Epoch: 2 [48000/50000 (96%)]\tLoss: 1.776922\n","\n","Test set: Average loss: 0.0013, Accuracy: 5301/10000 (53%)\n","\n","Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.139934\n","Train Epoch: 3 [8000/50000 (16%)]\tLoss: 1.740448\n","Train Epoch: 3 [16000/50000 (32%)]\tLoss: 1.308157\n","Train Epoch: 3 [24000/50000 (48%)]\tLoss: 1.285305\n","Train Epoch: 3 [32000/50000 (64%)]\tLoss: 2.106724\n","Train Epoch: 3 [40000/50000 (80%)]\tLoss: 1.417606\n","Train Epoch: 3 [48000/50000 (96%)]\tLoss: 1.456271\n","Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.214461\n","Train Epoch: 4 [8000/50000 (16%)]\tLoss: 1.419687\n","Train Epoch: 4 [16000/50000 (32%)]\tLoss: 1.257220\n","Train Epoch: 4 [24000/50000 (48%)]\tLoss: 1.736079\n","Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1.269007\n","Train Epoch: 4 [40000/50000 (80%)]\tLoss: 1.865748\n","Train Epoch: 4 [48000/50000 (96%)]\tLoss: 1.424067\n","\n","Test set: Average loss: 0.0014, Accuracy: 4917/10000 (49%)\n","\n","Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.281807\n","Train Epoch: 5 [8000/50000 (16%)]\tLoss: 1.270545\n","Train Epoch: 5 [16000/50000 (32%)]\tLoss: 1.657611\n","Train Epoch: 5 [24000/50000 (48%)]\tLoss: 1.061661\n","Train Epoch: 5 [32000/50000 (64%)]\tLoss: 1.563033\n","Train Epoch: 5 [40000/50000 (80%)]\tLoss: 1.856712\n","Train Epoch: 5 [48000/50000 (96%)]\tLoss: 0.892814\n","Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.650287\n","Train Epoch: 6 [8000/50000 (16%)]\tLoss: 1.391502\n","Train Epoch: 6 [16000/50000 (32%)]\tLoss: 0.787202\n","Train Epoch: 6 [24000/50000 (48%)]\tLoss: 1.737900\n","Train Epoch: 6 [32000/50000 (64%)]\tLoss: 1.525990\n","Train Epoch: 6 [40000/50000 (80%)]\tLoss: 1.089051\n","Train Epoch: 6 [48000/50000 (96%)]\tLoss: 1.338425\n","\n","Test set: Average loss: 0.0012, Accuracy: 5877/10000 (59%)\n","\n","Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.751687\n","Train Epoch: 7 [8000/50000 (16%)]\tLoss: 0.974181\n","Train Epoch: 7 [16000/50000 (32%)]\tLoss: 1.129653\n","Train Epoch: 7 [24000/50000 (48%)]\tLoss: 1.291665\n","Train Epoch: 7 [32000/50000 (64%)]\tLoss: 1.207281\n","Train Epoch: 7 [40000/50000 (80%)]\tLoss: 1.191701\n","Train Epoch: 7 [48000/50000 (96%)]\tLoss: 0.856132\n","Train Epoch: 8 [0/50000 (0%)]\tLoss: 1.276402\n","Train Epoch: 8 [8000/50000 (16%)]\tLoss: 1.157779\n","Train Epoch: 8 [16000/50000 (32%)]\tLoss: 1.102745\n","Train Epoch: 8 [24000/50000 (48%)]\tLoss: 1.406841\n","Train Epoch: 8 [32000/50000 (64%)]\tLoss: 1.291117\n","Train Epoch: 8 [40000/50000 (80%)]\tLoss: 1.296453\n","Train Epoch: 8 [48000/50000 (96%)]\tLoss: 1.385846\n","\n","Test set: Average loss: 0.0012, Accuracy: 5745/10000 (57%)\n","\n","Train Epoch: 9 [0/50000 (0%)]\tLoss: 1.240303\n","Train Epoch: 9 [8000/50000 (16%)]\tLoss: 1.345048\n","Train Epoch: 9 [16000/50000 (32%)]\tLoss: 1.413214\n","Train Epoch: 9 [24000/50000 (48%)]\tLoss: 1.032560\n","Train Epoch: 9 [32000/50000 (64%)]\tLoss: 1.231188\n","Train Epoch: 9 [40000/50000 (80%)]\tLoss: 1.846008\n","Train Epoch: 9 [48000/50000 (96%)]\tLoss: 1.345373\n","Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.833161\n","Train Epoch: 10 [8000/50000 (16%)]\tLoss: 1.143620\n","Train Epoch: 10 [16000/50000 (32%)]\tLoss: 1.494735\n","Train Epoch: 10 [24000/50000 (48%)]\tLoss: 0.956937\n","Train Epoch: 10 [32000/50000 (64%)]\tLoss: 1.168053\n","Train Epoch: 10 [40000/50000 (80%)]\tLoss: 1.223649\n","Train Epoch: 10 [48000/50000 (96%)]\tLoss: 1.543985\n","\n","Train set: Average loss: 0.0772, Accuracy: 28398/50000 (57%)\n","\n","\n","Test set: Average loss: 0.0013, Accuracy: 5632/10000 (56%)\n","\n","Train Epoch: 11 [0/50000 (0%)]\tLoss: 1.238800\n","Train Epoch: 11 [8000/50000 (16%)]\tLoss: 1.525556\n","Train Epoch: 11 [16000/50000 (32%)]\tLoss: 0.996577\n","Train Epoch: 11 [24000/50000 (48%)]\tLoss: 1.498219\n","Train Epoch: 11 [32000/50000 (64%)]\tLoss: 1.638646\n","Train Epoch: 11 [40000/50000 (80%)]\tLoss: 1.476475\n","Train Epoch: 11 [48000/50000 (96%)]\tLoss: 1.011896\n","Train Epoch: 12 [0/50000 (0%)]\tLoss: 1.327849\n","Train Epoch: 12 [8000/50000 (16%)]\tLoss: 1.067229\n","Train Epoch: 12 [16000/50000 (32%)]\tLoss: 1.673727\n","Train Epoch: 12 [24000/50000 (48%)]\tLoss: 1.052252\n","Train Epoch: 12 [32000/50000 (64%)]\tLoss: 2.156123\n","Train Epoch: 12 [40000/50000 (80%)]\tLoss: 0.869137\n","Train Epoch: 12 [48000/50000 (96%)]\tLoss: 1.223445\n","\n","Test set: Average loss: 0.0012, Accuracy: 5659/10000 (57%)\n","\n","Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.989163\n","Train Epoch: 13 [8000/50000 (16%)]\tLoss: 0.847882\n","Train Epoch: 13 [16000/50000 (32%)]\tLoss: 1.621353\n","Train Epoch: 13 [24000/50000 (48%)]\tLoss: 1.521416\n","Train Epoch: 13 [32000/50000 (64%)]\tLoss: 0.965955\n","Train Epoch: 13 [40000/50000 (80%)]\tLoss: 1.483765\n","Train Epoch: 13 [48000/50000 (96%)]\tLoss: 1.322438\n","Train Epoch: 14 [0/50000 (0%)]\tLoss: 1.108711\n","Train Epoch: 14 [8000/50000 (16%)]\tLoss: 1.004010\n","Train Epoch: 14 [16000/50000 (32%)]\tLoss: 1.284347\n","Train Epoch: 14 [24000/50000 (48%)]\tLoss: 0.968623\n","Train Epoch: 14 [32000/50000 (64%)]\tLoss: 1.148414\n","Train Epoch: 14 [40000/50000 (80%)]\tLoss: 1.196547\n","Train Epoch: 14 [48000/50000 (96%)]\tLoss: 1.070275\n","\n","Test set: Average loss: 0.0011, Accuracy: 6028/10000 (60%)\n","\n","Train Epoch: 15 [0/50000 (0%)]\tLoss: 1.085354\n","Train Epoch: 15 [8000/50000 (16%)]\tLoss: 0.925435\n","Train Epoch: 15 [16000/50000 (32%)]\tLoss: 1.155498\n","Train Epoch: 15 [24000/50000 (48%)]\tLoss: 1.278740\n","Train Epoch: 15 [32000/50000 (64%)]\tLoss: 1.072701\n","Train Epoch: 15 [40000/50000 (80%)]\tLoss: 1.584888\n","Train Epoch: 15 [48000/50000 (96%)]\tLoss: 1.097362\n","Train Epoch: 16 [0/50000 (0%)]\tLoss: 1.080277\n","Train Epoch: 16 [8000/50000 (16%)]\tLoss: 1.322693\n","Train Epoch: 16 [16000/50000 (32%)]\tLoss: 1.184269\n","Train Epoch: 16 [24000/50000 (48%)]\tLoss: 1.257091\n","Train Epoch: 16 [32000/50000 (64%)]\tLoss: 2.370225\n","Train Epoch: 16 [40000/50000 (80%)]\tLoss: 1.134533\n","Train Epoch: 16 [48000/50000 (96%)]\tLoss: 1.518736\n","\n","Test set: Average loss: 0.0012, Accuracy: 5955/10000 (60%)\n","\n","Train Epoch: 17 [0/50000 (0%)]\tLoss: 1.022628\n","Train Epoch: 17 [8000/50000 (16%)]\tLoss: 1.170894\n","Train Epoch: 17 [16000/50000 (32%)]\tLoss: 1.551910\n","Train Epoch: 17 [24000/50000 (48%)]\tLoss: 1.084145\n","Train Epoch: 17 [32000/50000 (64%)]\tLoss: 1.056733\n","Train Epoch: 17 [40000/50000 (80%)]\tLoss: 1.033871\n","Train Epoch: 17 [48000/50000 (96%)]\tLoss: 1.675839\n","Train Epoch: 18 [0/50000 (0%)]\tLoss: 1.317522\n","Train Epoch: 18 [8000/50000 (16%)]\tLoss: 0.934041\n","Train Epoch: 18 [16000/50000 (32%)]\tLoss: 0.844240\n","Train Epoch: 18 [24000/50000 (48%)]\tLoss: 1.165390\n","Train Epoch: 18 [32000/50000 (64%)]\tLoss: 1.294934\n","Train Epoch: 18 [40000/50000 (80%)]\tLoss: 1.214238\n","Train Epoch: 18 [48000/50000 (96%)]\tLoss: 1.638722\n","\n","Test set: Average loss: 0.0012, Accuracy: 5734/10000 (57%)\n","\n","Train Epoch: 19 [0/50000 (0%)]\tLoss: 1.572421\n","Train Epoch: 19 [8000/50000 (16%)]\tLoss: 1.198787\n","Train Epoch: 19 [16000/50000 (32%)]\tLoss: 1.188559\n","Train Epoch: 19 [24000/50000 (48%)]\tLoss: 1.281863\n","Train Epoch: 19 [32000/50000 (64%)]\tLoss: 1.390605\n","Train Epoch: 19 [40000/50000 (80%)]\tLoss: 1.506631\n","Train Epoch: 19 [48000/50000 (96%)]\tLoss: 0.903596\n","Train Epoch: 20 [0/50000 (0%)]\tLoss: 1.453740\n","Train Epoch: 20 [8000/50000 (16%)]\tLoss: 1.109839\n","Train Epoch: 20 [16000/50000 (32%)]\tLoss: 1.137112\n","Train Epoch: 20 [24000/50000 (48%)]\tLoss: 1.248461\n","Train Epoch: 20 [32000/50000 (64%)]\tLoss: 1.559487\n","Train Epoch: 20 [40000/50000 (80%)]\tLoss: 1.191764\n","Train Epoch: 20 [48000/50000 (96%)]\tLoss: 1.102488\n","\n","Train set: Average loss: 0.0704, Accuracy: 30201/50000 (60%)\n","\n","\n","Test set: Average loss: 0.0011, Accuracy: 5953/10000 (60%)\n","\n","Train Epoch: 21 [0/50000 (0%)]\tLoss: 0.752470\n","Train Epoch: 21 [8000/50000 (16%)]\tLoss: 1.078835\n","Train Epoch: 21 [16000/50000 (32%)]\tLoss: 1.239516\n","Train Epoch: 21 [24000/50000 (48%)]\tLoss: 1.111758\n","Train Epoch: 21 [32000/50000 (64%)]\tLoss: 1.395757\n","Train Epoch: 21 [40000/50000 (80%)]\tLoss: 1.196792\n","Train Epoch: 21 [48000/50000 (96%)]\tLoss: 1.363150\n","Train Epoch: 22 [0/50000 (0%)]\tLoss: 1.153471\n","Train Epoch: 22 [8000/50000 (16%)]\tLoss: 0.933971\n","Train Epoch: 22 [16000/50000 (32%)]\tLoss: 1.324612\n","Train Epoch: 22 [24000/50000 (48%)]\tLoss: 1.667988\n","Train Epoch: 22 [32000/50000 (64%)]\tLoss: 1.397909\n","Train Epoch: 22 [40000/50000 (80%)]\tLoss: 1.146871\n","Train Epoch: 22 [48000/50000 (96%)]\tLoss: 0.984286\n","\n","Test set: Average loss: 0.0012, Accuracy: 5846/10000 (58%)\n","\n","Train Epoch: 23 [0/50000 (0%)]\tLoss: 1.317073\n","Train Epoch: 23 [8000/50000 (16%)]\tLoss: 1.274241\n","Train Epoch: 23 [16000/50000 (32%)]\tLoss: 1.321073\n","Train Epoch: 23 [24000/50000 (48%)]\tLoss: 0.779052\n","Train Epoch: 23 [32000/50000 (64%)]\tLoss: 1.718598\n","Train Epoch: 23 [40000/50000 (80%)]\tLoss: 1.213748\n","Train Epoch: 23 [48000/50000 (96%)]\tLoss: 1.373537\n","Train Epoch: 24 [0/50000 (0%)]\tLoss: 1.040555\n","Train Epoch: 24 [8000/50000 (16%)]\tLoss: 1.162464\n","Train Epoch: 24 [16000/50000 (32%)]\tLoss: 0.965215\n","Train Epoch: 24 [24000/50000 (48%)]\tLoss: 1.169916\n","Train Epoch: 24 [32000/50000 (64%)]\tLoss: 1.459135\n","Train Epoch: 24 [40000/50000 (80%)]\tLoss: 1.235782\n","Train Epoch: 24 [48000/50000 (96%)]\tLoss: 1.353231\n","\n","Test set: Average loss: 0.0012, Accuracy: 5992/10000 (60%)\n","\n","Train Epoch: 25 [0/50000 (0%)]\tLoss: 0.831385\n","Train Epoch: 25 [8000/50000 (16%)]\tLoss: 1.564818\n","Train Epoch: 25 [16000/50000 (32%)]\tLoss: 1.194726\n","Train Epoch: 25 [24000/50000 (48%)]\tLoss: 1.282039\n","Train Epoch: 25 [32000/50000 (64%)]\tLoss: 1.550188\n","Train Epoch: 25 [40000/50000 (80%)]\tLoss: 0.907694\n","Train Epoch: 25 [48000/50000 (96%)]\tLoss: 0.904881\n","Train Epoch: 26 [0/50000 (0%)]\tLoss: 1.333811\n","Train Epoch: 26 [8000/50000 (16%)]\tLoss: 1.007199\n","Train Epoch: 26 [16000/50000 (32%)]\tLoss: 1.466829\n","Train Epoch: 26 [24000/50000 (48%)]\tLoss: 1.486539\n","Train Epoch: 26 [32000/50000 (64%)]\tLoss: 1.063831\n","Train Epoch: 26 [40000/50000 (80%)]\tLoss: 1.307246\n","Train Epoch: 26 [48000/50000 (96%)]\tLoss: 0.880784\n","\n","Test set: Average loss: 0.0011, Accuracy: 6247/10000 (62%)\n","\n","Train Epoch: 27 [0/50000 (0%)]\tLoss: 1.262242\n","Train Epoch: 27 [8000/50000 (16%)]\tLoss: 0.830903\n","Train Epoch: 27 [16000/50000 (32%)]\tLoss: 1.348051\n","Train Epoch: 27 [24000/50000 (48%)]\tLoss: 0.993441\n","Train Epoch: 27 [32000/50000 (64%)]\tLoss: 1.587900\n","Train Epoch: 27 [40000/50000 (80%)]\tLoss: 1.434984\n","Train Epoch: 27 [48000/50000 (96%)]\tLoss: 0.740685\n","Train Epoch: 28 [0/50000 (0%)]\tLoss: 1.311175\n","Train Epoch: 28 [8000/50000 (16%)]\tLoss: 0.924293\n","Train Epoch: 28 [16000/50000 (32%)]\tLoss: 1.010597\n","Train Epoch: 28 [24000/50000 (48%)]\tLoss: 1.484216\n","Train Epoch: 28 [32000/50000 (64%)]\tLoss: 1.616319\n","Train Epoch: 28 [40000/50000 (80%)]\tLoss: 1.002969\n","Train Epoch: 28 [48000/50000 (96%)]\tLoss: 1.252230\n","\n","Test set: Average loss: 0.0012, Accuracy: 5767/10000 (58%)\n","\n","Train Epoch: 29 [0/50000 (0%)]\tLoss: 1.158404\n","Train Epoch: 29 [8000/50000 (16%)]\tLoss: 1.359251\n","Train Epoch: 29 [16000/50000 (32%)]\tLoss: 1.377471\n","Train Epoch: 29 [24000/50000 (48%)]\tLoss: 1.294822\n","Train Epoch: 29 [32000/50000 (64%)]\tLoss: 1.513078\n","Train Epoch: 29 [40000/50000 (80%)]\tLoss: 1.589693\n","Train Epoch: 29 [48000/50000 (96%)]\tLoss: 1.504528\n","Train Epoch: 30 [0/50000 (0%)]\tLoss: 1.184741\n","Train Epoch: 30 [8000/50000 (16%)]\tLoss: 1.201903\n","Train Epoch: 30 [16000/50000 (32%)]\tLoss: 1.526503\n","Train Epoch: 30 [24000/50000 (48%)]\tLoss: 1.505147\n","Train Epoch: 30 [32000/50000 (64%)]\tLoss: 1.366685\n","Train Epoch: 30 [40000/50000 (80%)]\tLoss: 1.104377\n","Train Epoch: 30 [48000/50000 (96%)]\tLoss: 0.931476\n","\n","Train set: Average loss: 0.0707, Accuracy: 30224/50000 (60%)\n","\n","\n","Test set: Average loss: 0.0011, Accuracy: 6010/10000 (60%)\n","\n","Train Epoch: 31 [0/50000 (0%)]\tLoss: 0.776262\n","Train Epoch: 31 [8000/50000 (16%)]\tLoss: 0.825958\n","Train Epoch: 31 [16000/50000 (32%)]\tLoss: 1.357833\n","Train Epoch: 31 [24000/50000 (48%)]\tLoss: 1.021222\n","Train Epoch: 31 [32000/50000 (64%)]\tLoss: 1.198503\n","Train Epoch: 31 [40000/50000 (80%)]\tLoss: 1.129912\n","Train Epoch: 31 [48000/50000 (96%)]\tLoss: 1.448597\n","Train Epoch: 32 [0/50000 (0%)]\tLoss: 0.670459\n","Train Epoch: 32 [8000/50000 (16%)]\tLoss: 1.620692\n","Train Epoch: 32 [16000/50000 (32%)]\tLoss: 0.934873\n","Train Epoch: 32 [24000/50000 (48%)]\tLoss: 0.961211\n","Train Epoch: 32 [32000/50000 (64%)]\tLoss: 1.002867\n","Train Epoch: 32 [40000/50000 (80%)]\tLoss: 1.508777\n","Train Epoch: 32 [48000/50000 (96%)]\tLoss: 1.619573\n","\n","Test set: Average loss: 0.0011, Accuracy: 6233/10000 (62%)\n","\n","Train Epoch: 33 [0/50000 (0%)]\tLoss: 1.209967\n","Train Epoch: 33 [8000/50000 (16%)]\tLoss: 1.381502\n","Train Epoch: 33 [16000/50000 (32%)]\tLoss: 1.704386\n","Train Epoch: 33 [24000/50000 (48%)]\tLoss: 1.243585\n","Train Epoch: 33 [32000/50000 (64%)]\tLoss: 1.144237\n","Train Epoch: 33 [40000/50000 (80%)]\tLoss: 1.037750\n","Train Epoch: 33 [48000/50000 (96%)]\tLoss: 0.986456\n","Train Epoch: 34 [0/50000 (0%)]\tLoss: 1.415663\n","Train Epoch: 34 [8000/50000 (16%)]\tLoss: 1.624721\n","Train Epoch: 34 [16000/50000 (32%)]\tLoss: 0.835718\n","Train Epoch: 34 [24000/50000 (48%)]\tLoss: 1.060428\n","Train Epoch: 34 [32000/50000 (64%)]\tLoss: 1.369743\n","Train Epoch: 34 [40000/50000 (80%)]\tLoss: 0.974646\n","Train Epoch: 34 [48000/50000 (96%)]\tLoss: 1.428754\n","\n","Test set: Average loss: 0.0011, Accuracy: 6194/10000 (62%)\n","\n","Train Epoch: 35 [0/50000 (0%)]\tLoss: 0.988449\n","Train Epoch: 35 [8000/50000 (16%)]\tLoss: 1.087997\n","Train Epoch: 35 [16000/50000 (32%)]\tLoss: 1.192782\n","Train Epoch: 35 [24000/50000 (48%)]\tLoss: 1.490885\n","Train Epoch: 35 [32000/50000 (64%)]\tLoss: 1.433260\n","Train Epoch: 35 [40000/50000 (80%)]\tLoss: 0.909248\n","Train Epoch: 35 [48000/50000 (96%)]\tLoss: 1.158420\n","Train Epoch: 36 [0/50000 (0%)]\tLoss: 1.021747\n","Train Epoch: 36 [8000/50000 (16%)]\tLoss: 1.148176\n","Train Epoch: 36 [16000/50000 (32%)]\tLoss: 1.515598\n","Train Epoch: 36 [24000/50000 (48%)]\tLoss: 0.731095\n","Train Epoch: 36 [32000/50000 (64%)]\tLoss: 0.926387\n","Train Epoch: 36 [40000/50000 (80%)]\tLoss: 1.142694\n","Train Epoch: 36 [48000/50000 (96%)]\tLoss: 1.116877\n","\n","Test set: Average loss: 0.0011, Accuracy: 6335/10000 (63%)\n","\n","Train Epoch: 37 [0/50000 (0%)]\tLoss: 1.010609\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-ded48afe34bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m }\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-6-b20155abdf9f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(model_args, train_args, base_model, trial)\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Freezing {child} before epoch {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_augmented_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score_penalty'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_eval_interval\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtrain_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_eval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_on_last\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/hidden-networks/updated_penalized_supermask_pruning.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, log_interval, device, train_loader, optimizer, criterion, epoch, penalty)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["torch.cuda.get_device_name(0)"],"metadata":{"id":"Nhi2UZWWAKpf","executionInfo":{"status":"ok","timestamp":1651012320079,"user_tz":240,"elapsed":40,"user":{"displayName":"Neehal Tumma","userId":"02469198182259025401"}},"outputId":"fb2146bc-f1ba-4905-8319-38b683c70aa3","colab":{"base_uri":"https://localhost:8080/","height":35}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Tesla K80'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cSyyaEt6X8Hg","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["58fd0f238bcf4cb2b676ae599011d870","89eec571be8b4cb0846dc43a9ca478d3","a30c3c6950874ab394886c3978652a41","6957175891a54cea9db514da790853ed","5a0feea754854790a0d9f0f9275ef8bf","1958de9206a34a88bc082841ded5c62f","07197f2ea89e488593265cc5300a059d","566818e348504dcbbc955f570d09b788","02d1f9177ee24e0a9948557c2ac08220","8a6c40bf2ba5446684a407f121a50284","96dd599b5cdd409991e25453ab7c14ac"]},"executionInfo":{"status":"ok","timestamp":1650644562627,"user_tz":240,"elapsed":5171343,"user":{"displayName":"Rajat Mittal","userId":"08489345110470222360"}},"outputId":"25e67127-f19e-4f7a-dee6-7ef9b4b7ecf4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device cuda\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/CIFAR10/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58fd0f238bcf4cb2b676ae599011d870"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ../data/CIFAR10/cifar-10-python.tar.gz to ../data/CIFAR10\n","Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.297439\n","Train Epoch: 1 [8000/50000 (16%)]\tLoss: 1.973216\n","Train Epoch: 1 [16000/50000 (32%)]\tLoss: 1.795473\n","Train Epoch: 1 [24000/50000 (48%)]\tLoss: 1.683449\n","Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.746827\n","Train Epoch: 1 [40000/50000 (80%)]\tLoss: 1.598388\n","Train Epoch: 1 [48000/50000 (96%)]\tLoss: 1.221717\n","Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.326415\n","Train Epoch: 2 [8000/50000 (16%)]\tLoss: 1.189062\n","Train Epoch: 2 [16000/50000 (32%)]\tLoss: 1.336704\n","Train Epoch: 2 [24000/50000 (48%)]\tLoss: 1.259550\n","Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.404043\n","Train Epoch: 2 [40000/50000 (80%)]\tLoss: 1.508529\n","Train Epoch: 2 [48000/50000 (96%)]\tLoss: 1.796216\n","\n","Test set: Average loss: 0.0014, Accuracy: 4971/10000 (50%)\n","\n","Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.232263\n","Train Epoch: 3 [8000/50000 (16%)]\tLoss: 1.721723\n","Train Epoch: 3 [16000/50000 (32%)]\tLoss: 1.285642\n","Train Epoch: 3 [24000/50000 (48%)]\tLoss: 1.391894\n","Train Epoch: 3 [32000/50000 (64%)]\tLoss: 2.102700\n","Train Epoch: 3 [40000/50000 (80%)]\tLoss: 1.625224\n","Train Epoch: 3 [48000/50000 (96%)]\tLoss: 1.573530\n","Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.263764\n","Train Epoch: 4 [8000/50000 (16%)]\tLoss: 1.517620\n","Train Epoch: 4 [16000/50000 (32%)]\tLoss: 1.186249\n","Train Epoch: 4 [24000/50000 (48%)]\tLoss: 1.777176\n","Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1.146214\n","Train Epoch: 4 [40000/50000 (80%)]\tLoss: 1.819086\n","Train Epoch: 4 [48000/50000 (96%)]\tLoss: 1.464560\n","\n","Test set: Average loss: 0.0013, Accuracy: 5355/10000 (54%)\n","\n","Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.241904\n","Train Epoch: 5 [8000/50000 (16%)]\tLoss: 1.363114\n","Train Epoch: 5 [16000/50000 (32%)]\tLoss: 1.717295\n","Train Epoch: 5 [24000/50000 (48%)]\tLoss: 0.978682\n","Train Epoch: 5 [32000/50000 (64%)]\tLoss: 1.376716\n","Train Epoch: 5 [40000/50000 (80%)]\tLoss: 1.647117\n","Train Epoch: 5 [48000/50000 (96%)]\tLoss: 0.822178\n","Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.819134\n","Train Epoch: 6 [8000/50000 (16%)]\tLoss: 1.304157\n","Train Epoch: 6 [16000/50000 (32%)]\tLoss: 0.781918\n","Train Epoch: 6 [24000/50000 (48%)]\tLoss: 1.559177\n","Train Epoch: 6 [32000/50000 (64%)]\tLoss: 1.604905\n","Train Epoch: 6 [40000/50000 (80%)]\tLoss: 1.217964\n","Train Epoch: 6 [48000/50000 (96%)]\tLoss: 1.103843\n","\n","Test set: Average loss: 0.0012, Accuracy: 5900/10000 (59%)\n","\n","Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.722353\n","Train Epoch: 7 [8000/50000 (16%)]\tLoss: 1.063880\n","Train Epoch: 7 [16000/50000 (32%)]\tLoss: 1.156158\n","Train Epoch: 7 [24000/50000 (48%)]\tLoss: 1.181118\n","Train Epoch: 7 [32000/50000 (64%)]\tLoss: 0.933617\n","Train Epoch: 7 [40000/50000 (80%)]\tLoss: 1.270281\n","Train Epoch: 7 [48000/50000 (96%)]\tLoss: 0.942464\n","Train Epoch: 8 [0/50000 (0%)]\tLoss: 1.149361\n","Train Epoch: 8 [8000/50000 (16%)]\tLoss: 1.406122\n","Train Epoch: 8 [16000/50000 (32%)]\tLoss: 1.150644\n","Train Epoch: 8 [24000/50000 (48%)]\tLoss: 1.252544\n","Train Epoch: 8 [32000/50000 (64%)]\tLoss: 1.287676\n","Train Epoch: 8 [40000/50000 (80%)]\tLoss: 1.239784\n","Train Epoch: 8 [48000/50000 (96%)]\tLoss: 1.500581\n","\n","Test set: Average loss: 0.0012, Accuracy: 5762/10000 (58%)\n","\n","Train Epoch: 9 [0/50000 (0%)]\tLoss: 1.155740\n","Train Epoch: 9 [8000/50000 (16%)]\tLoss: 1.470684\n","Train Epoch: 9 [16000/50000 (32%)]\tLoss: 1.188932\n","Train Epoch: 9 [24000/50000 (48%)]\tLoss: 1.080629\n","Train Epoch: 9 [32000/50000 (64%)]\tLoss: 1.149185\n","Train Epoch: 9 [40000/50000 (80%)]\tLoss: 1.881479\n","Train Epoch: 9 [48000/50000 (96%)]\tLoss: 1.141976\n","Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.843355\n","Train Epoch: 10 [8000/50000 (16%)]\tLoss: 1.090191\n","Train Epoch: 10 [16000/50000 (32%)]\tLoss: 1.658105\n","Train Epoch: 10 [24000/50000 (48%)]\tLoss: 0.793913\n","Train Epoch: 10 [32000/50000 (64%)]\tLoss: 1.148602\n","Train Epoch: 10 [40000/50000 (80%)]\tLoss: 1.201751\n","Train Epoch: 10 [48000/50000 (96%)]\tLoss: 1.380884\n","\n","Train set: Average loss: 0.0732, Accuracy: 29485/50000 (59%)\n","\n","\n","Test set: Average loss: 0.0012, Accuracy: 5860/10000 (59%)\n","\n","Train Epoch: 11 [0/50000 (0%)]\tLoss: 1.093619\n","Train Epoch: 11 [8000/50000 (16%)]\tLoss: 1.329339\n","Train Epoch: 11 [16000/50000 (32%)]\tLoss: 1.108140\n","Train Epoch: 11 [24000/50000 (48%)]\tLoss: 1.514114\n","Train Epoch: 11 [32000/50000 (64%)]\tLoss: 1.696691\n","Train Epoch: 11 [40000/50000 (80%)]\tLoss: 1.473980\n","Train Epoch: 11 [48000/50000 (96%)]\tLoss: 1.058961\n","Train Epoch: 12 [0/50000 (0%)]\tLoss: 1.356046\n","Train Epoch: 12 [8000/50000 (16%)]\tLoss: 1.116793\n","Train Epoch: 12 [16000/50000 (32%)]\tLoss: 1.819012\n","Train Epoch: 12 [24000/50000 (48%)]\tLoss: 1.141146\n","Train Epoch: 12 [32000/50000 (64%)]\tLoss: 1.834187\n","Train Epoch: 12 [40000/50000 (80%)]\tLoss: 1.133876\n","Train Epoch: 12 [48000/50000 (96%)]\tLoss: 1.147881\n","\n","Test set: Average loss: 0.0013, Accuracy: 5541/10000 (55%)\n","\n","Train Epoch: 13 [0/50000 (0%)]\tLoss: 1.169564\n","Train Epoch: 13 [8000/50000 (16%)]\tLoss: 0.725110\n","Train Epoch: 13 [16000/50000 (32%)]\tLoss: 1.530723\n","Train Epoch: 13 [24000/50000 (48%)]\tLoss: 1.584785\n","Train Epoch: 13 [32000/50000 (64%)]\tLoss: 1.138699\n","Train Epoch: 13 [40000/50000 (80%)]\tLoss: 1.429182\n","Train Epoch: 13 [48000/50000 (96%)]\tLoss: 1.498019\n","Train Epoch: 14 [0/50000 (0%)]\tLoss: 1.170797\n","Train Epoch: 14 [8000/50000 (16%)]\tLoss: 0.998998\n","Train Epoch: 14 [16000/50000 (32%)]\tLoss: 1.273799\n","Train Epoch: 14 [24000/50000 (48%)]\tLoss: 1.158149\n","Train Epoch: 14 [32000/50000 (64%)]\tLoss: 1.269089\n","Train Epoch: 14 [40000/50000 (80%)]\tLoss: 1.024031\n","Train Epoch: 14 [48000/50000 (96%)]\tLoss: 1.121984\n","\n","Test set: Average loss: 0.0011, Accuracy: 6001/10000 (60%)\n","\n","Train Epoch: 15 [0/50000 (0%)]\tLoss: 1.257359\n","Train Epoch: 15 [8000/50000 (16%)]\tLoss: 0.884964\n","Train Epoch: 15 [16000/50000 (32%)]\tLoss: 1.343823\n","Train Epoch: 15 [24000/50000 (48%)]\tLoss: 1.292309\n","Train Epoch: 15 [32000/50000 (64%)]\tLoss: 1.062352\n","Train Epoch: 15 [40000/50000 (80%)]\tLoss: 1.593057\n","Train Epoch: 15 [48000/50000 (96%)]\tLoss: 1.156659\n","Train Epoch: 16 [0/50000 (0%)]\tLoss: 0.846192\n","Train Epoch: 16 [8000/50000 (16%)]\tLoss: 1.350132\n","Train Epoch: 16 [16000/50000 (32%)]\tLoss: 1.039949\n","Train Epoch: 16 [24000/50000 (48%)]\tLoss: 1.336354\n","Train Epoch: 16 [32000/50000 (64%)]\tLoss: 2.257715\n","Train Epoch: 16 [40000/50000 (80%)]\tLoss: 1.242486\n","Train Epoch: 16 [48000/50000 (96%)]\tLoss: 1.511232\n","\n","Test set: Average loss: 0.0012, Accuracy: 5952/10000 (60%)\n","\n","Train Epoch: 17 [0/50000 (0%)]\tLoss: 1.111609\n","Train Epoch: 17 [8000/50000 (16%)]\tLoss: 1.265660\n","Train Epoch: 17 [16000/50000 (32%)]\tLoss: 1.350698\n","Train Epoch: 17 [24000/50000 (48%)]\tLoss: 1.009226\n","Train Epoch: 17 [32000/50000 (64%)]\tLoss: 1.305632\n","Train Epoch: 17 [40000/50000 (80%)]\tLoss: 1.120632\n","Train Epoch: 17 [48000/50000 (96%)]\tLoss: 1.610377\n","Train Epoch: 18 [0/50000 (0%)]\tLoss: 1.328892\n","Train Epoch: 18 [8000/50000 (16%)]\tLoss: 0.894693\n","Train Epoch: 18 [16000/50000 (32%)]\tLoss: 0.774011\n","Train Epoch: 18 [24000/50000 (48%)]\tLoss: 1.271196\n","Train Epoch: 18 [32000/50000 (64%)]\tLoss: 1.231543\n","Train Epoch: 18 [40000/50000 (80%)]\tLoss: 1.374030\n","Train Epoch: 18 [48000/50000 (96%)]\tLoss: 1.759869\n","\n","Test set: Average loss: 0.0011, Accuracy: 5944/10000 (59%)\n","\n","Train Epoch: 19 [0/50000 (0%)]\tLoss: 1.385128\n","Train Epoch: 19 [8000/50000 (16%)]\tLoss: 1.304516\n","Train Epoch: 19 [16000/50000 (32%)]\tLoss: 1.134649\n","Train Epoch: 19 [24000/50000 (48%)]\tLoss: 1.438188\n","Train Epoch: 19 [32000/50000 (64%)]\tLoss: 1.262398\n","Train Epoch: 19 [40000/50000 (80%)]\tLoss: 1.564175\n","Train Epoch: 19 [48000/50000 (96%)]\tLoss: 0.826178\n","Train Epoch: 20 [0/50000 (0%)]\tLoss: 1.324134\n","Train Epoch: 20 [8000/50000 (16%)]\tLoss: 1.051805\n","Train Epoch: 20 [16000/50000 (32%)]\tLoss: 1.166888\n","Train Epoch: 20 [24000/50000 (48%)]\tLoss: 1.205704\n","Train Epoch: 20 [32000/50000 (64%)]\tLoss: 1.560053\n","Train Epoch: 20 [40000/50000 (80%)]\tLoss: 1.159641\n","Train Epoch: 20 [48000/50000 (96%)]\tLoss: 1.075228\n","\n","Train set: Average loss: 0.0682, Accuracy: 30860/50000 (62%)\n","\n","\n","Test set: Average loss: 0.0011, Accuracy: 6084/10000 (61%)\n","\n","Train Epoch: 21 [0/50000 (0%)]\tLoss: 0.682271\n","Train Epoch: 21 [8000/50000 (16%)]\tLoss: 1.138827\n","Train Epoch: 21 [16000/50000 (32%)]\tLoss: 1.280663\n","Train Epoch: 21 [24000/50000 (48%)]\tLoss: 1.162292\n","Train Epoch: 21 [32000/50000 (64%)]\tLoss: 1.246697\n","Train Epoch: 21 [40000/50000 (80%)]\tLoss: 1.068392\n","Train Epoch: 21 [48000/50000 (96%)]\tLoss: 1.329147\n","Train Epoch: 22 [0/50000 (0%)]\tLoss: 1.257552\n","Train Epoch: 22 [8000/50000 (16%)]\tLoss: 0.938239\n","Train Epoch: 22 [16000/50000 (32%)]\tLoss: 1.204009\n","Train Epoch: 22 [24000/50000 (48%)]\tLoss: 1.400277\n","Train Epoch: 22 [32000/50000 (64%)]\tLoss: 1.376287\n","Train Epoch: 22 [40000/50000 (80%)]\tLoss: 1.204888\n","Train Epoch: 22 [48000/50000 (96%)]\tLoss: 0.906580\n","\n","Test set: Average loss: 0.0012, Accuracy: 6004/10000 (60%)\n","\n","Train Epoch: 23 [0/50000 (0%)]\tLoss: 1.157868\n","Train Epoch: 23 [8000/50000 (16%)]\tLoss: 1.317712\n","Train Epoch: 23 [16000/50000 (32%)]\tLoss: 1.299135\n","Train Epoch: 23 [24000/50000 (48%)]\tLoss: 0.875120\n","Train Epoch: 23 [32000/50000 (64%)]\tLoss: 1.664595\n","Train Epoch: 23 [40000/50000 (80%)]\tLoss: 1.286895\n","Train Epoch: 23 [48000/50000 (96%)]\tLoss: 1.284510\n","Train Epoch: 24 [0/50000 (0%)]\tLoss: 0.934907\n","Train Epoch: 24 [8000/50000 (16%)]\tLoss: 1.357464\n","Train Epoch: 24 [16000/50000 (32%)]\tLoss: 1.050015\n","Train Epoch: 24 [24000/50000 (48%)]\tLoss: 1.129925\n","Train Epoch: 24 [32000/50000 (64%)]\tLoss: 1.590056\n","Train Epoch: 24 [40000/50000 (80%)]\tLoss: 1.143595\n","Train Epoch: 24 [48000/50000 (96%)]\tLoss: 1.535108\n","\n","Test set: Average loss: 0.0012, Accuracy: 6047/10000 (60%)\n","\n","Train Epoch: 25 [0/50000 (0%)]\tLoss: 0.934959\n","Train Epoch: 25 [8000/50000 (16%)]\tLoss: 1.701137\n","Train Epoch: 25 [16000/50000 (32%)]\tLoss: 1.191475\n","Train Epoch: 25 [24000/50000 (48%)]\tLoss: 1.253088\n","Train Epoch: 25 [32000/50000 (64%)]\tLoss: 1.582224\n","Train Epoch: 25 [40000/50000 (80%)]\tLoss: 0.880803\n","Train Epoch: 25 [48000/50000 (96%)]\tLoss: 1.025580\n","Train Epoch: 26 [0/50000 (0%)]\tLoss: 1.221674\n","Train Epoch: 26 [8000/50000 (16%)]\tLoss: 1.159842\n","Train Epoch: 26 [16000/50000 (32%)]\tLoss: 1.234504\n","Train Epoch: 26 [24000/50000 (48%)]\tLoss: 1.552344\n","Train Epoch: 26 [32000/50000 (64%)]\tLoss: 1.098610\n","Train Epoch: 26 [40000/50000 (80%)]\tLoss: 1.446150\n","Train Epoch: 26 [48000/50000 (96%)]\tLoss: 0.897454\n","\n","Test set: Average loss: 0.0011, Accuracy: 6207/10000 (62%)\n","\n","Train Epoch: 27 [0/50000 (0%)]\tLoss: 1.270126\n","Train Epoch: 27 [8000/50000 (16%)]\tLoss: 0.917051\n","Train Epoch: 27 [16000/50000 (32%)]\tLoss: 1.089693\n","Train Epoch: 27 [24000/50000 (48%)]\tLoss: 1.040717\n","Train Epoch: 27 [32000/50000 (64%)]\tLoss: 1.786880\n","Train Epoch: 27 [40000/50000 (80%)]\tLoss: 1.213551\n","Train Epoch: 27 [48000/50000 (96%)]\tLoss: 0.807203\n","Train Epoch: 28 [0/50000 (0%)]\tLoss: 1.396135\n","Train Epoch: 28 [8000/50000 (16%)]\tLoss: 0.874957\n","Train Epoch: 28 [16000/50000 (32%)]\tLoss: 1.187186\n","Train Epoch: 28 [24000/50000 (48%)]\tLoss: 1.328597\n","Train Epoch: 28 [32000/50000 (64%)]\tLoss: 1.557070\n","Train Epoch: 28 [40000/50000 (80%)]\tLoss: 1.207050\n","Train Epoch: 28 [48000/50000 (96%)]\tLoss: 1.332582\n","\n","Test set: Average loss: 0.0012, Accuracy: 5765/10000 (58%)\n","\n","Train Epoch: 29 [0/50000 (0%)]\tLoss: 1.181454\n","Train Epoch: 29 [8000/50000 (16%)]\tLoss: 1.295522\n","Train Epoch: 29 [16000/50000 (32%)]\tLoss: 1.365602\n","Train Epoch: 29 [24000/50000 (48%)]\tLoss: 1.266714\n","Train Epoch: 29 [32000/50000 (64%)]\tLoss: 1.326080\n","Train Epoch: 29 [40000/50000 (80%)]\tLoss: 1.399128\n","Train Epoch: 29 [48000/50000 (96%)]\tLoss: 1.323212\n","Train Epoch: 30 [0/50000 (0%)]\tLoss: 1.223432\n","Train Epoch: 30 [8000/50000 (16%)]\tLoss: 1.408435\n","Train Epoch: 30 [16000/50000 (32%)]\tLoss: 1.449813\n","Train Epoch: 30 [24000/50000 (48%)]\tLoss: 1.508940\n","Train Epoch: 30 [32000/50000 (64%)]\tLoss: 1.306612\n","Train Epoch: 30 [40000/50000 (80%)]\tLoss: 1.136458\n","Train Epoch: 30 [48000/50000 (96%)]\tLoss: 0.947777\n","\n","Train set: Average loss: 0.0676, Accuracy: 31141/50000 (62%)\n","\n","\n","Test set: Average loss: 0.0011, Accuracy: 6190/10000 (62%)\n","\n","Train Epoch: 31 [0/50000 (0%)]\tLoss: 0.727262\n","Train Epoch: 31 [8000/50000 (16%)]\tLoss: 0.918936\n","Train Epoch: 31 [16000/50000 (32%)]\tLoss: 1.204624\n","Train Epoch: 31 [24000/50000 (48%)]\tLoss: 1.181537\n","Train Epoch: 31 [32000/50000 (64%)]\tLoss: 1.281153\n","Train Epoch: 31 [40000/50000 (80%)]\tLoss: 1.167073\n","Train Epoch: 31 [48000/50000 (96%)]\tLoss: 1.196556\n","Train Epoch: 32 [0/50000 (0%)]\tLoss: 0.568256\n","Train Epoch: 32 [8000/50000 (16%)]\tLoss: 1.406956\n","Train Epoch: 32 [16000/50000 (32%)]\tLoss: 0.915670\n","Train Epoch: 32 [24000/50000 (48%)]\tLoss: 1.013797\n","Train Epoch: 32 [32000/50000 (64%)]\tLoss: 1.037495\n","Train Epoch: 32 [40000/50000 (80%)]\tLoss: 1.565221\n","Train Epoch: 32 [48000/50000 (96%)]\tLoss: 1.510518\n","\n","Test set: Average loss: 0.0011, Accuracy: 6197/10000 (62%)\n","\n","Train Epoch: 33 [0/50000 (0%)]\tLoss: 1.195079\n","Train Epoch: 33 [8000/50000 (16%)]\tLoss: 1.743297\n","Train Epoch: 33 [16000/50000 (32%)]\tLoss: 1.755631\n","Train Epoch: 33 [24000/50000 (48%)]\tLoss: 1.265494\n","Train Epoch: 33 [32000/50000 (64%)]\tLoss: 1.041561\n","Train Epoch: 33 [40000/50000 (80%)]\tLoss: 1.161805\n","Train Epoch: 33 [48000/50000 (96%)]\tLoss: 1.160382\n","Train Epoch: 34 [0/50000 (0%)]\tLoss: 1.437406\n","Train Epoch: 34 [8000/50000 (16%)]\tLoss: 1.451455\n","Train Epoch: 34 [16000/50000 (32%)]\tLoss: 0.801892\n","Train Epoch: 34 [24000/50000 (48%)]\tLoss: 1.088882\n","Train Epoch: 34 [32000/50000 (64%)]\tLoss: 1.313034\n","Train Epoch: 34 [40000/50000 (80%)]\tLoss: 0.819888\n","Train Epoch: 34 [48000/50000 (96%)]\tLoss: 1.518523\n","\n","Test set: Average loss: 0.0012, Accuracy: 5990/10000 (60%)\n","\n","Train Epoch: 35 [0/50000 (0%)]\tLoss: 0.827889\n","Train Epoch: 35 [8000/50000 (16%)]\tLoss: 1.051729\n","Train Epoch: 35 [16000/50000 (32%)]\tLoss: 1.269096\n","Train Epoch: 35 [24000/50000 (48%)]\tLoss: 1.476529\n","Train Epoch: 35 [32000/50000 (64%)]\tLoss: 1.327598\n","Train Epoch: 35 [40000/50000 (80%)]\tLoss: 1.091199\n","Train Epoch: 35 [48000/50000 (96%)]\tLoss: 1.251384\n","Train Epoch: 36 [0/50000 (0%)]\tLoss: 1.032253\n","Train Epoch: 36 [8000/50000 (16%)]\tLoss: 1.272746\n","Train Epoch: 36 [16000/50000 (32%)]\tLoss: 1.553485\n","Train Epoch: 36 [24000/50000 (48%)]\tLoss: 0.769229\n","Train Epoch: 36 [32000/50000 (64%)]\tLoss: 1.073240\n","Train Epoch: 36 [40000/50000 (80%)]\tLoss: 1.110169\n","Train Epoch: 36 [48000/50000 (96%)]\tLoss: 0.989661\n","\n","Test set: Average loss: 0.0011, Accuracy: 6355/10000 (64%)\n","\n","Train Epoch: 37 [0/50000 (0%)]\tLoss: 1.031700\n","Train Epoch: 37 [8000/50000 (16%)]\tLoss: 1.193151\n","Train Epoch: 37 [16000/50000 (32%)]\tLoss: 1.447468\n","Train Epoch: 37 [24000/50000 (48%)]\tLoss: 1.508906\n","Train Epoch: 37 [32000/50000 (64%)]\tLoss: 1.118859\n","Train Epoch: 37 [40000/50000 (80%)]\tLoss: 1.160351\n","Train Epoch: 37 [48000/50000 (96%)]\tLoss: 1.039345\n","Train Epoch: 38 [0/50000 (0%)]\tLoss: 1.189323\n","Train Epoch: 38 [8000/50000 (16%)]\tLoss: 0.847696\n","Train Epoch: 38 [16000/50000 (32%)]\tLoss: 1.471095\n","Train Epoch: 38 [24000/50000 (48%)]\tLoss: 1.522992\n","Train Epoch: 38 [32000/50000 (64%)]\tLoss: 1.157564\n","Train Epoch: 38 [40000/50000 (80%)]\tLoss: 1.553806\n","Train Epoch: 38 [48000/50000 (96%)]\tLoss: 0.882549\n","\n","Test set: Average loss: 0.0011, Accuracy: 6010/10000 (60%)\n","\n","Train Epoch: 39 [0/50000 (0%)]\tLoss: 1.443651\n","Train Epoch: 39 [8000/50000 (16%)]\tLoss: 1.331518\n","Train Epoch: 39 [16000/50000 (32%)]\tLoss: 1.183836\n","Train Epoch: 39 [24000/50000 (48%)]\tLoss: 0.960864\n","Train Epoch: 39 [32000/50000 (64%)]\tLoss: 1.325441\n","Train Epoch: 39 [40000/50000 (80%)]\tLoss: 0.978326\n","Train Epoch: 39 [48000/50000 (96%)]\tLoss: 1.389200\n","Train Epoch: 40 [0/50000 (0%)]\tLoss: 0.838894\n","Train Epoch: 40 [8000/50000 (16%)]\tLoss: 1.479635\n","Train Epoch: 40 [16000/50000 (32%)]\tLoss: 1.350537\n","Train Epoch: 40 [24000/50000 (48%)]\tLoss: 1.115338\n","Train Epoch: 40 [32000/50000 (64%)]\tLoss: 1.261557\n","Train Epoch: 40 [40000/50000 (80%)]\tLoss: 1.080241\n","Train Epoch: 40 [48000/50000 (96%)]\tLoss: 1.085190\n","\n","Train set: Average loss: 0.0692, Accuracy: 30279/50000 (61%)\n","\n","\n","Test set: Average loss: 0.0011, Accuracy: 5957/10000 (60%)\n","\n","Train Epoch: 41 [0/50000 (0%)]\tLoss: 0.987801\n","Train Epoch: 41 [8000/50000 (16%)]\tLoss: 0.707159\n","Train Epoch: 41 [16000/50000 (32%)]\tLoss: 1.315676\n","Train Epoch: 41 [24000/50000 (48%)]\tLoss: 1.164774\n","Train Epoch: 41 [32000/50000 (64%)]\tLoss: 2.271240\n","Train Epoch: 41 [40000/50000 (80%)]\tLoss: 0.796133\n","Train Epoch: 41 [48000/50000 (96%)]\tLoss: 1.309328\n","Train Epoch: 42 [0/50000 (0%)]\tLoss: 1.093349\n","Train Epoch: 42 [8000/50000 (16%)]\tLoss: 1.324219\n","Train Epoch: 42 [16000/50000 (32%)]\tLoss: 1.181141\n","Train Epoch: 42 [24000/50000 (48%)]\tLoss: 1.161884\n","Train Epoch: 42 [32000/50000 (64%)]\tLoss: 1.706468\n","Train Epoch: 42 [40000/50000 (80%)]\tLoss: 1.296918\n","Train Epoch: 42 [48000/50000 (96%)]\tLoss: 1.099478\n","\n","Test set: Average loss: 0.0011, Accuracy: 6172/10000 (62%)\n","\n","Train Epoch: 43 [0/50000 (0%)]\tLoss: 0.920081\n","Train Epoch: 43 [8000/50000 (16%)]\tLoss: 1.364902\n","Train Epoch: 43 [16000/50000 (32%)]\tLoss: 0.960807\n","Train Epoch: 43 [24000/50000 (48%)]\tLoss: 1.340182\n","Train Epoch: 43 [32000/50000 (64%)]\tLoss: 1.526791\n","Train Epoch: 43 [40000/50000 (80%)]\tLoss: 1.708841\n","Train Epoch: 43 [48000/50000 (96%)]\tLoss: 1.135829\n","Train Epoch: 44 [0/50000 (0%)]\tLoss: 1.195717\n","Train Epoch: 44 [8000/50000 (16%)]\tLoss: 1.326673\n","Train Epoch: 44 [16000/50000 (32%)]\tLoss: 1.198503\n","Train Epoch: 44 [24000/50000 (48%)]\tLoss: 1.341040\n","Train Epoch: 44 [32000/50000 (64%)]\tLoss: 1.094519\n","Train Epoch: 44 [40000/50000 (80%)]\tLoss: 1.090962\n","Train Epoch: 44 [48000/50000 (96%)]\tLoss: 0.968312\n","\n","Test set: Average loss: 0.0012, Accuracy: 5892/10000 (59%)\n","\n","Train Epoch: 45 [0/50000 (0%)]\tLoss: 1.317451\n","Train Epoch: 45 [8000/50000 (16%)]\tLoss: 0.834314\n","Train Epoch: 45 [16000/50000 (32%)]\tLoss: 1.007660\n","Train Epoch: 45 [24000/50000 (48%)]\tLoss: 0.962798\n","Train Epoch: 45 [32000/50000 (64%)]\tLoss: 0.808007\n","Train Epoch: 45 [40000/50000 (80%)]\tLoss: 1.001111\n","Train Epoch: 45 [48000/50000 (96%)]\tLoss: 1.041689\n","Train Epoch: 46 [0/50000 (0%)]\tLoss: 0.755230\n","Train Epoch: 46 [8000/50000 (16%)]\tLoss: 1.216573\n","Train Epoch: 46 [16000/50000 (32%)]\tLoss: 1.349572\n","Train Epoch: 46 [24000/50000 (48%)]\tLoss: 1.442725\n","Train Epoch: 46 [32000/50000 (64%)]\tLoss: 1.437327\n","Train Epoch: 46 [40000/50000 (80%)]\tLoss: 1.546690\n","Train Epoch: 46 [48000/50000 (96%)]\tLoss: 1.250890\n","\n","Test set: Average loss: 0.0011, Accuracy: 6254/10000 (63%)\n","\n","Train Epoch: 47 [0/50000 (0%)]\tLoss: 0.606408\n","Train Epoch: 47 [8000/50000 (16%)]\tLoss: 1.006662\n","Train Epoch: 47 [16000/50000 (32%)]\tLoss: 1.030461\n","Train Epoch: 47 [24000/50000 (48%)]\tLoss: 1.087091\n","Train Epoch: 47 [32000/50000 (64%)]\tLoss: 1.155577\n","Train Epoch: 47 [40000/50000 (80%)]\tLoss: 1.064567\n","Train Epoch: 47 [48000/50000 (96%)]\tLoss: 0.880214\n","Train Epoch: 48 [0/50000 (0%)]\tLoss: 1.304446\n","Train Epoch: 48 [8000/50000 (16%)]\tLoss: 2.040485\n","Train Epoch: 48 [16000/50000 (32%)]\tLoss: 1.587160\n","Train Epoch: 48 [24000/50000 (48%)]\tLoss: 1.485023\n","Train Epoch: 48 [32000/50000 (64%)]\tLoss: 1.113468\n","Train Epoch: 48 [40000/50000 (80%)]\tLoss: 0.865606\n","Train Epoch: 48 [48000/50000 (96%)]\tLoss: 1.734196\n","\n","Test set: Average loss: 0.0011, Accuracy: 6202/10000 (62%)\n","\n","Train Epoch: 49 [0/50000 (0%)]\tLoss: 0.775386\n","Train Epoch: 49 [8000/50000 (16%)]\tLoss: 1.176071\n","Train Epoch: 49 [16000/50000 (32%)]\tLoss: 1.290208\n","Train Epoch: 49 [24000/50000 (48%)]\tLoss: 1.073531\n","Train Epoch: 49 [32000/50000 (64%)]\tLoss: 1.032880\n","Train Epoch: 49 [40000/50000 (80%)]\tLoss: 1.003978\n","Train Epoch: 49 [48000/50000 (96%)]\tLoss: 0.921096\n","Train Epoch: 50 [0/50000 (0%)]\tLoss: 1.441162\n","Train Epoch: 50 [8000/50000 (16%)]\tLoss: 0.936623\n","Train Epoch: 50 [16000/50000 (32%)]\tLoss: 1.043203\n","Train Epoch: 50 [24000/50000 (48%)]\tLoss: 1.029596\n","Train Epoch: 50 [32000/50000 (64%)]\tLoss: 1.337885\n","Train Epoch: 50 [40000/50000 (80%)]\tLoss: 1.343340\n","Train Epoch: 50 [48000/50000 (96%)]\tLoss: 1.098771\n","\n","Train set: Average loss: 0.0719, Accuracy: 30352/50000 (61%)\n","\n","\n","Test set: Average loss: 0.0012, Accuracy: 6025/10000 (60%)\n","\n","Train Epoch: 51 [0/50000 (0%)]\tLoss: 1.358203\n","Train Epoch: 51 [8000/50000 (16%)]\tLoss: 0.968376\n","Train Epoch: 51 [16000/50000 (32%)]\tLoss: 1.375454\n","Train Epoch: 51 [24000/50000 (48%)]\tLoss: 0.700676\n","Train Epoch: 51 [32000/50000 (64%)]\tLoss: 1.331178\n","Train Epoch: 51 [40000/50000 (80%)]\tLoss: 1.581653\n","Train Epoch: 51 [48000/50000 (96%)]\tLoss: 1.184290\n","Train Epoch: 52 [0/50000 (0%)]\tLoss: 1.133438\n","Train Epoch: 52 [8000/50000 (16%)]\tLoss: 1.434941\n","Train Epoch: 52 [16000/50000 (32%)]\tLoss: 0.884625\n","Train Epoch: 52 [24000/50000 (48%)]\tLoss: 1.017416\n","Train Epoch: 52 [32000/50000 (64%)]\tLoss: 1.366700\n","Train Epoch: 52 [40000/50000 (80%)]\tLoss: 0.992315\n","Train Epoch: 52 [48000/50000 (96%)]\tLoss: 0.981351\n","\n","Test set: Average loss: 0.0012, Accuracy: 5937/10000 (59%)\n","\n","Train Epoch: 53 [0/50000 (0%)]\tLoss: 1.077348\n","Train Epoch: 53 [8000/50000 (16%)]\tLoss: 1.247800\n","Train Epoch: 53 [16000/50000 (32%)]\tLoss: 1.181248\n","Train Epoch: 53 [24000/50000 (48%)]\tLoss: 1.518033\n","Train Epoch: 53 [32000/50000 (64%)]\tLoss: 1.717108\n","Train Epoch: 53 [40000/50000 (80%)]\tLoss: 1.353759\n","Train Epoch: 53 [48000/50000 (96%)]\tLoss: 0.948199\n","Train Epoch: 54 [0/50000 (0%)]\tLoss: 1.174538\n","Train Epoch: 54 [8000/50000 (16%)]\tLoss: 1.535574\n","Train Epoch: 54 [16000/50000 (32%)]\tLoss: 1.135781\n","Train Epoch: 54 [24000/50000 (48%)]\tLoss: 1.075814\n","Train Epoch: 54 [32000/50000 (64%)]\tLoss: 0.907139\n","Train Epoch: 54 [40000/50000 (80%)]\tLoss: 1.096094\n","Train Epoch: 54 [48000/50000 (96%)]\tLoss: 1.211466\n","\n","Test set: Average loss: 0.0011, Accuracy: 6284/10000 (63%)\n","\n","Train Epoch: 55 [0/50000 (0%)]\tLoss: 1.312688\n","Train Epoch: 55 [8000/50000 (16%)]\tLoss: 0.806485\n","Train Epoch: 55 [16000/50000 (32%)]\tLoss: 1.622425\n","Train Epoch: 55 [24000/50000 (48%)]\tLoss: 1.166952\n","Train Epoch: 55 [32000/50000 (64%)]\tLoss: 1.466980\n","Train Epoch: 55 [40000/50000 (80%)]\tLoss: 1.116706\n","Train Epoch: 55 [48000/50000 (96%)]\tLoss: 1.030901\n","Train Epoch: 56 [0/50000 (0%)]\tLoss: 1.454469\n","Train Epoch: 56 [8000/50000 (16%)]\tLoss: 1.646933\n","Train Epoch: 56 [16000/50000 (32%)]\tLoss: 0.837827\n","Train Epoch: 56 [24000/50000 (48%)]\tLoss: 1.160321\n","Train Epoch: 56 [32000/50000 (64%)]\tLoss: 1.190902\n","Train Epoch: 56 [40000/50000 (80%)]\tLoss: 1.055932\n","Train Epoch: 56 [48000/50000 (96%)]\tLoss: 1.146438\n","\n","Test set: Average loss: 0.0010, Accuracy: 6463/10000 (65%)\n","\n","Train Epoch: 57 [0/50000 (0%)]\tLoss: 1.596724\n","Train Epoch: 57 [8000/50000 (16%)]\tLoss: 1.530176\n","Train Epoch: 57 [16000/50000 (32%)]\tLoss: 1.015708\n","Train Epoch: 57 [24000/50000 (48%)]\tLoss: 1.061236\n","Train Epoch: 57 [32000/50000 (64%)]\tLoss: 0.739358\n","Train Epoch: 57 [40000/50000 (80%)]\tLoss: 0.719377\n","Train Epoch: 57 [48000/50000 (96%)]\tLoss: 1.100360\n","Train Epoch: 58 [0/50000 (0%)]\tLoss: 1.242331\n","Train Epoch: 58 [8000/50000 (16%)]\tLoss: 1.096189\n","Train Epoch: 58 [16000/50000 (32%)]\tLoss: 1.319681\n","Train Epoch: 58 [24000/50000 (48%)]\tLoss: 1.076159\n","Train Epoch: 58 [32000/50000 (64%)]\tLoss: 0.954343\n","Train Epoch: 58 [40000/50000 (80%)]\tLoss: 1.324387\n","Train Epoch: 58 [48000/50000 (96%)]\tLoss: 1.159245\n","\n","Test set: Average loss: 0.0011, Accuracy: 6124/10000 (61%)\n","\n","Train Epoch: 59 [0/50000 (0%)]\tLoss: 0.856756\n","Train Epoch: 59 [8000/50000 (16%)]\tLoss: 0.636900\n","Train Epoch: 59 [16000/50000 (32%)]\tLoss: 1.267968\n","Train Epoch: 59 [24000/50000 (48%)]\tLoss: 0.868726\n","Train Epoch: 59 [32000/50000 (64%)]\tLoss: 1.086687\n","Train Epoch: 59 [40000/50000 (80%)]\tLoss: 1.359947\n","Train Epoch: 59 [48000/50000 (96%)]\tLoss: 0.844296\n","Train Epoch: 60 [0/50000 (0%)]\tLoss: 1.777231\n","Train Epoch: 60 [8000/50000 (16%)]\tLoss: 1.240673\n","Train Epoch: 60 [16000/50000 (32%)]\tLoss: 1.189585\n","Train Epoch: 60 [24000/50000 (48%)]\tLoss: 1.288767\n","Train Epoch: 60 [32000/50000 (64%)]\tLoss: 1.015247\n","Train Epoch: 60 [40000/50000 (80%)]\tLoss: 1.168072\n","Train Epoch: 60 [48000/50000 (96%)]\tLoss: 1.095829\n","\n","Train set: Average loss: 0.0667, Accuracy: 31557/50000 (63%)\n","\n","\n","Test set: Average loss: 0.0011, Accuracy: 6246/10000 (62%)\n","\n","Train Epoch: 61 [0/50000 (0%)]\tLoss: 1.269597\n","Train Epoch: 61 [8000/50000 (16%)]\tLoss: 0.892978\n","Train Epoch: 61 [16000/50000 (32%)]\tLoss: 0.895074\n","Train Epoch: 61 [24000/50000 (48%)]\tLoss: 1.004260\n","Train Epoch: 61 [32000/50000 (64%)]\tLoss: 1.069026\n","Train Epoch: 61 [40000/50000 (80%)]\tLoss: 1.262931\n","Train Epoch: 61 [48000/50000 (96%)]\tLoss: 0.771326\n","Train Epoch: 62 [0/50000 (0%)]\tLoss: 1.256921\n","Train Epoch: 62 [8000/50000 (16%)]\tLoss: 1.671398\n","Train Epoch: 62 [16000/50000 (32%)]\tLoss: 1.372256\n","Train Epoch: 62 [24000/50000 (48%)]\tLoss: 0.874607\n","Train Epoch: 62 [32000/50000 (64%)]\tLoss: 0.989130\n","Train Epoch: 62 [40000/50000 (80%)]\tLoss: 1.214806\n","Train Epoch: 62 [48000/50000 (96%)]\tLoss: 0.810057\n","\n","Test set: Average loss: 0.0011, Accuracy: 6183/10000 (62%)\n","\n","Train Epoch: 63 [0/50000 (0%)]\tLoss: 1.063685\n","Train Epoch: 63 [8000/50000 (16%)]\tLoss: 1.247240\n","Train Epoch: 63 [16000/50000 (32%)]\tLoss: 1.025681\n","Train Epoch: 63 [24000/50000 (48%)]\tLoss: 1.385340\n","Train Epoch: 63 [32000/50000 (64%)]\tLoss: 0.881700\n","Train Epoch: 63 [40000/50000 (80%)]\tLoss: 0.987757\n","Train Epoch: 63 [48000/50000 (96%)]\tLoss: 0.857718\n","Train Epoch: 64 [0/50000 (0%)]\tLoss: 1.286234\n","Train Epoch: 64 [8000/50000 (16%)]\tLoss: 1.287408\n","Train Epoch: 64 [16000/50000 (32%)]\tLoss: 1.407748\n","Train Epoch: 64 [24000/50000 (48%)]\tLoss: 0.787683\n","Train Epoch: 64 [32000/50000 (64%)]\tLoss: 1.035780\n","Train Epoch: 64 [40000/50000 (80%)]\tLoss: 0.874917\n","Train Epoch: 64 [48000/50000 (96%)]\tLoss: 0.933223\n","\n","Test set: Average loss: 0.0011, Accuracy: 6193/10000 (62%)\n","\n","Train Epoch: 65 [0/50000 (0%)]\tLoss: 1.937448\n","Train Epoch: 65 [8000/50000 (16%)]\tLoss: 1.228471\n","Train Epoch: 65 [16000/50000 (32%)]\tLoss: 0.965093\n","Train Epoch: 65 [24000/50000 (48%)]\tLoss: 1.215853\n","Train Epoch: 65 [32000/50000 (64%)]\tLoss: 0.564267\n","Train Epoch: 65 [40000/50000 (80%)]\tLoss: 0.917866\n","Train Epoch: 65 [48000/50000 (96%)]\tLoss: 1.285507\n","Train Epoch: 66 [0/50000 (0%)]\tLoss: 0.867126\n","Train Epoch: 66 [8000/50000 (16%)]\tLoss: 0.992596\n","Train Epoch: 66 [16000/50000 (32%)]\tLoss: 1.320879\n","Train Epoch: 66 [24000/50000 (48%)]\tLoss: 0.858627\n","Train Epoch: 66 [32000/50000 (64%)]\tLoss: 1.463843\n","Train Epoch: 66 [40000/50000 (80%)]\tLoss: 1.493841\n","Train Epoch: 66 [48000/50000 (96%)]\tLoss: 1.632562\n","\n","Test set: Average loss: 0.0010, Accuracy: 6499/10000 (65%)\n","\n","Train Epoch: 67 [0/50000 (0%)]\tLoss: 1.445069\n","Train Epoch: 67 [8000/50000 (16%)]\tLoss: 1.076718\n","Train Epoch: 67 [16000/50000 (32%)]\tLoss: 1.243390\n","Train Epoch: 67 [24000/50000 (48%)]\tLoss: 1.410891\n","Train Epoch: 67 [32000/50000 (64%)]\tLoss: 1.315750\n","Train Epoch: 67 [40000/50000 (80%)]\tLoss: 0.882036\n","Train Epoch: 67 [48000/50000 (96%)]\tLoss: 0.875132\n","Train Epoch: 68 [0/50000 (0%)]\tLoss: 1.083167\n","Train Epoch: 68 [8000/50000 (16%)]\tLoss: 0.852240\n","Train Epoch: 68 [16000/50000 (32%)]\tLoss: 1.090202\n","Train Epoch: 68 [24000/50000 (48%)]\tLoss: 1.338357\n","Train Epoch: 68 [32000/50000 (64%)]\tLoss: 1.431579\n","Train Epoch: 68 [40000/50000 (80%)]\tLoss: 0.930775\n","Train Epoch: 68 [48000/50000 (96%)]\tLoss: 1.310380\n","\n","Test set: Average loss: 0.0011, Accuracy: 6100/10000 (61%)\n","\n","Train Epoch: 69 [0/50000 (0%)]\tLoss: 0.993411\n","Train Epoch: 69 [8000/50000 (16%)]\tLoss: 0.962074\n","Train Epoch: 69 [16000/50000 (32%)]\tLoss: 1.187967\n","Train Epoch: 69 [24000/50000 (48%)]\tLoss: 0.804164\n","Train Epoch: 69 [32000/50000 (64%)]\tLoss: 0.975710\n","Train Epoch: 69 [40000/50000 (80%)]\tLoss: 1.269940\n","Train Epoch: 69 [48000/50000 (96%)]\tLoss: 0.973309\n","Train Epoch: 70 [0/50000 (0%)]\tLoss: 0.976790\n","Train Epoch: 70 [8000/50000 (16%)]\tLoss: 1.449340\n","Train Epoch: 70 [16000/50000 (32%)]\tLoss: 1.278266\n","Train Epoch: 70 [24000/50000 (48%)]\tLoss: 0.988549\n","Train Epoch: 70 [32000/50000 (64%)]\tLoss: 0.898119\n","Train Epoch: 70 [40000/50000 (80%)]\tLoss: 0.996458\n","Train Epoch: 70 [48000/50000 (96%)]\tLoss: 1.560539\n","\n","Train set: Average loss: 0.0645, Accuracy: 32413/50000 (65%)\n","\n","\n","Test set: Average loss: 0.0011, Accuracy: 6448/10000 (64%)\n","\n","Train Epoch: 71 [0/50000 (0%)]\tLoss: 1.454458\n","Train Epoch: 71 [8000/50000 (16%)]\tLoss: 1.144067\n","Train Epoch: 71 [16000/50000 (32%)]\tLoss: 1.269286\n","Train Epoch: 71 [24000/50000 (48%)]\tLoss: 1.002687\n","Train Epoch: 71 [32000/50000 (64%)]\tLoss: 0.859150\n","Train Epoch: 71 [40000/50000 (80%)]\tLoss: 0.805167\n","Train Epoch: 71 [48000/50000 (96%)]\tLoss: 1.224647\n","Train Epoch: 72 [0/50000 (0%)]\tLoss: 1.277431\n","Train Epoch: 72 [8000/50000 (16%)]\tLoss: 0.933715\n","Train Epoch: 72 [16000/50000 (32%)]\tLoss: 1.092270\n","Train Epoch: 72 [24000/50000 (48%)]\tLoss: 1.028032\n","Train Epoch: 72 [32000/50000 (64%)]\tLoss: 1.159587\n","Train Epoch: 72 [40000/50000 (80%)]\tLoss: 1.258477\n","Train Epoch: 72 [48000/50000 (96%)]\tLoss: 1.734703\n","\n","Test set: Average loss: 0.0011, Accuracy: 6152/10000 (62%)\n","\n","Train Epoch: 73 [0/50000 (0%)]\tLoss: 0.959875\n","Train Epoch: 73 [8000/50000 (16%)]\tLoss: 1.026115\n","Train Epoch: 73 [16000/50000 (32%)]\tLoss: 1.176808\n","Train Epoch: 73 [24000/50000 (48%)]\tLoss: 1.453254\n","Train Epoch: 73 [32000/50000 (64%)]\tLoss: 1.072462\n","Train Epoch: 73 [40000/50000 (80%)]\tLoss: 0.890022\n","Train Epoch: 73 [48000/50000 (96%)]\tLoss: 1.679286\n","Train Epoch: 74 [0/50000 (0%)]\tLoss: 1.371943\n","Train Epoch: 74 [8000/50000 (16%)]\tLoss: 1.166972\n","Train Epoch: 74 [16000/50000 (32%)]\tLoss: 0.606218\n","Train Epoch: 74 [24000/50000 (48%)]\tLoss: 0.790718\n","Train Epoch: 74 [32000/50000 (64%)]\tLoss: 1.837740\n","Train Epoch: 74 [40000/50000 (80%)]\tLoss: 1.128534\n","Train Epoch: 74 [48000/50000 (96%)]\tLoss: 1.399555\n","\n","Test set: Average loss: 0.0010, Accuracy: 6378/10000 (64%)\n","\n","Train Epoch: 75 [0/50000 (0%)]\tLoss: 0.825264\n","Train Epoch: 75 [8000/50000 (16%)]\tLoss: 0.557806\n","Train Epoch: 75 [16000/50000 (32%)]\tLoss: 1.082826\n","Train Epoch: 75 [24000/50000 (48%)]\tLoss: 0.728688\n","Train Epoch: 75 [32000/50000 (64%)]\tLoss: 1.182985\n","Train Epoch: 75 [40000/50000 (80%)]\tLoss: 1.056964\n","Train Epoch: 75 [48000/50000 (96%)]\tLoss: 1.435159\n","Train Epoch: 76 [0/50000 (0%)]\tLoss: 1.227555\n","Train Epoch: 76 [8000/50000 (16%)]\tLoss: 0.816067\n","Train Epoch: 76 [16000/50000 (32%)]\tLoss: 1.042214\n","Train Epoch: 76 [24000/50000 (48%)]\tLoss: 0.874009\n","Train Epoch: 76 [32000/50000 (64%)]\tLoss: 1.108357\n","Train Epoch: 76 [40000/50000 (80%)]\tLoss: 0.624134\n","Train Epoch: 76 [48000/50000 (96%)]\tLoss: 1.227559\n","\n","Test set: Average loss: 0.0011, Accuracy: 6347/10000 (63%)\n","\n","Train Epoch: 77 [0/50000 (0%)]\tLoss: 1.725023\n","Train Epoch: 77 [8000/50000 (16%)]\tLoss: 1.554928\n","Train Epoch: 77 [16000/50000 (32%)]\tLoss: 1.372393\n","Train Epoch: 77 [24000/50000 (48%)]\tLoss: 1.645961\n","Train Epoch: 77 [32000/50000 (64%)]\tLoss: 1.522871\n","Train Epoch: 77 [40000/50000 (80%)]\tLoss: 1.162746\n","Train Epoch: 77 [48000/50000 (96%)]\tLoss: 1.295151\n","Train Epoch: 78 [0/50000 (0%)]\tLoss: 1.017731\n","Train Epoch: 78 [8000/50000 (16%)]\tLoss: 1.575895\n","Train Epoch: 78 [16000/50000 (32%)]\tLoss: 0.785903\n","Train Epoch: 78 [24000/50000 (48%)]\tLoss: 0.936666\n","Train Epoch: 78 [32000/50000 (64%)]\tLoss: 1.204915\n","Train Epoch: 78 [40000/50000 (80%)]\tLoss: 1.230171\n","Train Epoch: 78 [48000/50000 (96%)]\tLoss: 1.084632\n","\n","Test set: Average loss: 0.0010, Accuracy: 6484/10000 (65%)\n","\n","Train Epoch: 79 [0/50000 (0%)]\tLoss: 0.953754\n","Train Epoch: 79 [8000/50000 (16%)]\tLoss: 0.846180\n","Train Epoch: 79 [16000/50000 (32%)]\tLoss: 1.027177\n","Train Epoch: 79 [24000/50000 (48%)]\tLoss: 1.223403\n","Train Epoch: 79 [32000/50000 (64%)]\tLoss: 1.494343\n","Train Epoch: 79 [40000/50000 (80%)]\tLoss: 0.998247\n","Train Epoch: 79 [48000/50000 (96%)]\tLoss: 1.448138\n","Train Epoch: 80 [0/50000 (0%)]\tLoss: 1.011806\n","Train Epoch: 80 [8000/50000 (16%)]\tLoss: 1.149847\n","Train Epoch: 80 [16000/50000 (32%)]\tLoss: 1.414699\n","Train Epoch: 80 [24000/50000 (48%)]\tLoss: 0.961629\n","Train Epoch: 80 [32000/50000 (64%)]\tLoss: 0.997420\n","Train Epoch: 80 [40000/50000 (80%)]\tLoss: 1.142969\n","Train Epoch: 80 [48000/50000 (96%)]\tLoss: 0.804447\n","\n","Train set: Average loss: 0.0626, Accuracy: 33052/50000 (66%)\n","\n","\n","Test set: Average loss: 0.0010, Accuracy: 6507/10000 (65%)\n","\n","Freezing SupermaskConv(3, 64, kernel_size=(3, 3), stride=(1, 1), bias=False, sparsity=0.5) before epoch 81\n","Train Epoch: 81 [0/50000 (0%)]\tLoss: 1.339820\n","Train Epoch: 81 [8000/50000 (16%)]\tLoss: 1.093685\n","Train Epoch: 81 [16000/50000 (32%)]\tLoss: 0.920805\n","Train Epoch: 81 [24000/50000 (48%)]\tLoss: 0.652611\n","Train Epoch: 81 [32000/50000 (64%)]\tLoss: 1.126060\n","Train Epoch: 81 [40000/50000 (80%)]\tLoss: 1.108329\n","Train Epoch: 81 [48000/50000 (96%)]\tLoss: 1.537529\n","Train Epoch: 82 [0/50000 (0%)]\tLoss: 1.015809\n","Train Epoch: 82 [8000/50000 (16%)]\tLoss: 0.689472\n","Train Epoch: 82 [16000/50000 (32%)]\tLoss: 0.923733\n","Train Epoch: 82 [24000/50000 (48%)]\tLoss: 1.136797\n","Train Epoch: 82 [32000/50000 (64%)]\tLoss: 1.319974\n","Train Epoch: 82 [40000/50000 (80%)]\tLoss: 1.276547\n","Train Epoch: 82 [48000/50000 (96%)]\tLoss: 0.986115\n","\n","Test set: Average loss: 0.0010, Accuracy: 6575/10000 (66%)\n","\n","Train Epoch: 83 [0/50000 (0%)]\tLoss: 0.690038\n","Train Epoch: 83 [8000/50000 (16%)]\tLoss: 0.979815\n","Train Epoch: 83 [16000/50000 (32%)]\tLoss: 1.534459\n","Train Epoch: 83 [24000/50000 (48%)]\tLoss: 0.807703\n","Train Epoch: 83 [32000/50000 (64%)]\tLoss: 0.871142\n","Train Epoch: 83 [40000/50000 (80%)]\tLoss: 1.076116\n","Train Epoch: 83 [48000/50000 (96%)]\tLoss: 1.884220\n","Train Epoch: 84 [0/50000 (0%)]\tLoss: 1.005959\n","Train Epoch: 84 [8000/50000 (16%)]\tLoss: 0.836375\n","Train Epoch: 84 [16000/50000 (32%)]\tLoss: 1.369107\n","Train Epoch: 84 [24000/50000 (48%)]\tLoss: 0.883298\n","Train Epoch: 84 [32000/50000 (64%)]\tLoss: 0.775305\n","Train Epoch: 84 [40000/50000 (80%)]\tLoss: 1.213267\n","Train Epoch: 84 [48000/50000 (96%)]\tLoss: 1.070502\n","\n","Test set: Average loss: 0.0010, Accuracy: 6509/10000 (65%)\n","\n","Train Epoch: 85 [0/50000 (0%)]\tLoss: 1.117851\n","Train Epoch: 85 [8000/50000 (16%)]\tLoss: 1.327549\n","Train Epoch: 85 [16000/50000 (32%)]\tLoss: 1.011397\n","Train Epoch: 85 [24000/50000 (48%)]\tLoss: 1.003086\n","Train Epoch: 85 [32000/50000 (64%)]\tLoss: 1.115600\n","Train Epoch: 85 [40000/50000 (80%)]\tLoss: 1.157367\n","Train Epoch: 85 [48000/50000 (96%)]\tLoss: 1.116492\n","Train Epoch: 86 [0/50000 (0%)]\tLoss: 1.102169\n","Train Epoch: 86 [8000/50000 (16%)]\tLoss: 1.185414\n","Train Epoch: 86 [16000/50000 (32%)]\tLoss: 0.866194\n","Train Epoch: 86 [24000/50000 (48%)]\tLoss: 1.541748\n","Train Epoch: 86 [32000/50000 (64%)]\tLoss: 0.807078\n","Train Epoch: 86 [40000/50000 (80%)]\tLoss: 1.324332\n","Train Epoch: 86 [48000/50000 (96%)]\tLoss: 1.163628\n","\n","Test set: Average loss: 0.0010, Accuracy: 6454/10000 (65%)\n","\n","Train Epoch: 87 [0/50000 (0%)]\tLoss: 0.951531\n","Train Epoch: 87 [8000/50000 (16%)]\tLoss: 1.259627\n","Train Epoch: 87 [16000/50000 (32%)]\tLoss: 1.029906\n","Train Epoch: 87 [24000/50000 (48%)]\tLoss: 1.244059\n","Train Epoch: 87 [32000/50000 (64%)]\tLoss: 1.107805\n","Train Epoch: 87 [40000/50000 (80%)]\tLoss: 0.592493\n","Train Epoch: 87 [48000/50000 (96%)]\tLoss: 0.733910\n","Train Epoch: 88 [0/50000 (0%)]\tLoss: 0.856907\n","Train Epoch: 88 [8000/50000 (16%)]\tLoss: 0.689723\n","Train Epoch: 88 [16000/50000 (32%)]\tLoss: 1.161488\n","Train Epoch: 88 [24000/50000 (48%)]\tLoss: 0.873194\n","Train Epoch: 88 [32000/50000 (64%)]\tLoss: 1.222457\n","Train Epoch: 88 [40000/50000 (80%)]\tLoss: 0.799375\n","Train Epoch: 88 [48000/50000 (96%)]\tLoss: 0.594606\n","\n","Test set: Average loss: 0.0010, Accuracy: 6729/10000 (67%)\n","\n","Train Epoch: 89 [0/50000 (0%)]\tLoss: 0.688198\n","Train Epoch: 89 [8000/50000 (16%)]\tLoss: 0.749854\n","Train Epoch: 89 [16000/50000 (32%)]\tLoss: 1.239231\n","Train Epoch: 89 [24000/50000 (48%)]\tLoss: 1.282226\n","Train Epoch: 89 [32000/50000 (64%)]\tLoss: 1.445760\n","Train Epoch: 89 [40000/50000 (80%)]\tLoss: 0.789023\n","Train Epoch: 89 [48000/50000 (96%)]\tLoss: 0.906277\n","Train Epoch: 90 [0/50000 (0%)]\tLoss: 1.159890\n","Train Epoch: 90 [8000/50000 (16%)]\tLoss: 1.054208\n","Train Epoch: 90 [16000/50000 (32%)]\tLoss: 1.101747\n","Train Epoch: 90 [24000/50000 (48%)]\tLoss: 1.364455\n","Train Epoch: 90 [32000/50000 (64%)]\tLoss: 0.717291\n","Train Epoch: 90 [40000/50000 (80%)]\tLoss: 0.921219\n","Train Epoch: 90 [48000/50000 (96%)]\tLoss: 1.169868\n","\n","Train set: Average loss: 0.0595, Accuracy: 33401/50000 (67%)\n","\n","\n","Test set: Average loss: 0.0010, Accuracy: 6600/10000 (66%)\n","\n","Train Epoch: 91 [0/50000 (0%)]\tLoss: 1.110340\n","Train Epoch: 91 [8000/50000 (16%)]\tLoss: 1.098142\n","Train Epoch: 91 [16000/50000 (32%)]\tLoss: 1.054633\n","Train Epoch: 91 [24000/50000 (48%)]\tLoss: 0.756473\n","Train Epoch: 91 [32000/50000 (64%)]\tLoss: 0.654815\n","Train Epoch: 91 [40000/50000 (80%)]\tLoss: 0.828380\n","Train Epoch: 91 [48000/50000 (96%)]\tLoss: 0.982403\n","Train Epoch: 92 [0/50000 (0%)]\tLoss: 0.890894\n","Train Epoch: 92 [8000/50000 (16%)]\tLoss: 0.953628\n","Train Epoch: 92 [16000/50000 (32%)]\tLoss: 0.928223\n","Train Epoch: 92 [24000/50000 (48%)]\tLoss: 1.156829\n","Train Epoch: 92 [32000/50000 (64%)]\tLoss: 1.590038\n","Train Epoch: 92 [40000/50000 (80%)]\tLoss: 1.055938\n","Train Epoch: 92 [48000/50000 (96%)]\tLoss: 0.666852\n","\n","Test set: Average loss: 0.0010, Accuracy: 6769/10000 (68%)\n","\n","Train Epoch: 93 [0/50000 (0%)]\tLoss: 0.938466\n","Train Epoch: 93 [8000/50000 (16%)]\tLoss: 0.807034\n","Train Epoch: 93 [16000/50000 (32%)]\tLoss: 0.901743\n","Train Epoch: 93 [24000/50000 (48%)]\tLoss: 1.192565\n","Train Epoch: 93 [32000/50000 (64%)]\tLoss: 1.583931\n","Train Epoch: 93 [40000/50000 (80%)]\tLoss: 0.747988\n","Train Epoch: 93 [48000/50000 (96%)]\tLoss: 1.076169\n","Train Epoch: 94 [0/50000 (0%)]\tLoss: 1.770801\n","Train Epoch: 94 [8000/50000 (16%)]\tLoss: 0.869739\n","Train Epoch: 94 [16000/50000 (32%)]\tLoss: 0.744596\n","Train Epoch: 94 [24000/50000 (48%)]\tLoss: 0.809922\n","Train Epoch: 94 [32000/50000 (64%)]\tLoss: 0.541082\n","Train Epoch: 94 [40000/50000 (80%)]\tLoss: 1.099128\n","Train Epoch: 94 [48000/50000 (96%)]\tLoss: 1.024334\n","\n","Test set: Average loss: 0.0009, Accuracy: 6839/10000 (68%)\n","\n","Train Epoch: 95 [0/50000 (0%)]\tLoss: 1.222012\n","Train Epoch: 95 [8000/50000 (16%)]\tLoss: 1.231357\n","Train Epoch: 95 [16000/50000 (32%)]\tLoss: 0.961966\n","Train Epoch: 95 [24000/50000 (48%)]\tLoss: 1.081017\n","Train Epoch: 95 [32000/50000 (64%)]\tLoss: 1.204429\n","Train Epoch: 95 [40000/50000 (80%)]\tLoss: 1.308112\n","Train Epoch: 95 [48000/50000 (96%)]\tLoss: 1.126080\n","Train Epoch: 96 [0/50000 (0%)]\tLoss: 0.727469\n","Train Epoch: 96 [8000/50000 (16%)]\tLoss: 0.861044\n","Train Epoch: 96 [16000/50000 (32%)]\tLoss: 1.294588\n","Train Epoch: 96 [24000/50000 (48%)]\tLoss: 1.014013\n","Train Epoch: 96 [32000/50000 (64%)]\tLoss: 0.913139\n","Train Epoch: 96 [40000/50000 (80%)]\tLoss: 0.875866\n","Train Epoch: 96 [48000/50000 (96%)]\tLoss: 0.891899\n","\n","Test set: Average loss: 0.0009, Accuracy: 6898/10000 (69%)\n","\n","Train Epoch: 97 [0/50000 (0%)]\tLoss: 0.508612\n","Train Epoch: 97 [8000/50000 (16%)]\tLoss: 0.887272\n","Train Epoch: 97 [16000/50000 (32%)]\tLoss: 1.188528\n","Train Epoch: 97 [24000/50000 (48%)]\tLoss: 1.130934\n","Train Epoch: 97 [32000/50000 (64%)]\tLoss: 0.578534\n","Train Epoch: 97 [40000/50000 (80%)]\tLoss: 1.117870\n","Train Epoch: 97 [48000/50000 (96%)]\tLoss: 1.122400\n","Train Epoch: 98 [0/50000 (0%)]\tLoss: 1.592746\n","Train Epoch: 98 [8000/50000 (16%)]\tLoss: 1.257437\n","Train Epoch: 98 [16000/50000 (32%)]\tLoss: 0.727059\n","Train Epoch: 98 [24000/50000 (48%)]\tLoss: 0.447885\n","Train Epoch: 98 [32000/50000 (64%)]\tLoss: 0.743633\n","Train Epoch: 98 [40000/50000 (80%)]\tLoss: 1.122286\n","Train Epoch: 98 [48000/50000 (96%)]\tLoss: 0.978829\n","\n","Test set: Average loss: 0.0010, Accuracy: 6688/10000 (67%)\n","\n","Train Epoch: 99 [0/50000 (0%)]\tLoss: 1.102897\n","Train Epoch: 99 [8000/50000 (16%)]\tLoss: 1.129164\n","Train Epoch: 99 [16000/50000 (32%)]\tLoss: 1.043447\n","Train Epoch: 99 [24000/50000 (48%)]\tLoss: 0.866190\n","Train Epoch: 99 [32000/50000 (64%)]\tLoss: 1.192179\n","Train Epoch: 99 [40000/50000 (80%)]\tLoss: 0.738781\n","Train Epoch: 99 [48000/50000 (96%)]\tLoss: 1.101663\n","Train Epoch: 100 [0/50000 (0%)]\tLoss: 0.905079\n","Train Epoch: 100 [8000/50000 (16%)]\tLoss: 1.367204\n","Train Epoch: 100 [16000/50000 (32%)]\tLoss: 0.815833\n","Train Epoch: 100 [24000/50000 (48%)]\tLoss: 1.091402\n","Train Epoch: 100 [32000/50000 (64%)]\tLoss: 0.991799\n","Train Epoch: 100 [40000/50000 (80%)]\tLoss: 1.016866\n","Train Epoch: 100 [48000/50000 (96%)]\tLoss: 1.067228\n","\n","Train set: Average loss: 0.0533, Accuracy: 35538/50000 (71%)\n","\n","\n","Test set: Average loss: 0.0009, Accuracy: 6978/10000 (70%)\n","\n","Freezing SupermaskConv(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False, sparsity=0.5) before epoch 101\n","Train Epoch: 101 [0/50000 (0%)]\tLoss: 0.890039\n","Train Epoch: 101 [8000/50000 (16%)]\tLoss: 0.956845\n","Train Epoch: 101 [16000/50000 (32%)]\tLoss: 1.191318\n","Train Epoch: 101 [24000/50000 (48%)]\tLoss: 1.116627\n","Train Epoch: 101 [32000/50000 (64%)]\tLoss: 1.182087\n","Train Epoch: 101 [40000/50000 (80%)]\tLoss: 0.614813\n","Train Epoch: 101 [48000/50000 (96%)]\tLoss: 1.075853\n","Train Epoch: 102 [0/50000 (0%)]\tLoss: 0.889255\n","Train Epoch: 102 [8000/50000 (16%)]\tLoss: 0.677040\n","Train Epoch: 102 [16000/50000 (32%)]\tLoss: 0.994297\n","Train Epoch: 102 [24000/50000 (48%)]\tLoss: 0.795384\n","Train Epoch: 102 [32000/50000 (64%)]\tLoss: 0.972044\n","Train Epoch: 102 [40000/50000 (80%)]\tLoss: 0.578518\n","Train Epoch: 102 [48000/50000 (96%)]\tLoss: 0.922536\n","\n","Test set: Average loss: 0.0009, Accuracy: 6955/10000 (70%)\n","\n","Train Epoch: 103 [0/50000 (0%)]\tLoss: 0.697778\n","Train Epoch: 103 [8000/50000 (16%)]\tLoss: 0.660854\n","Train Epoch: 103 [16000/50000 (32%)]\tLoss: 0.933800\n","Train Epoch: 103 [24000/50000 (48%)]\tLoss: 0.803717\n","Train Epoch: 103 [32000/50000 (64%)]\tLoss: 0.981788\n","Train Epoch: 103 [40000/50000 (80%)]\tLoss: 1.124969\n","Train Epoch: 103 [48000/50000 (96%)]\tLoss: 0.869860\n","Train Epoch: 104 [0/50000 (0%)]\tLoss: 0.616819\n","Train Epoch: 104 [8000/50000 (16%)]\tLoss: 1.126300\n","Train Epoch: 104 [16000/50000 (32%)]\tLoss: 1.218631\n","Train Epoch: 104 [24000/50000 (48%)]\tLoss: 0.985200\n","Train Epoch: 104 [32000/50000 (64%)]\tLoss: 0.886165\n","Train Epoch: 104 [40000/50000 (80%)]\tLoss: 1.369772\n","Train Epoch: 104 [48000/50000 (96%)]\tLoss: 0.963460\n","\n","Test set: Average loss: 0.0009, Accuracy: 6869/10000 (69%)\n","\n","Train Epoch: 105 [0/50000 (0%)]\tLoss: 1.279049\n","Train Epoch: 105 [8000/50000 (16%)]\tLoss: 0.905660\n","Train Epoch: 105 [16000/50000 (32%)]\tLoss: 0.684851\n","Train Epoch: 105 [24000/50000 (48%)]\tLoss: 1.141030\n","Train Epoch: 105 [32000/50000 (64%)]\tLoss: 0.807071\n","Train Epoch: 105 [40000/50000 (80%)]\tLoss: 0.742270\n","Train Epoch: 105 [48000/50000 (96%)]\tLoss: 0.932947\n","Train Epoch: 106 [0/50000 (0%)]\tLoss: 0.595223\n","Train Epoch: 106 [8000/50000 (16%)]\tLoss: 0.913714\n","Train Epoch: 106 [16000/50000 (32%)]\tLoss: 0.893262\n","Train Epoch: 106 [24000/50000 (48%)]\tLoss: 1.098423\n","Train Epoch: 106 [32000/50000 (64%)]\tLoss: 0.975181\n","Train Epoch: 106 [40000/50000 (80%)]\tLoss: 0.636770\n","Train Epoch: 106 [48000/50000 (96%)]\tLoss: 0.870941\n","\n","Test set: Average loss: 0.0009, Accuracy: 7023/10000 (70%)\n","\n","Train Epoch: 107 [0/50000 (0%)]\tLoss: 1.218243\n","Train Epoch: 107 [8000/50000 (16%)]\tLoss: 0.549520\n","Train Epoch: 107 [16000/50000 (32%)]\tLoss: 1.418790\n","Train Epoch: 107 [24000/50000 (48%)]\tLoss: 0.861269\n","Train Epoch: 107 [32000/50000 (64%)]\tLoss: 0.558355\n","Train Epoch: 107 [40000/50000 (80%)]\tLoss: 0.312292\n","Train Epoch: 107 [48000/50000 (96%)]\tLoss: 0.888863\n","Train Epoch: 108 [0/50000 (0%)]\tLoss: 0.645230\n","Train Epoch: 108 [8000/50000 (16%)]\tLoss: 1.055120\n","Train Epoch: 108 [16000/50000 (32%)]\tLoss: 1.054368\n","Train Epoch: 108 [24000/50000 (48%)]\tLoss: 0.882466\n","Train Epoch: 108 [32000/50000 (64%)]\tLoss: 0.963229\n","Train Epoch: 108 [40000/50000 (80%)]\tLoss: 1.139250\n","Train Epoch: 108 [48000/50000 (96%)]\tLoss: 0.764445\n","\n","Test set: Average loss: 0.0009, Accuracy: 7045/10000 (70%)\n","\n","Train Epoch: 109 [0/50000 (0%)]\tLoss: 1.054396\n","Train Epoch: 109 [8000/50000 (16%)]\tLoss: 0.812830\n","Train Epoch: 109 [16000/50000 (32%)]\tLoss: 1.179061\n","Train Epoch: 109 [24000/50000 (48%)]\tLoss: 0.591205\n","Train Epoch: 109 [32000/50000 (64%)]\tLoss: 1.322912\n","Train Epoch: 109 [40000/50000 (80%)]\tLoss: 0.812847\n","Train Epoch: 109 [48000/50000 (96%)]\tLoss: 1.011941\n","Train Epoch: 110 [0/50000 (0%)]\tLoss: 0.603307\n","Train Epoch: 110 [8000/50000 (16%)]\tLoss: 0.899971\n","Train Epoch: 110 [16000/50000 (32%)]\tLoss: 0.836545\n","Train Epoch: 110 [24000/50000 (48%)]\tLoss: 0.631800\n","Train Epoch: 110 [32000/50000 (64%)]\tLoss: 0.467971\n","Train Epoch: 110 [40000/50000 (80%)]\tLoss: 0.912941\n","Train Epoch: 110 [48000/50000 (96%)]\tLoss: 0.770704\n","\n","Train set: Average loss: 0.0511, Accuracy: 36216/50000 (72%)\n","\n","\n","Test set: Average loss: 0.0009, Accuracy: 7110/10000 (71%)\n","\n","Train Epoch: 111 [0/50000 (0%)]\tLoss: 1.601474\n","Train Epoch: 111 [8000/50000 (16%)]\tLoss: 0.359125\n","Train Epoch: 111 [16000/50000 (32%)]\tLoss: 1.205962\n","Train Epoch: 111 [24000/50000 (48%)]\tLoss: 1.006667\n","Train Epoch: 111 [32000/50000 (64%)]\tLoss: 0.952557\n","Train Epoch: 111 [40000/50000 (80%)]\tLoss: 1.117662\n","Train Epoch: 111 [48000/50000 (96%)]\tLoss: 0.890944\n","Train Epoch: 112 [0/50000 (0%)]\tLoss: 1.355388\n","Train Epoch: 112 [8000/50000 (16%)]\tLoss: 1.538396\n","Train Epoch: 112 [16000/50000 (32%)]\tLoss: 0.949786\n","Train Epoch: 112 [24000/50000 (48%)]\tLoss: 1.118725\n","Train Epoch: 112 [32000/50000 (64%)]\tLoss: 0.733788\n","Train Epoch: 112 [40000/50000 (80%)]\tLoss: 0.740413\n","Train Epoch: 112 [48000/50000 (96%)]\tLoss: 0.919500\n","\n","Test set: Average loss: 0.0009, Accuracy: 7111/10000 (71%)\n","\n","Train Epoch: 113 [0/50000 (0%)]\tLoss: 0.799255\n","Train Epoch: 113 [8000/50000 (16%)]\tLoss: 0.919615\n","Train Epoch: 113 [16000/50000 (32%)]\tLoss: 0.555827\n","Train Epoch: 113 [24000/50000 (48%)]\tLoss: 0.943916\n","Train Epoch: 113 [32000/50000 (64%)]\tLoss: 1.249936\n","Train Epoch: 113 [40000/50000 (80%)]\tLoss: 0.644182\n","Train Epoch: 113 [48000/50000 (96%)]\tLoss: 1.012507\n","Train Epoch: 114 [0/50000 (0%)]\tLoss: 0.730769\n","Train Epoch: 114 [8000/50000 (16%)]\tLoss: 0.764659\n","Train Epoch: 114 [16000/50000 (32%)]\tLoss: 0.690289\n","Train Epoch: 114 [24000/50000 (48%)]\tLoss: 0.805167\n","Train Epoch: 114 [32000/50000 (64%)]\tLoss: 0.947176\n","Train Epoch: 114 [40000/50000 (80%)]\tLoss: 1.161173\n","Train Epoch: 114 [48000/50000 (96%)]\tLoss: 1.433785\n","\n","Test set: Average loss: 0.0009, Accuracy: 7196/10000 (72%)\n","\n","Train Epoch: 115 [0/50000 (0%)]\tLoss: 0.717649\n","Train Epoch: 115 [8000/50000 (16%)]\tLoss: 1.087885\n","Train Epoch: 115 [16000/50000 (32%)]\tLoss: 0.946158\n","Train Epoch: 115 [24000/50000 (48%)]\tLoss: 0.979074\n","Train Epoch: 115 [32000/50000 (64%)]\tLoss: 1.168785\n","Train Epoch: 115 [40000/50000 (80%)]\tLoss: 0.817985\n","Train Epoch: 115 [48000/50000 (96%)]\tLoss: 0.829621\n","Train Epoch: 116 [0/50000 (0%)]\tLoss: 0.598045\n","Train Epoch: 116 [8000/50000 (16%)]\tLoss: 0.603411\n","Train Epoch: 116 [16000/50000 (32%)]\tLoss: 0.868689\n","Train Epoch: 116 [24000/50000 (48%)]\tLoss: 0.718399\n","Train Epoch: 116 [32000/50000 (64%)]\tLoss: 0.519805\n","Train Epoch: 116 [40000/50000 (80%)]\tLoss: 0.981266\n","Train Epoch: 116 [48000/50000 (96%)]\tLoss: 0.425655\n","\n","Test set: Average loss: 0.0009, Accuracy: 7046/10000 (70%)\n","\n","Train Epoch: 117 [0/50000 (0%)]\tLoss: 1.142789\n","Train Epoch: 117 [8000/50000 (16%)]\tLoss: 0.800595\n","Train Epoch: 117 [16000/50000 (32%)]\tLoss: 1.096879\n","Train Epoch: 117 [24000/50000 (48%)]\tLoss: 1.085535\n","Train Epoch: 117 [32000/50000 (64%)]\tLoss: 0.521090\n","Train Epoch: 117 [40000/50000 (80%)]\tLoss: 1.105235\n","Train Epoch: 117 [48000/50000 (96%)]\tLoss: 0.737775\n","Train Epoch: 118 [0/50000 (0%)]\tLoss: 0.935581\n","Train Epoch: 118 [8000/50000 (16%)]\tLoss: 0.917511\n","Train Epoch: 118 [16000/50000 (32%)]\tLoss: 0.911207\n","Train Epoch: 118 [24000/50000 (48%)]\tLoss: 0.323655\n","Train Epoch: 118 [32000/50000 (64%)]\tLoss: 0.701199\n","Train Epoch: 118 [40000/50000 (80%)]\tLoss: 0.931757\n","Train Epoch: 118 [48000/50000 (96%)]\tLoss: 0.723939\n","\n","Test set: Average loss: 0.0009, Accuracy: 7153/10000 (72%)\n","\n","Train Epoch: 119 [0/50000 (0%)]\tLoss: 0.839849\n","Train Epoch: 119 [8000/50000 (16%)]\tLoss: 0.726958\n","Train Epoch: 119 [16000/50000 (32%)]\tLoss: 1.327005\n","Train Epoch: 119 [24000/50000 (48%)]\tLoss: 1.281517\n","Train Epoch: 119 [32000/50000 (64%)]\tLoss: 1.074652\n","Train Epoch: 119 [40000/50000 (80%)]\tLoss: 0.663360\n","Train Epoch: 119 [48000/50000 (96%)]\tLoss: 0.649215\n","Train Epoch: 120 [0/50000 (0%)]\tLoss: 1.200369\n","Train Epoch: 120 [8000/50000 (16%)]\tLoss: 0.600182\n","Train Epoch: 120 [16000/50000 (32%)]\tLoss: 0.761374\n","Train Epoch: 120 [24000/50000 (48%)]\tLoss: 1.131637\n","Train Epoch: 120 [32000/50000 (64%)]\tLoss: 0.680766\n","Train Epoch: 120 [40000/50000 (80%)]\tLoss: 0.458889\n","Train Epoch: 120 [48000/50000 (96%)]\tLoss: 1.082326\n","\n","Train set: Average loss: 0.0486, Accuracy: 36960/50000 (74%)\n","\n","\n","Test set: Average loss: 0.0008, Accuracy: 7195/10000 (72%)\n","\n","Freezing SupermaskLinear(in_features=256, out_features=10, bias=False, sparsity=0.5) before epoch 121\n","Train Epoch: 121 [0/50000 (0%)]\tLoss: 0.856906\n","Train Epoch: 121 [8000/50000 (16%)]\tLoss: 0.874196\n","Train Epoch: 121 [16000/50000 (32%)]\tLoss: 0.898614\n","Train Epoch: 121 [24000/50000 (48%)]\tLoss: 0.612201\n","Train Epoch: 121 [32000/50000 (64%)]\tLoss: 1.260094\n","Train Epoch: 121 [40000/50000 (80%)]\tLoss: 0.836078\n","Train Epoch: 121 [48000/50000 (96%)]\tLoss: 1.026344\n","Train Epoch: 122 [0/50000 (0%)]\tLoss: 0.963397\n","Train Epoch: 122 [8000/50000 (16%)]\tLoss: 0.875838\n","Train Epoch: 122 [16000/50000 (32%)]\tLoss: 1.070061\n","Train Epoch: 122 [24000/50000 (48%)]\tLoss: 0.450915\n","Train Epoch: 122 [32000/50000 (64%)]\tLoss: 0.654649\n","Train Epoch: 122 [40000/50000 (80%)]\tLoss: 0.923467\n","Train Epoch: 122 [48000/50000 (96%)]\tLoss: 0.623302\n","\n","Test set: Average loss: 0.0008, Accuracy: 7226/10000 (72%)\n","\n","Train Epoch: 123 [0/50000 (0%)]\tLoss: 0.986921\n","Train Epoch: 123 [8000/50000 (16%)]\tLoss: 0.713464\n","Train Epoch: 123 [16000/50000 (32%)]\tLoss: 0.581815\n","Train Epoch: 123 [24000/50000 (48%)]\tLoss: 0.862281\n","Train Epoch: 123 [32000/50000 (64%)]\tLoss: 0.745677\n","Train Epoch: 123 [40000/50000 (80%)]\tLoss: 0.864726\n","Train Epoch: 123 [48000/50000 (96%)]\tLoss: 0.674685\n","Train Epoch: 124 [0/50000 (0%)]\tLoss: 1.152149\n","Train Epoch: 124 [8000/50000 (16%)]\tLoss: 0.950914\n","Train Epoch: 124 [16000/50000 (32%)]\tLoss: 0.591571\n","Train Epoch: 124 [24000/50000 (48%)]\tLoss: 0.823646\n","Train Epoch: 124 [32000/50000 (64%)]\tLoss: 0.580119\n","Train Epoch: 124 [40000/50000 (80%)]\tLoss: 0.647296\n","Train Epoch: 124 [48000/50000 (96%)]\tLoss: 0.458663\n","\n","Test set: Average loss: 0.0008, Accuracy: 7335/10000 (73%)\n","\n","Train Epoch: 125 [0/50000 (0%)]\tLoss: 0.542298\n","Train Epoch: 125 [8000/50000 (16%)]\tLoss: 1.005300\n","Train Epoch: 125 [16000/50000 (32%)]\tLoss: 0.627256\n","Train Epoch: 125 [24000/50000 (48%)]\tLoss: 0.982723\n","Train Epoch: 125 [32000/50000 (64%)]\tLoss: 0.831282\n","Train Epoch: 125 [40000/50000 (80%)]\tLoss: 0.797765\n","Train Epoch: 125 [48000/50000 (96%)]\tLoss: 0.919746\n","Train Epoch: 126 [0/50000 (0%)]\tLoss: 1.594037\n","Train Epoch: 126 [8000/50000 (16%)]\tLoss: 0.976392\n","Train Epoch: 126 [16000/50000 (32%)]\tLoss: 1.216519\n","Train Epoch: 126 [24000/50000 (48%)]\tLoss: 0.607484\n","Train Epoch: 126 [32000/50000 (64%)]\tLoss: 0.623815\n","Train Epoch: 126 [40000/50000 (80%)]\tLoss: 0.706983\n","Train Epoch: 126 [48000/50000 (96%)]\tLoss: 0.431315\n","\n","Test set: Average loss: 0.0008, Accuracy: 7317/10000 (73%)\n","\n","Train Epoch: 127 [0/50000 (0%)]\tLoss: 1.183212\n","Train Epoch: 127 [8000/50000 (16%)]\tLoss: 0.751806\n","Train Epoch: 127 [16000/50000 (32%)]\tLoss: 1.140345\n","Train Epoch: 127 [24000/50000 (48%)]\tLoss: 0.962636\n","Train Epoch: 127 [32000/50000 (64%)]\tLoss: 0.737465\n","Train Epoch: 127 [40000/50000 (80%)]\tLoss: 0.649117\n","Train Epoch: 127 [48000/50000 (96%)]\tLoss: 0.558096\n","Train Epoch: 128 [0/50000 (0%)]\tLoss: 0.912782\n","Train Epoch: 128 [8000/50000 (16%)]\tLoss: 0.827860\n","Train Epoch: 128 [16000/50000 (32%)]\tLoss: 0.819218\n","Train Epoch: 128 [24000/50000 (48%)]\tLoss: 0.520376\n","Train Epoch: 128 [32000/50000 (64%)]\tLoss: 0.803843\n","Train Epoch: 128 [40000/50000 (80%)]\tLoss: 0.981770\n","Train Epoch: 128 [48000/50000 (96%)]\tLoss: 0.922228\n","\n","Test set: Average loss: 0.0008, Accuracy: 7221/10000 (72%)\n","\n","Train Epoch: 129 [0/50000 (0%)]\tLoss: 0.830057\n","Train Epoch: 129 [8000/50000 (16%)]\tLoss: 0.983157\n","Train Epoch: 129 [16000/50000 (32%)]\tLoss: 0.705005\n","Train Epoch: 129 [24000/50000 (48%)]\tLoss: 0.684853\n","Train Epoch: 129 [32000/50000 (64%)]\tLoss: 0.890986\n","Train Epoch: 129 [40000/50000 (80%)]\tLoss: 0.766680\n","Train Epoch: 129 [48000/50000 (96%)]\tLoss: 1.211908\n","Train Epoch: 130 [0/50000 (0%)]\tLoss: 1.156115\n","Train Epoch: 130 [8000/50000 (16%)]\tLoss: 0.748589\n","Train Epoch: 130 [16000/50000 (32%)]\tLoss: 0.887546\n","Train Epoch: 130 [24000/50000 (48%)]\tLoss: 1.034024\n","Train Epoch: 130 [32000/50000 (64%)]\tLoss: 0.785583\n","Train Epoch: 130 [40000/50000 (80%)]\tLoss: 0.851022\n","Train Epoch: 130 [48000/50000 (96%)]\tLoss: 1.105827\n","\n","Train set: Average loss: 0.0457, Accuracy: 37879/50000 (76%)\n","\n","\n","Test set: Average loss: 0.0008, Accuracy: 7344/10000 (73%)\n","\n","Train Epoch: 131 [0/50000 (0%)]\tLoss: 0.863257\n","Train Epoch: 131 [8000/50000 (16%)]\tLoss: 0.879586\n","Train Epoch: 131 [16000/50000 (32%)]\tLoss: 0.964300\n","Train Epoch: 131 [24000/50000 (48%)]\tLoss: 0.536175\n","Train Epoch: 131 [32000/50000 (64%)]\tLoss: 0.952649\n","Train Epoch: 131 [40000/50000 (80%)]\tLoss: 0.692995\n","Train Epoch: 131 [48000/50000 (96%)]\tLoss: 1.165154\n","Train Epoch: 132 [0/50000 (0%)]\tLoss: 0.741223\n","Train Epoch: 132 [8000/50000 (16%)]\tLoss: 1.005875\n","Train Epoch: 132 [16000/50000 (32%)]\tLoss: 1.304336\n","Train Epoch: 132 [24000/50000 (48%)]\tLoss: 0.860829\n","Train Epoch: 132 [32000/50000 (64%)]\tLoss: 0.502256\n","Train Epoch: 132 [40000/50000 (80%)]\tLoss: 0.792199\n","Train Epoch: 132 [48000/50000 (96%)]\tLoss: 0.834476\n","\n","Test set: Average loss: 0.0008, Accuracy: 7381/10000 (74%)\n","\n","Train Epoch: 133 [0/50000 (0%)]\tLoss: 0.523730\n","Train Epoch: 133 [8000/50000 (16%)]\tLoss: 1.095544\n","Train Epoch: 133 [16000/50000 (32%)]\tLoss: 0.546794\n","Train Epoch: 133 [24000/50000 (48%)]\tLoss: 0.689883\n","Train Epoch: 133 [32000/50000 (64%)]\tLoss: 1.353389\n","Train Epoch: 133 [40000/50000 (80%)]\tLoss: 0.887958\n","Train Epoch: 133 [48000/50000 (96%)]\tLoss: 1.182754\n","Train Epoch: 134 [0/50000 (0%)]\tLoss: 0.867612\n","Train Epoch: 134 [8000/50000 (16%)]\tLoss: 0.713572\n","Train Epoch: 134 [16000/50000 (32%)]\tLoss: 0.365145\n","Train Epoch: 134 [24000/50000 (48%)]\tLoss: 0.652553\n","Train Epoch: 134 [32000/50000 (64%)]\tLoss: 0.991949\n","Train Epoch: 134 [40000/50000 (80%)]\tLoss: 0.652450\n","Train Epoch: 134 [48000/50000 (96%)]\tLoss: 0.928300\n","\n","Test set: Average loss: 0.0008, Accuracy: 7386/10000 (74%)\n","\n","Train Epoch: 135 [0/50000 (0%)]\tLoss: 0.504481\n","Train Epoch: 135 [8000/50000 (16%)]\tLoss: 0.753216\n","Train Epoch: 135 [16000/50000 (32%)]\tLoss: 0.406021\n","Train Epoch: 135 [24000/50000 (48%)]\tLoss: 0.688833\n","Train Epoch: 135 [32000/50000 (64%)]\tLoss: 0.511551\n","Train Epoch: 135 [40000/50000 (80%)]\tLoss: 0.938719\n","Train Epoch: 135 [48000/50000 (96%)]\tLoss: 0.730076\n","Train Epoch: 136 [0/50000 (0%)]\tLoss: 0.783442\n","Train Epoch: 136 [8000/50000 (16%)]\tLoss: 0.705317\n","Train Epoch: 136 [16000/50000 (32%)]\tLoss: 0.409891\n","Train Epoch: 136 [24000/50000 (48%)]\tLoss: 0.570418\n","Train Epoch: 136 [32000/50000 (64%)]\tLoss: 0.943001\n","Train Epoch: 136 [40000/50000 (80%)]\tLoss: 0.906353\n","Train Epoch: 136 [48000/50000 (96%)]\tLoss: 0.989637\n","\n","Test set: Average loss: 0.0008, Accuracy: 7342/10000 (73%)\n","\n","Train Epoch: 137 [0/50000 (0%)]\tLoss: 0.830542\n","Train Epoch: 137 [8000/50000 (16%)]\tLoss: 1.161607\n","Train Epoch: 137 [16000/50000 (32%)]\tLoss: 0.801044\n","Train Epoch: 137 [24000/50000 (48%)]\tLoss: 1.484706\n","Train Epoch: 137 [32000/50000 (64%)]\tLoss: 0.622217\n","Train Epoch: 137 [40000/50000 (80%)]\tLoss: 0.751143\n","Train Epoch: 137 [48000/50000 (96%)]\tLoss: 0.981720\n","Train Epoch: 138 [0/50000 (0%)]\tLoss: 0.494432\n","Train Epoch: 138 [8000/50000 (16%)]\tLoss: 0.926629\n","Train Epoch: 138 [16000/50000 (32%)]\tLoss: 0.704390\n","Train Epoch: 138 [24000/50000 (48%)]\tLoss: 0.786267\n","Train Epoch: 138 [32000/50000 (64%)]\tLoss: 0.660883\n","Train Epoch: 138 [40000/50000 (80%)]\tLoss: 1.155801\n","Train Epoch: 138 [48000/50000 (96%)]\tLoss: 0.539297\n","\n","Test set: Average loss: 0.0008, Accuracy: 7364/10000 (74%)\n","\n","Train Epoch: 139 [0/50000 (0%)]\tLoss: 0.560130\n","Train Epoch: 139 [8000/50000 (16%)]\tLoss: 0.872119\n","Train Epoch: 139 [16000/50000 (32%)]\tLoss: 0.882191\n","Train Epoch: 139 [24000/50000 (48%)]\tLoss: 0.582896\n","Train Epoch: 139 [32000/50000 (64%)]\tLoss: 0.646707\n","Train Epoch: 139 [40000/50000 (80%)]\tLoss: 0.737480\n","Train Epoch: 139 [48000/50000 (96%)]\tLoss: 0.777841\n","Train Epoch: 140 [0/50000 (0%)]\tLoss: 0.517272\n","Train Epoch: 140 [8000/50000 (16%)]\tLoss: 0.635057\n","Train Epoch: 140 [16000/50000 (32%)]\tLoss: 1.111189\n","Train Epoch: 140 [24000/50000 (48%)]\tLoss: 0.261146\n","Train Epoch: 140 [32000/50000 (64%)]\tLoss: 0.451484\n","Train Epoch: 140 [40000/50000 (80%)]\tLoss: 0.376673\n","Train Epoch: 140 [48000/50000 (96%)]\tLoss: 0.384856\n","\n","Train set: Average loss: 0.0436, Accuracy: 38589/50000 (77%)\n","\n","\n","Test set: Average loss: 0.0008, Accuracy: 7450/10000 (74%)\n","\n","Freezing SupermaskLinear(in_features=256, out_features=256, bias=False, sparsity=0.5) before epoch 141\n","Train Epoch: 141 [0/50000 (0%)]\tLoss: 1.115994\n","Train Epoch: 141 [8000/50000 (16%)]\tLoss: 0.569105\n","Train Epoch: 141 [16000/50000 (32%)]\tLoss: 0.981947\n","Train Epoch: 141 [24000/50000 (48%)]\tLoss: 0.951641\n","Train Epoch: 141 [32000/50000 (64%)]\tLoss: 0.469414\n","Train Epoch: 141 [40000/50000 (80%)]\tLoss: 0.672010\n","Train Epoch: 141 [48000/50000 (96%)]\tLoss: 0.676844\n","Train Epoch: 142 [0/50000 (0%)]\tLoss: 0.649319\n","Train Epoch: 142 [8000/50000 (16%)]\tLoss: 0.855079\n","Train Epoch: 142 [16000/50000 (32%)]\tLoss: 0.833188\n","Train Epoch: 142 [24000/50000 (48%)]\tLoss: 0.788065\n","Train Epoch: 142 [32000/50000 (64%)]\tLoss: 0.625778\n","Train Epoch: 142 [40000/50000 (80%)]\tLoss: 0.873605\n","Train Epoch: 142 [48000/50000 (96%)]\tLoss: 0.829416\n","\n","Test set: Average loss: 0.0008, Accuracy: 7461/10000 (75%)\n","\n","Train Epoch: 143 [0/50000 (0%)]\tLoss: 0.708110\n","Train Epoch: 143 [8000/50000 (16%)]\tLoss: 0.740691\n","Train Epoch: 143 [16000/50000 (32%)]\tLoss: 1.243505\n","Train Epoch: 143 [24000/50000 (48%)]\tLoss: 0.811338\n","Train Epoch: 143 [32000/50000 (64%)]\tLoss: 1.311197\n","Train Epoch: 143 [40000/50000 (80%)]\tLoss: 0.945680\n","Train Epoch: 143 [48000/50000 (96%)]\tLoss: 0.795291\n","Train Epoch: 144 [0/50000 (0%)]\tLoss: 0.868492\n","Train Epoch: 144 [8000/50000 (16%)]\tLoss: 0.602486\n","Train Epoch: 144 [16000/50000 (32%)]\tLoss: 0.620599\n","Train Epoch: 144 [24000/50000 (48%)]\tLoss: 0.616850\n","Train Epoch: 144 [32000/50000 (64%)]\tLoss: 0.714775\n","Train Epoch: 144 [40000/50000 (80%)]\tLoss: 0.304365\n","Train Epoch: 144 [48000/50000 (96%)]\tLoss: 1.006179\n","\n","Test set: Average loss: 0.0008, Accuracy: 7426/10000 (74%)\n","\n","Train Epoch: 145 [0/50000 (0%)]\tLoss: 0.339650\n","Train Epoch: 145 [8000/50000 (16%)]\tLoss: 1.110025\n","Train Epoch: 145 [16000/50000 (32%)]\tLoss: 0.852661\n","Train Epoch: 145 [24000/50000 (48%)]\tLoss: 0.813439\n","Train Epoch: 145 [32000/50000 (64%)]\tLoss: 0.656694\n","Train Epoch: 145 [40000/50000 (80%)]\tLoss: 0.881743\n","Train Epoch: 145 [48000/50000 (96%)]\tLoss: 0.457209\n","Train Epoch: 146 [0/50000 (0%)]\tLoss: 1.126315\n","Train Epoch: 146 [8000/50000 (16%)]\tLoss: 0.778236\n","Train Epoch: 146 [16000/50000 (32%)]\tLoss: 0.866892\n","Train Epoch: 146 [24000/50000 (48%)]\tLoss: 0.650636\n","Train Epoch: 146 [32000/50000 (64%)]\tLoss: 0.909510\n","Train Epoch: 146 [40000/50000 (80%)]\tLoss: 0.986699\n","Train Epoch: 146 [48000/50000 (96%)]\tLoss: 0.339220\n","\n","Test set: Average loss: 0.0008, Accuracy: 7449/10000 (74%)\n","\n","Train Epoch: 147 [0/50000 (0%)]\tLoss: 0.670285\n","Train Epoch: 147 [8000/50000 (16%)]\tLoss: 1.289261\n","Train Epoch: 147 [16000/50000 (32%)]\tLoss: 0.968724\n","Train Epoch: 147 [24000/50000 (48%)]\tLoss: 0.705087\n","Train Epoch: 147 [32000/50000 (64%)]\tLoss: 0.651096\n","Train Epoch: 147 [40000/50000 (80%)]\tLoss: 0.717216\n","Train Epoch: 147 [48000/50000 (96%)]\tLoss: 1.036174\n","Train Epoch: 148 [0/50000 (0%)]\tLoss: 0.501358\n","Train Epoch: 148 [8000/50000 (16%)]\tLoss: 0.965286\n","Train Epoch: 148 [16000/50000 (32%)]\tLoss: 0.567746\n","Train Epoch: 148 [24000/50000 (48%)]\tLoss: 0.697164\n","Train Epoch: 148 [32000/50000 (64%)]\tLoss: 0.624159\n","Train Epoch: 148 [40000/50000 (80%)]\tLoss: 0.813341\n","Train Epoch: 148 [48000/50000 (96%)]\tLoss: 0.730909\n","\n","Test set: Average loss: 0.0008, Accuracy: 7469/10000 (75%)\n","\n","Train Epoch: 149 [0/50000 (0%)]\tLoss: 0.603393\n","Train Epoch: 149 [8000/50000 (16%)]\tLoss: 0.966550\n","Train Epoch: 149 [16000/50000 (32%)]\tLoss: 0.775523\n","Train Epoch: 149 [24000/50000 (48%)]\tLoss: 0.922591\n","Train Epoch: 149 [32000/50000 (64%)]\tLoss: 0.832509\n","Train Epoch: 149 [40000/50000 (80%)]\tLoss: 0.910561\n","Train Epoch: 149 [48000/50000 (96%)]\tLoss: 0.470512\n","Train Epoch: 150 [0/50000 (0%)]\tLoss: 0.646270\n","Train Epoch: 150 [8000/50000 (16%)]\tLoss: 0.720411\n","Train Epoch: 150 [16000/50000 (32%)]\tLoss: 0.841799\n","Train Epoch: 150 [24000/50000 (48%)]\tLoss: 0.798721\n","Train Epoch: 150 [32000/50000 (64%)]\tLoss: 0.932719\n","Train Epoch: 150 [40000/50000 (80%)]\tLoss: 1.129377\n","Train Epoch: 150 [48000/50000 (96%)]\tLoss: 0.580047\n","\n","Train set: Average loss: 0.0427, Accuracy: 38861/50000 (78%)\n","\n","\n","Test set: Average loss: 0.0008, Accuracy: 7487/10000 (75%)\n","\n","Train Epoch: 151 [0/50000 (0%)]\tLoss: 1.065248\n","Train Epoch: 151 [8000/50000 (16%)]\tLoss: 1.019820\n","Train Epoch: 151 [16000/50000 (32%)]\tLoss: 0.750241\n","Train Epoch: 151 [24000/50000 (48%)]\tLoss: 0.628251\n","Train Epoch: 151 [32000/50000 (64%)]\tLoss: 0.798038\n","Train Epoch: 151 [40000/50000 (80%)]\tLoss: 0.631048\n","Train Epoch: 151 [48000/50000 (96%)]\tLoss: 0.840696\n","Train Epoch: 152 [0/50000 (0%)]\tLoss: 0.999284\n","Train Epoch: 152 [8000/50000 (16%)]\tLoss: 0.499667\n","Train Epoch: 152 [16000/50000 (32%)]\tLoss: 0.456302\n","Train Epoch: 152 [24000/50000 (48%)]\tLoss: 0.723068\n","Train Epoch: 152 [32000/50000 (64%)]\tLoss: 1.140130\n","Train Epoch: 152 [40000/50000 (80%)]\tLoss: 0.760927\n","Train Epoch: 152 [48000/50000 (96%)]\tLoss: 0.651389\n","\n","Test set: Average loss: 0.0008, Accuracy: 7484/10000 (75%)\n","\n","Train Epoch: 153 [0/50000 (0%)]\tLoss: 0.614049\n","Train Epoch: 153 [8000/50000 (16%)]\tLoss: 0.640629\n","Train Epoch: 153 [16000/50000 (32%)]\tLoss: 0.961397\n","Train Epoch: 153 [24000/50000 (48%)]\tLoss: 0.606487\n","Train Epoch: 153 [32000/50000 (64%)]\tLoss: 0.695410\n","Train Epoch: 153 [40000/50000 (80%)]\tLoss: 0.449633\n","Train Epoch: 153 [48000/50000 (96%)]\tLoss: 0.879993\n","Train Epoch: 154 [0/50000 (0%)]\tLoss: 0.615816\n","Train Epoch: 154 [8000/50000 (16%)]\tLoss: 0.664181\n","Train Epoch: 154 [16000/50000 (32%)]\tLoss: 0.917707\n","Train Epoch: 154 [24000/50000 (48%)]\tLoss: 1.056888\n","Train Epoch: 154 [32000/50000 (64%)]\tLoss: 0.937115\n","Train Epoch: 154 [40000/50000 (80%)]\tLoss: 0.492303\n","Train Epoch: 154 [48000/50000 (96%)]\tLoss: 0.847153\n","\n","Test set: Average loss: 0.0008, Accuracy: 7489/10000 (75%)\n","\n","Train Epoch: 155 [0/50000 (0%)]\tLoss: 0.907225\n","Train Epoch: 155 [8000/50000 (16%)]\tLoss: 0.617959\n","Train Epoch: 155 [16000/50000 (32%)]\tLoss: 0.529307\n","Train Epoch: 155 [24000/50000 (48%)]\tLoss: 0.659755\n","Train Epoch: 155 [32000/50000 (64%)]\tLoss: 0.706752\n","Train Epoch: 155 [40000/50000 (80%)]\tLoss: 0.470266\n","Train Epoch: 155 [48000/50000 (96%)]\tLoss: 0.392842\n","Train Epoch: 156 [0/50000 (0%)]\tLoss: 0.771489\n","Train Epoch: 156 [8000/50000 (16%)]\tLoss: 0.566821\n","Train Epoch: 156 [16000/50000 (32%)]\tLoss: 0.606205\n","Train Epoch: 156 [24000/50000 (48%)]\tLoss: 0.591497\n","Train Epoch: 156 [32000/50000 (64%)]\tLoss: 0.621833\n","Train Epoch: 156 [40000/50000 (80%)]\tLoss: 0.573541\n","Train Epoch: 156 [48000/50000 (96%)]\tLoss: 0.712090\n","\n","Test set: Average loss: 0.0008, Accuracy: 7515/10000 (75%)\n","\n","Train Epoch: 157 [0/50000 (0%)]\tLoss: 1.014436\n","Train Epoch: 157 [8000/50000 (16%)]\tLoss: 0.677174\n","Train Epoch: 157 [16000/50000 (32%)]\tLoss: 0.874957\n","Train Epoch: 157 [24000/50000 (48%)]\tLoss: 0.616359\n","Train Epoch: 157 [32000/50000 (64%)]\tLoss: 0.417996\n","Train Epoch: 157 [40000/50000 (80%)]\tLoss: 0.796046\n","Train Epoch: 157 [48000/50000 (96%)]\tLoss: 0.539530\n","Train Epoch: 158 [0/50000 (0%)]\tLoss: 0.889989\n","Train Epoch: 158 [8000/50000 (16%)]\tLoss: 0.978121\n","Train Epoch: 158 [16000/50000 (32%)]\tLoss: 0.641306\n","Train Epoch: 158 [24000/50000 (48%)]\tLoss: 0.560478\n","Train Epoch: 158 [32000/50000 (64%)]\tLoss: 1.052553\n","Train Epoch: 158 [40000/50000 (80%)]\tLoss: 0.790058\n","Train Epoch: 158 [48000/50000 (96%)]\tLoss: 0.729104\n","\n","Test set: Average loss: 0.0008, Accuracy: 7504/10000 (75%)\n","\n","Train Epoch: 159 [0/50000 (0%)]\tLoss: 0.641336\n","Train Epoch: 159 [8000/50000 (16%)]\tLoss: 0.993068\n","Train Epoch: 159 [16000/50000 (32%)]\tLoss: 1.100998\n","Train Epoch: 159 [24000/50000 (48%)]\tLoss: 0.933195\n","Train Epoch: 159 [32000/50000 (64%)]\tLoss: 0.674534\n","Train Epoch: 159 [40000/50000 (80%)]\tLoss: 0.478119\n","Train Epoch: 159 [48000/50000 (96%)]\tLoss: 0.423263\n","Train Epoch: 160 [0/50000 (0%)]\tLoss: 0.743001\n","Train Epoch: 160 [8000/50000 (16%)]\tLoss: 0.790806\n","Train Epoch: 160 [16000/50000 (32%)]\tLoss: 0.906737\n","Train Epoch: 160 [24000/50000 (48%)]\tLoss: 0.745595\n","Train Epoch: 160 [32000/50000 (64%)]\tLoss: 0.440990\n","Train Epoch: 160 [40000/50000 (80%)]\tLoss: 0.296812\n","Train Epoch: 160 [48000/50000 (96%)]\tLoss: 0.929508\n","\n","Train set: Average loss: 0.0423, Accuracy: 39009/50000 (78%)\n","\n","\n","Test set: Average loss: 0.0008, Accuracy: 7512/10000 (75%)\n","\n"]}],"source":["# # Arguments that do not affect model at all\n","train_args = {\n","    \"test_batch_size\": 1000, # input batch size for testing (default: 1000)\n","    'data': '../data', # Location to store data (e.g. MNIST)\n","    'log_interval': 500, # how many batches to wait before logging training status\n","    'train_eval_interval': 10, # epoch interval at which to print training accuracy\n","    'test_eval_interval': 2, # epoch interval at which to print test accuracy\n","    'eval_on_last': True\n","}\n","\n","args = {\n","  \"dataset\": \"CIFAR10\",\n","  \"init\": \"signed_constant\",\n","  \"batch_size\": 16, # input batch size for training (default: 64)\n","  \"epochs\": [80, 100, 160, 140, 120], # number of epochs to train (default: 14)\n","  \"optimizer\": \"SGD\",\n","  \"optim_kwargs\": {\"lr\": 0.1, \"momentum\": 0.9, \"weight_decay\": 0.0001},\n","  \"scheduler\": True, # False for Adam, True for SGD, does CosineAnnealing\n","  'no_cuda': False, # disables CUDA training\n","  'seed': 1000, # random seed (default: 1)\n","  'save_name': None, #\"conv2_frozen_sp50_rs1000\", # \"simple20_rs2\", # For Saving the current Model, None if not saving\n","  'sparsity': [{\"sparsity\": 0.5}, {\"sparsity\": 0.5}, {\"sparsity\": 0.5}, {\"sparsity\": 0.5}, {\"sparsity\": 0.5}], # 'how sparse is each layer'\n","  'copy_layers': [], # ['conv1', 'conv2', 'fc2'],\n","  'bias': False\n","}\n","\n","trained_model, device, train_loader, test_loader, criterion = main(args, train_args)\n","# # name_of_experiment = 'threshold-100epoch'\n","# # train_results = []\n","# # test_results = []\n","# # for rs in range(100, 105):\n","# #     args[\"seed\"] = rs\n","# #     trained_model, device, train_loader, test_loader, criterion = main(args, train_args)\n","# #     train_acc, train_loss = test(trained_model, device, criterion, train_loader, name=\"Train\")\n","# #     test_acc, test_loss = test(trained_model, device, criterion, test_loader)\n","# #     train_results.append((train_acc, train_loss))\n","# #     test_results.append((test_acc, test_loss))\n","# #     torch.save((train_args, args, train_results, test_results), \\\n","# #                os.path.join(os.environ[\"HOME_DIR\"], \"results\", f\"{name_of_experiment}_{args['dataset']}.pt\"))\n","\n","# # thresholds = [0.01, 0.02, 0.05, 0.1, 0.2, 0.3]\n","# # train_results = {x: [] for x in thresholds}\n","# # test_results = {x: [] for x in thresholds}\n","# # name_of_experiment = \"fc1_thresholds\"\n","    \n","# # for fc1_threshold in thresholds:\n","# #     for rs in range(70, 73):\n","# #         args[\"seed\"] = rs\n","# #         args['sparsity'][2]['threshold'] = fc1_threshold\n","# #         print(f\"----{rs}----{fc1_threshold}----\")\n","# #         print(args['sparsity'])\n","# #         # args[\"save_name\"] = f\"{args['dataset']}_{rs}_{name_of_experiment}_{fc1_threshold}.pt\"\n","# #         trained_model, device, train_loader, test_loader, criterion = main(args, train_args)\n","# #         train_acc, train_loss = test(trained_model, device, criterion, train_loader)\n","# #         test_acc, test_loss = test(trained_model, device, criterion, test_loader)\n","# #         train_results[fc1_threshold].append((train_acc, train_loss))\n","# #         test_results[fc1_threshold].append((test_acc, test_loss))\n","# #         torch.save((train_args, args, train_results, test_results), \\\n","# #                    os.path.join(os.environ[\"HOME_DIR\"], \"results\", f\"{args['dataset']}-{name_of_experiment}.pt\"))"]},{"cell_type":"code","source":["trained_model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a8INAneIZqsL","executionInfo":{"status":"ok","timestamp":1650951925384,"user_tz":240,"elapsed":208,"user":{"displayName":"Neehal Tumma","userId":"16200375369847542065"}},"outputId":"0dc28f9e-3b89-43a1-92b8-d8926bdd5d0c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Net(\n","  (conv1): SupermaskConv(3, 64, kernel_size=(3, 3), stride=(1, 1), bias=False, sparsity=0.5)\n","  (conv2): SupermaskConv(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False, sparsity=0.5)\n","  (fc1): SupermaskLinear(in_features=12544, out_features=256, bias=False, sparsity=0.5)\n","  (fc2): SupermaskLinear(in_features=256, out_features=256, bias=False, sparsity=0.5)\n","  (fc3): SupermaskLinear(in_features=256, out_features=10, bias=False, sparsity=0.5)\n",")"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"emsXZdACEd77"},"outputs":[],"source":["import pickle\n","def save_study(study, filename):\n","    with open(os.path.join(os.environ[\"HOME_DIR\"], \"results\", \"studies\", f\"{filename}.pickle\"), \"wb\") as f:\n","        pickle.dump(study, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AwL9v9H1qC89"},"outputs":[],"source":["def objective(trial):\n","    train_args = {\n","      \"test_batch_size\": 1000, # input batch size for testing (default: 1000)\n","      'data': '../data', # Location to store data (e.g. MNIST)\n","      'log_interval': 1000000, # how many batches to wait before logging training status\n","      'train_eval_interval': 20, # epoch interval at which to print training accuracy\n","      'test_eval_interval': 20, # epoch interval at which to print test accuracy\n","      'eval_on_last': True\n","    }\n","\n","    args = {\n","      \"dataset\": \"CIFAR10\",\n","      \"init\": \"signed_constant\",\n","      \"batch_size\": 64, # input batch size for training (default: 64)\n","      \"epochs\": [trial.suggest_int(f'layer{i}_epochs', 20, 160) for i in range(4)], # number of epochs to train (default: 14)\n","      \"optimizer\": \"SGD\",\n","      \"optim_kwargs\": {\"lr\": trial.suggest_float('learning_rate', 0.01, 0.5), \n","                       \"momentum\": trial.suggest_float('momentum', 0.2, 0.95),\n","                       \"weight_decay\": trial.suggest_float('weight_decay', 0.0001, 0.001)},\n","      \"scheduler\": True, # False for Adam, True for SGD, does CosineAnnealing\n","      'no_cuda': False, # disables CUDA training\n","      'seed': 500, # random seed (default: 1)\n","      'save_name': None, # \"simple20_rs2\", # For Saving the current Model, None if not saving\n","      'sparsity': [\n","                   {\"sparsity\": trial.suggest_float('sparsity_conv1', 0.1, 0.95)}, \n","                   {\"sparsity\": trial.suggest_float('sparsity_conv2', 0.1, 0.95)}, \n","                   {\"sparsity\": trial.suggest_float('sparsity_fc1', 0.1, 0.95)}, \n","                   {\"sparsity\": trial.suggest_float('sparsity_fc2', 0.1, 0.95)},\n","                   {\"sparsity\": trial.suggest_float('sparsity_fc3', 0.1, 0.95)}\n","                  ], # 'how sparse is each layer'\n","      'copy_layers': [], # ['conv1', 'conv2', 'fc2'],\n","      'bias': False\n","    }\n","\n","    print(args)\n","\n","    trained_model, device, train_loader, test_loader, criterion = main(args, train_args, trial=trial)\n","    train_acc, train_loss = test(trained_model, device, criterion, train_loader)\n","    test_acc, test_loss = test(trained_model, device, criterion, test_loader)\n","\n","    return test_acc"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["787c798bdade45408928547731210fc3","9126d4bb9bee4d79b8efca734994cd08","b6d159c7ba554393b1c0737dc36cde3c","9726a1602be546118dabcda8faaac562","4e3ed76b4fc448478a0e1e0c27423ac4"]},"id":"eqZ1h_q0sEiQ","outputId":"782e95aa-9bd5-4a65-9081-b8cdf3c9ca33"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/optuna/progress_bar.py:47: ExperimentalWarning: Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n","  self._init_valid()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"787c798bdade45408928547731210fc3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'dataset': 'CIFAR10', 'init': 'signed_constant', 'batch_size': 64, 'epochs': [156, 141, 100, 108], 'optimizer': 'SGD', 'optim_kwargs': {'lr': 0.45958742350989223, 'momentum': 0.20586289441261235, 'weight_decay': 0.000940641581032237}, 'scheduler': True, 'no_cuda': False, 'seed': 500, 'save_name': None, 'sparsity': [{'sparsity': 0.5765381764615303}, {'sparsity': 0.14134903253384878}, {'sparsity': 0.7820934128266148}, {'sparsity': 0.5602424184243817}, {'sparsity': 0.3638287789179808}], 'copy_layers': [], 'bias': False}\n","Using device cuda\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/CIFAR10/cifar-10-python.tar.gz\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9126d4bb9bee4d79b8efca734994cd08","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Extracting ../data/CIFAR10/cifar-10-python.tar.gz to ../data/CIFAR10\n","Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.297512\n","Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.794400\n","Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.429945\n","Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.641175\n","Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.339698\n","Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.306296\n","Train Epoch: 7 [0/50000 (0%)]\tLoss: 1.520341\n","Train Epoch: 8 [0/50000 (0%)]\tLoss: 1.545367\n","Train Epoch: 9 [0/50000 (0%)]\tLoss: 1.596636\n","Train Epoch: 10 [0/50000 (0%)]\tLoss: 1.455094\n","Train Epoch: 11 [0/50000 (0%)]\tLoss: 1.809127\n","Train Epoch: 12 [0/50000 (0%)]\tLoss: 1.676581\n","Train Epoch: 13 [0/50000 (0%)]\tLoss: 1.680508\n","Train Epoch: 14 [0/50000 (0%)]\tLoss: 1.952487\n","Train Epoch: 15 [0/50000 (0%)]\tLoss: 1.680164\n","Train Epoch: 16 [0/50000 (0%)]\tLoss: 1.603668\n","Train Epoch: 17 [0/50000 (0%)]\tLoss: 1.656784\n","Train Epoch: 18 [0/50000 (0%)]\tLoss: 1.632963\n","Train Epoch: 19 [0/50000 (0%)]\tLoss: 2.203250\n","Train Epoch: 20 [0/50000 (0%)]\tLoss: 1.767518\n","\n","Train set: Average loss: 0.0264, Accuracy: 20378/50000 (41%)\n","\n","\n","Test set: Average loss: 0.0017, Accuracy: 4010/10000 (40%)\n","\n","Train Epoch: 21 [0/50000 (0%)]\tLoss: 1.841320\n","Train Epoch: 22 [0/50000 (0%)]\tLoss: 1.821986\n","Train Epoch: 23 [0/50000 (0%)]\tLoss: 1.501218\n","Train Epoch: 24 [0/50000 (0%)]\tLoss: 1.562467\n","Train Epoch: 25 [0/50000 (0%)]\tLoss: 1.533144\n","Train Epoch: 26 [0/50000 (0%)]\tLoss: 1.893563\n","Train Epoch: 27 [0/50000 (0%)]\tLoss: 1.492726\n","Train Epoch: 28 [0/50000 (0%)]\tLoss: 1.367071\n","Train Epoch: 29 [0/50000 (0%)]\tLoss: 1.983648\n","Train Epoch: 30 [0/50000 (0%)]\tLoss: 1.664707\n","Train Epoch: 31 [0/50000 (0%)]\tLoss: 1.825306\n","Train Epoch: 32 [0/50000 (0%)]\tLoss: 1.967222\n","Train Epoch: 33 [0/50000 (0%)]\tLoss: 2.275820\n","Train Epoch: 34 [0/50000 (0%)]\tLoss: 2.316351\n","Train Epoch: 35 [0/50000 (0%)]\tLoss: 2.105458\n","Train Epoch: 36 [0/50000 (0%)]\tLoss: 1.371809\n","Train Epoch: 37 [0/50000 (0%)]\tLoss: 1.664478\n","Train Epoch: 38 [0/50000 (0%)]\tLoss: 1.875190\n","Train Epoch: 39 [0/50000 (0%)]\tLoss: 1.959209\n","Train Epoch: 40 [0/50000 (0%)]\tLoss: 2.111108\n","\n","Train set: Average loss: 0.0242, Accuracy: 23013/50000 (46%)\n","\n","\n","Test set: Average loss: 0.0015, Accuracy: 4583/10000 (46%)\n","\n","Train Epoch: 41 [0/50000 (0%)]\tLoss: 1.680761\n","Train Epoch: 42 [0/50000 (0%)]\tLoss: 1.940851\n","Train Epoch: 43 [0/50000 (0%)]\tLoss: 1.801904\n","Train Epoch: 44 [0/50000 (0%)]\tLoss: 1.489649\n","Train Epoch: 45 [0/50000 (0%)]\tLoss: 1.234804\n","Train Epoch: 46 [0/50000 (0%)]\tLoss: 2.590724\n","Train Epoch: 47 [0/50000 (0%)]\tLoss: 1.538090\n","Train Epoch: 48 [0/50000 (0%)]\tLoss: 1.860167\n","Train Epoch: 49 [0/50000 (0%)]\tLoss: 1.592990\n","Train Epoch: 50 [0/50000 (0%)]\tLoss: 1.971422\n","Train Epoch: 51 [0/50000 (0%)]\tLoss: 1.605131\n","Train Epoch: 52 [0/50000 (0%)]\tLoss: 1.658500\n","Train Epoch: 53 [0/50000 (0%)]\tLoss: 2.546557\n","Train Epoch: 54 [0/50000 (0%)]\tLoss: 2.127674\n","Train Epoch: 55 [0/50000 (0%)]\tLoss: 1.604292\n","Train Epoch: 56 [0/50000 (0%)]\tLoss: 1.739165\n","Train Epoch: 57 [0/50000 (0%)]\tLoss: 1.734806\n","Train Epoch: 58 [0/50000 (0%)]\tLoss: 1.467902\n","Train Epoch: 59 [0/50000 (0%)]\tLoss: 1.890577\n","Train Epoch: 60 [0/50000 (0%)]\tLoss: 1.883748\n","\n","Train set: Average loss: 0.0253, Accuracy: 21574/50000 (43%)\n","\n","\n","Test set: Average loss: 0.0016, Accuracy: 4227/10000 (42%)\n","\n","Train Epoch: 61 [0/50000 (0%)]\tLoss: 1.788283\n","Train Epoch: 62 [0/50000 (0%)]\tLoss: 1.624875\n","Train Epoch: 63 [0/50000 (0%)]\tLoss: 2.057827\n","Train Epoch: 64 [0/50000 (0%)]\tLoss: 1.544276\n","Train Epoch: 65 [0/50000 (0%)]\tLoss: 1.368075\n","Train Epoch: 66 [0/50000 (0%)]\tLoss: 1.748314\n","Train Epoch: 67 [0/50000 (0%)]\tLoss: 2.137279\n","Train Epoch: 68 [0/50000 (0%)]\tLoss: 2.064157\n","Train Epoch: 69 [0/50000 (0%)]\tLoss: 1.540134\n","Train Epoch: 70 [0/50000 (0%)]\tLoss: 1.708613\n","Train Epoch: 71 [0/50000 (0%)]\tLoss: 1.563475\n","Train Epoch: 72 [0/50000 (0%)]\tLoss: 1.352134\n","Train Epoch: 73 [0/50000 (0%)]\tLoss: 1.561311\n","Train Epoch: 74 [0/50000 (0%)]\tLoss: 1.489847\n","Train Epoch: 75 [0/50000 (0%)]\tLoss: 1.878883\n","Train Epoch: 76 [0/50000 (0%)]\tLoss: 1.541536\n","Train Epoch: 77 [0/50000 (0%)]\tLoss: 1.589916\n","Train Epoch: 78 [0/50000 (0%)]\tLoss: 1.261370\n","Train Epoch: 79 [0/50000 (0%)]\tLoss: 1.249206\n","Train Epoch: 80 [0/50000 (0%)]\tLoss: 1.346626\n","\n","Train set: Average loss: 0.0208, Accuracy: 26005/50000 (52%)\n","\n","\n","Test set: Average loss: 0.0013, Accuracy: 5148/10000 (51%)\n","\n","Train Epoch: 81 [0/50000 (0%)]\tLoss: 1.314746\n","Train Epoch: 82 [0/50000 (0%)]\tLoss: 1.696507\n","Train Epoch: 83 [0/50000 (0%)]\tLoss: 1.560482\n","Train Epoch: 84 [0/50000 (0%)]\tLoss: 1.615717\n","Train Epoch: 85 [0/50000 (0%)]\tLoss: 1.504316\n","Train Epoch: 86 [0/50000 (0%)]\tLoss: 1.256873\n","Train Epoch: 87 [0/50000 (0%)]\tLoss: 1.322315\n","Train Epoch: 88 [0/50000 (0%)]\tLoss: 1.610623\n","Train Epoch: 89 [0/50000 (0%)]\tLoss: 1.650536\n","Train Epoch: 90 [0/50000 (0%)]\tLoss: 1.382566\n","Train Epoch: 91 [0/50000 (0%)]\tLoss: 2.011896\n","Train Epoch: 92 [0/50000 (0%)]\tLoss: 1.685048\n","Train Epoch: 93 [0/50000 (0%)]\tLoss: 1.735274\n","Train Epoch: 94 [0/50000 (0%)]\tLoss: 1.285795\n","Train Epoch: 95 [0/50000 (0%)]\tLoss: 1.720833\n","Train Epoch: 96 [0/50000 (0%)]\tLoss: 2.065892\n","Train Epoch: 97 [0/50000 (0%)]\tLoss: 1.610905\n","Train Epoch: 98 [0/50000 (0%)]\tLoss: 1.703549\n","Train Epoch: 99 [0/50000 (0%)]\tLoss: 1.511770\n","Train Epoch: 100 [0/50000 (0%)]\tLoss: 1.519860\n","\n","Train set: Average loss: 0.0203, Accuracy: 26668/50000 (53%)\n","\n","\n","Test set: Average loss: 0.0013, Accuracy: 5347/10000 (53%)\n","\n","Freezing SupermaskLinear(in_features=12544, out_features=256, bias=False, sparsity=0.7820934128266148) before epoch 101\n","Train Epoch: 101 [0/50000 (0%)]\tLoss: 1.183268\n","Train Epoch: 102 [0/50000 (0%)]\tLoss: 1.207970\n","Train Epoch: 103 [0/50000 (0%)]\tLoss: 1.551011\n","Train Epoch: 104 [0/50000 (0%)]\tLoss: 1.262164\n","Train Epoch: 105 [0/50000 (0%)]\tLoss: 1.264479\n","Train Epoch: 106 [0/50000 (0%)]\tLoss: 1.072891\n","Train Epoch: 107 [0/50000 (0%)]\tLoss: 1.454060\n","Train Epoch: 108 [0/50000 (0%)]\tLoss: 1.310817\n","Freezing SupermaskLinear(in_features=256, out_features=256, bias=False, sparsity=0.5602424184243817) before epoch 109\n","Train Epoch: 109 [0/50000 (0%)]\tLoss: 1.218457\n","Train Epoch: 110 [0/50000 (0%)]\tLoss: 1.200411\n","Train Epoch: 111 [0/50000 (0%)]\tLoss: 1.196632\n","Train Epoch: 112 [0/50000 (0%)]\tLoss: 1.011822\n","Train Epoch: 113 [0/50000 (0%)]\tLoss: 1.223614\n","Train Epoch: 114 [0/50000 (0%)]\tLoss: 1.185890\n","Train Epoch: 115 [0/50000 (0%)]\tLoss: 1.109886\n","Train Epoch: 116 [0/50000 (0%)]\tLoss: 1.047855\n","Train Epoch: 117 [0/50000 (0%)]\tLoss: 1.169014\n","Train Epoch: 118 [0/50000 (0%)]\tLoss: 1.021177\n","Train Epoch: 119 [0/50000 (0%)]\tLoss: 1.016268\n","Train Epoch: 120 [0/50000 (0%)]\tLoss: 1.143282\n","\n","Train set: Average loss: 0.0175, Accuracy: 30368/50000 (61%)\n","\n","\n","Test set: Average loss: 0.0011, Accuracy: 6043/10000 (60%)\n","\n","Train Epoch: 121 [0/50000 (0%)]\tLoss: 1.223992\n","Train Epoch: 122 [0/50000 (0%)]\tLoss: 1.155367\n","Train Epoch: 123 [0/50000 (0%)]\tLoss: 1.188689\n","Train Epoch: 124 [0/50000 (0%)]\tLoss: 1.064704\n","Train Epoch: 125 [0/50000 (0%)]\tLoss: 1.188100\n","Train Epoch: 126 [0/50000 (0%)]\tLoss: 1.182463\n","Train Epoch: 127 [0/50000 (0%)]\tLoss: 1.269625\n","Train Epoch: 128 [0/50000 (0%)]\tLoss: 1.494988\n","Train Epoch: 129 [0/50000 (0%)]\tLoss: 1.087152\n","Train Epoch: 130 [0/50000 (0%)]\tLoss: 1.194083\n","Train Epoch: 131 [0/50000 (0%)]\tLoss: 1.315592\n","Train Epoch: 132 [0/50000 (0%)]\tLoss: 1.423036\n","Train Epoch: 133 [0/50000 (0%)]\tLoss: 1.147727\n","Train Epoch: 134 [0/50000 (0%)]\tLoss: 0.963829\n","Train Epoch: 135 [0/50000 (0%)]\tLoss: 1.211663\n","Train Epoch: 136 [0/50000 (0%)]\tLoss: 1.222166\n","Train Epoch: 137 [0/50000 (0%)]\tLoss: 1.079494\n","Train Epoch: 138 [0/50000 (0%)]\tLoss: 1.170103\n","Train Epoch: 139 [0/50000 (0%)]\tLoss: 1.061293\n","Train Epoch: 140 [0/50000 (0%)]\tLoss: 1.233133\n","\n","Train set: Average loss: 0.0177, Accuracy: 30184/50000 (60%)\n","\n","\n","Test set: Average loss: 0.0011, Accuracy: 5976/10000 (60%)\n","\n","Train Epoch: 141 [0/50000 (0%)]\tLoss: 1.210329\n","Freezing SupermaskConv(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False, sparsity=0.14134903253384878) before epoch 142\n","Train Epoch: 142 [0/50000 (0%)]\tLoss: 0.937906\n","Train Epoch: 143 [0/50000 (0%)]\tLoss: 1.248634\n","Train Epoch: 144 [0/50000 (0%)]\tLoss: 1.199986\n","Train Epoch: 145 [0/50000 (0%)]\tLoss: 1.088857\n","Train Epoch: 146 [0/50000 (0%)]\tLoss: 1.144492\n","Train Epoch: 147 [0/50000 (0%)]\tLoss: 1.138151\n","Train Epoch: 148 [0/50000 (0%)]\tLoss: 1.302780\n","Train Epoch: 149 [0/50000 (0%)]\tLoss: 1.313498\n","Train Epoch: 150 [0/50000 (0%)]\tLoss: 1.157605\n","Train Epoch: 151 [0/50000 (0%)]\tLoss: 1.338669\n","Train Epoch: 152 [0/50000 (0%)]\tLoss: 1.101166\n","Train Epoch: 153 [0/50000 (0%)]\tLoss: 1.219795\n","Train Epoch: 154 [0/50000 (0%)]\tLoss: 1.432228\n","Train Epoch: 155 [0/50000 (0%)]\tLoss: 1.156425\n","Train Epoch: 156 [0/50000 (0%)]\tLoss: 1.201089\n","\n","Test set: Average loss: 0.0172, Accuracy: 30699/50000 (61%)\n","\n","\n","Test set: Average loss: 0.0011, Accuracy: 6095/10000 (61%)\n","\n","\u001b[32m[I 2022-04-22 03:08:51,709]\u001b[0m Trial 4 finished with value: 60.95 and parameters: {'layer0_epochs': 156, 'layer1_epochs': 141, 'layer2_epochs': 100, 'layer3_epochs': 108, 'learning_rate': 0.45958742350989223, 'momentum': 0.20586289441261235, 'weight_decay': 0.000940641581032237, 'sparsity_conv1': 0.5765381764615303, 'sparsity_conv2': 0.14134903253384878, 'sparsity_fc1': 0.7820934128266148, 'sparsity_fc2': 0.5602424184243817, 'sparsity_fc3': 0.3638287789179808}. Best is trial 3 with value: 69.8.\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/optuna/progress_bar.py:47: ExperimentalWarning: Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n","  self._init_valid()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b6d159c7ba554393b1c0737dc36cde3c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'dataset': 'CIFAR10', 'init': 'signed_constant', 'batch_size': 64, 'epochs': [48, 153, 116, 77], 'optimizer': 'SGD', 'optim_kwargs': {'lr': 0.19392713319908203, 'momentum': 0.5440907568578053, 'weight_decay': 0.0008990447248441982}, 'scheduler': True, 'no_cuda': False, 'seed': 500, 'save_name': None, 'sparsity': [{'sparsity': 0.7083618112585519}, {'sparsity': 0.4177374344237944}, {'sparsity': 0.6901192267053294}, {'sparsity': 0.3375777103605044}, {'sparsity': 0.5422276725501263}], 'copy_layers': [], 'bias': False}\n","Using device cuda\n","Files already downloaded and verified\n","Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.301074\n","Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.752332\n","Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.514416\n","Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.631956\n","Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.186828\n","Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.264989\n","Train Epoch: 7 [0/50000 (0%)]\tLoss: 1.177410\n","Train Epoch: 8 [0/50000 (0%)]\tLoss: 1.333797\n","Train Epoch: 9 [0/50000 (0%)]\tLoss: 1.292844\n","Train Epoch: 10 [0/50000 (0%)]\tLoss: 1.371381\n","Train Epoch: 11 [0/50000 (0%)]\tLoss: 1.477640\n","Train Epoch: 12 [0/50000 (0%)]\tLoss: 1.348695\n","Train Epoch: 13 [0/50000 (0%)]\tLoss: 1.430687\n","Train Epoch: 14 [0/50000 (0%)]\tLoss: 1.653449\n","Train Epoch: 15 [0/50000 (0%)]\tLoss: 1.295452\n","Train Epoch: 16 [0/50000 (0%)]\tLoss: 1.404801\n","Train Epoch: 17 [0/50000 (0%)]\tLoss: 1.439297\n","Train Epoch: 18 [0/50000 (0%)]\tLoss: 1.578896\n","Train Epoch: 19 [0/50000 (0%)]\tLoss: 1.893691\n","Train Epoch: 20 [0/50000 (0%)]\tLoss: 1.523573\n","\n","Train set: Average loss: 0.0213, Accuracy: 26108/50000 (52%)\n","\n","\n","Test set: Average loss: 0.0014, Accuracy: 5166/10000 (52%)\n","\n","Train Epoch: 21 [0/50000 (0%)]\tLoss: 1.580761\n","Train Epoch: 22 [0/50000 (0%)]\tLoss: 1.810057\n","Train Epoch: 23 [0/50000 (0%)]\tLoss: 1.425588\n","Train Epoch: 24 [0/50000 (0%)]\tLoss: 1.510102\n","Train Epoch: 25 [0/50000 (0%)]\tLoss: 1.381990\n","Train Epoch: 26 [0/50000 (0%)]\tLoss: 1.605853\n","Train Epoch: 27 [0/50000 (0%)]\tLoss: 1.177055\n","Train Epoch: 28 [0/50000 (0%)]\tLoss: 1.131660\n","Train Epoch: 29 [0/50000 (0%)]\tLoss: 1.623092\n","Train Epoch: 30 [0/50000 (0%)]\tLoss: 1.478284\n","Train Epoch: 31 [0/50000 (0%)]\tLoss: 1.684744\n","Train Epoch: 32 [0/50000 (0%)]\tLoss: 1.711177\n","Train Epoch: 33 [0/50000 (0%)]\tLoss: 1.649089\n","Train Epoch: 34 [0/50000 (0%)]\tLoss: 1.875404\n","Train Epoch: 35 [0/50000 (0%)]\tLoss: 1.399593\n","Train Epoch: 36 [0/50000 (0%)]\tLoss: 1.452508\n","Train Epoch: 37 [0/50000 (0%)]\tLoss: 1.530769\n","Train Epoch: 38 [0/50000 (0%)]\tLoss: 1.614976\n","Train Epoch: 39 [0/50000 (0%)]\tLoss: 1.713918\n","Train Epoch: 40 [0/50000 (0%)]\tLoss: 1.374409\n","\n","Train set: Average loss: 0.0197, Accuracy: 27639/50000 (55%)\n","\n","\n","Test set: Average loss: 0.0013, Accuracy: 5527/10000 (55%)\n","\n","Train Epoch: 41 [0/50000 (0%)]\tLoss: 1.450788\n","Train Epoch: 42 [0/50000 (0%)]\tLoss: 1.792510\n","Train Epoch: 43 [0/50000 (0%)]\tLoss: 1.604294\n","Train Epoch: 44 [0/50000 (0%)]\tLoss: 1.411333\n","Train Epoch: 45 [0/50000 (0%)]\tLoss: 1.064271\n","Train Epoch: 46 [0/50000 (0%)]\tLoss: 1.737474\n","Train Epoch: 47 [0/50000 (0%)]\tLoss: 1.611804\n","Train Epoch: 48 [0/50000 (0%)]\tLoss: 1.325248\n","Freezing SupermaskConv(3, 64, kernel_size=(3, 3), stride=(1, 1), bias=False, sparsity=0.7083618112585519) before epoch 49\n","Train Epoch: 49 [0/50000 (0%)]\tLoss: 1.300936\n","Train Epoch: 50 [0/50000 (0%)]\tLoss: 1.585383\n","Train Epoch: 51 [0/50000 (0%)]\tLoss: 1.112294\n","Train Epoch: 52 [0/50000 (0%)]\tLoss: 1.246397\n","Train Epoch: 53 [0/50000 (0%)]\tLoss: 1.669974\n","Train Epoch: 54 [0/50000 (0%)]\tLoss: 1.442572\n","Train Epoch: 55 [0/50000 (0%)]\tLoss: 1.210852\n","Train Epoch: 56 [0/50000 (0%)]\tLoss: 1.335351\n","Train Epoch: 57 [0/50000 (0%)]\tLoss: 1.104168\n","Train Epoch: 58 [0/50000 (0%)]\tLoss: 1.172978\n","Train Epoch: 59 [0/50000 (0%)]\tLoss: 1.427296\n","Train Epoch: 60 [0/50000 (0%)]\tLoss: 1.346655\n","\n","Train set: Average loss: 0.0170, Accuracy: 30748/50000 (61%)\n","\n","\n","Test set: Average loss: 0.0011, Accuracy: 6076/10000 (61%)\n","\n","Train Epoch: 61 [0/50000 (0%)]\tLoss: 1.144488\n","Train Epoch: 62 [0/50000 (0%)]\tLoss: 1.128138\n","Train Epoch: 63 [0/50000 (0%)]\tLoss: 1.429991\n","Train Epoch: 64 [0/50000 (0%)]\tLoss: 1.168182\n","Train Epoch: 65 [0/50000 (0%)]\tLoss: 0.919960\n","Train Epoch: 66 [0/50000 (0%)]\tLoss: 1.360132\n","Train Epoch: 67 [0/50000 (0%)]\tLoss: 1.426334\n","Train Epoch: 68 [0/50000 (0%)]\tLoss: 1.293896\n","Train Epoch: 69 [0/50000 (0%)]\tLoss: 1.239964\n","Train Epoch: 70 [0/50000 (0%)]\tLoss: 1.179602\n","Train Epoch: 71 [0/50000 (0%)]\tLoss: 1.090768\n","Train Epoch: 72 [0/50000 (0%)]\tLoss: 1.226582\n","Train Epoch: 73 [0/50000 (0%)]\tLoss: 1.305037\n","Train Epoch: 74 [0/50000 (0%)]\tLoss: 0.993149\n","Train Epoch: 75 [0/50000 (0%)]\tLoss: 1.056433\n","Train Epoch: 76 [0/50000 (0%)]\tLoss: 1.050202\n","Train Epoch: 77 [0/50000 (0%)]\tLoss: 1.204022\n","Freezing SupermaskLinear(in_features=256, out_features=256, bias=False, sparsity=0.3375777103605044) before epoch 78\n","Train Epoch: 78 [0/50000 (0%)]\tLoss: 1.028835\n","Train Epoch: 79 [0/50000 (0%)]\tLoss: 1.166047\n","Train Epoch: 80 [0/50000 (0%)]\tLoss: 0.899997\n","\n","Train set: Average loss: 0.0163, Accuracy: 31899/50000 (64%)\n","\n","\n","Test set: Average loss: 0.0011, Accuracy: 6305/10000 (63%)\n","\n","Train Epoch: 81 [0/50000 (0%)]\tLoss: 1.017100\n","Train Epoch: 82 [0/50000 (0%)]\tLoss: 1.077022\n","Train Epoch: 83 [0/50000 (0%)]\tLoss: 0.971727\n","Train Epoch: 84 [0/50000 (0%)]\tLoss: 1.044644\n","Train Epoch: 85 [0/50000 (0%)]\tLoss: 1.074437\n","Train Epoch: 86 [0/50000 (0%)]\tLoss: 1.089304\n","Train Epoch: 87 [0/50000 (0%)]\tLoss: 0.797827\n","Train Epoch: 88 [0/50000 (0%)]\tLoss: 1.219804\n","Train Epoch: 89 [0/50000 (0%)]\tLoss: 1.181747\n","Train Epoch: 90 [0/50000 (0%)]\tLoss: 1.038315\n","Train Epoch: 91 [0/50000 (0%)]\tLoss: 1.078241\n","Train Epoch: 92 [0/50000 (0%)]\tLoss: 1.026448\n","Train Epoch: 93 [0/50000 (0%)]\tLoss: 1.120470\n","Train Epoch: 94 [0/50000 (0%)]\tLoss: 0.959318\n","Train Epoch: 95 [0/50000 (0%)]\tLoss: 1.023806\n","Train Epoch: 96 [0/50000 (0%)]\tLoss: 1.299743\n","Train Epoch: 97 [0/50000 (0%)]\tLoss: 1.360949\n","Train Epoch: 98 [0/50000 (0%)]\tLoss: 1.045522\n","Train Epoch: 99 [0/50000 (0%)]\tLoss: 1.002394\n","Train Epoch: 100 [0/50000 (0%)]\tLoss: 1.024817\n","\n","Train set: Average loss: 0.0152, Accuracy: 33124/50000 (66%)\n","\n","\n","Test set: Average loss: 0.0010, Accuracy: 6500/10000 (65%)\n","\n","Train Epoch: 101 [0/50000 (0%)]\tLoss: 0.928323\n","Train Epoch: 102 [0/50000 (0%)]\tLoss: 0.899348\n","Train Epoch: 103 [0/50000 (0%)]\tLoss: 1.248812\n","Train Epoch: 104 [0/50000 (0%)]\tLoss: 1.047138\n","Train Epoch: 105 [0/50000 (0%)]\tLoss: 1.100410\n","Train Epoch: 106 [0/50000 (0%)]\tLoss: 0.885915\n","Train Epoch: 107 [0/50000 (0%)]\tLoss: 1.080420\n","Train Epoch: 108 [0/50000 (0%)]\tLoss: 1.063087\n","Train Epoch: 109 [0/50000 (0%)]\tLoss: 1.048314\n","Train Epoch: 110 [0/50000 (0%)]\tLoss: 0.893132\n","Train Epoch: 111 [0/50000 (0%)]\tLoss: 1.022199\n","Train Epoch: 112 [0/50000 (0%)]\tLoss: 0.970646\n","Train Epoch: 113 [0/50000 (0%)]\tLoss: 0.976609\n","Train Epoch: 114 [0/50000 (0%)]\tLoss: 1.088709\n","Train Epoch: 115 [0/50000 (0%)]\tLoss: 0.962073\n","Train Epoch: 116 [0/50000 (0%)]\tLoss: 0.909688\n","Freezing SupermaskLinear(in_features=12544, out_features=256, bias=False, sparsity=0.6901192267053294) before epoch 117\n","Train Epoch: 117 [0/50000 (0%)]\tLoss: 1.040960\n","Train Epoch: 118 [0/50000 (0%)]\tLoss: 0.873010\n","Train Epoch: 119 [0/50000 (0%)]\tLoss: 0.789619\n","Train Epoch: 120 [0/50000 (0%)]\tLoss: 0.938750\n","\n","Train set: Average loss: 0.0139, Accuracy: 34906/50000 (70%)\n","\n","\n","Test set: Average loss: 0.0009, Accuracy: 6828/10000 (68%)\n","\n","Train Epoch: 121 [0/50000 (0%)]\tLoss: 1.092933\n","Train Epoch: 122 [0/50000 (0%)]\tLoss: 0.823968\n","Train Epoch: 123 [0/50000 (0%)]\tLoss: 0.779313\n","Train Epoch: 124 [0/50000 (0%)]\tLoss: 0.953024\n","Train Epoch: 125 [0/50000 (0%)]\tLoss: 0.978195\n","Train Epoch: 126 [0/50000 (0%)]\tLoss: 0.982115\n","Train Epoch: 127 [0/50000 (0%)]\tLoss: 1.117644\n","Train Epoch: 128 [0/50000 (0%)]\tLoss: 1.308196\n","Train Epoch: 129 [0/50000 (0%)]\tLoss: 0.844011\n","Train Epoch: 130 [0/50000 (0%)]\tLoss: 0.894943\n","Train Epoch: 131 [0/50000 (0%)]\tLoss: 1.045193\n","Train Epoch: 132 [0/50000 (0%)]\tLoss: 0.981876\n","Train Epoch: 133 [0/50000 (0%)]\tLoss: 1.028052\n","Train Epoch: 134 [0/50000 (0%)]\tLoss: 0.745869\n","Train Epoch: 135 [0/50000 (0%)]\tLoss: 0.928227\n","Train Epoch: 136 [0/50000 (0%)]\tLoss: 0.986409\n","Train Epoch: 137 [0/50000 (0%)]\tLoss: 0.810395\n","Train Epoch: 138 [0/50000 (0%)]\tLoss: 1.005689\n","Train Epoch: 139 [0/50000 (0%)]\tLoss: 0.847954\n","Train Epoch: 140 [0/50000 (0%)]\tLoss: 0.932662\n","\n","Train set: Average loss: 0.0140, Accuracy: 34797/50000 (70%)\n","\n","\n","Test set: Average loss: 0.0009, Accuracy: 6791/10000 (68%)\n","\n","Train Epoch: 141 [0/50000 (0%)]\tLoss: 0.834934\n","Train Epoch: 142 [0/50000 (0%)]\tLoss: 0.777007\n","Train Epoch: 143 [0/50000 (0%)]\tLoss: 1.099468\n","Train Epoch: 144 [0/50000 (0%)]\tLoss: 0.917983\n","Train Epoch: 145 [0/50000 (0%)]\tLoss: 0.828528\n","Train Epoch: 146 [0/50000 (0%)]\tLoss: 0.823353\n","Train Epoch: 147 [0/50000 (0%)]\tLoss: 0.816132\n","Train Epoch: 148 [0/50000 (0%)]\tLoss: 1.116055\n","Train Epoch: 149 [0/50000 (0%)]\tLoss: 1.104124\n","Train Epoch: 150 [0/50000 (0%)]\tLoss: 0.885838\n","Train Epoch: 151 [0/50000 (0%)]\tLoss: 1.005685\n","Train Epoch: 152 [0/50000 (0%)]\tLoss: 0.892703\n","Train Epoch: 153 [0/50000 (0%)]\tLoss: 0.997138\n","\n","Test set: Average loss: 0.0138, Accuracy: 35062/50000 (70%)\n","\n","\n","Test set: Average loss: 0.0009, Accuracy: 6831/10000 (68%)\n","\n","\u001b[32m[I 2022-04-22 03:59:46,869]\u001b[0m Trial 5 finished with value: 68.31 and parameters: {'layer0_epochs': 48, 'layer1_epochs': 153, 'layer2_epochs': 116, 'layer3_epochs': 77, 'learning_rate': 0.19392713319908203, 'momentum': 0.5440907568578053, 'weight_decay': 0.0008990447248441982, 'sparsity_conv1': 0.7083618112585519, 'sparsity_conv2': 0.4177374344237944, 'sparsity_fc1': 0.6901192267053294, 'sparsity_fc2': 0.3375777103605044, 'sparsity_fc3': 0.5422276725501263}. Best is trial 3 with value: 69.8.\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/optuna/progress_bar.py:47: ExperimentalWarning: Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n","  self._init_valid()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9726a1602be546118dabcda8faaac562","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'dataset': 'CIFAR10', 'init': 'signed_constant', 'batch_size': 64, 'epochs': [27, 40, 88, 119], 'optimizer': 'SGD', 'optim_kwargs': {'lr': 0.1399020037275351, 'momentum': 0.3888026279025666, 'weight_decay': 0.0005905809990740046}, 'scheduler': True, 'no_cuda': False, 'seed': 500, 'save_name': None, 'sparsity': [{'sparsity': 0.31454342404694857}, {'sparsity': 0.2072698755731019}, {'sparsity': 0.6154815646358571}, {'sparsity': 0.10747298097678944}, {'sparsity': 0.3127737631716025}], 'copy_layers': [], 'bias': False}\n","Using device cuda\n","Files already downloaded and verified\n","Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.301549\n","Train Epoch: 2 [0/50000 (0%)]\tLoss: 2.179573\n","Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.767686\n","Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.697245\n","Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.470590\n","Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.504838\n","Train Epoch: 7 [0/50000 (0%)]\tLoss: 1.393424\n","Train Epoch: 8 [0/50000 (0%)]\tLoss: 1.489616\n","Train Epoch: 9 [0/50000 (0%)]\tLoss: 1.434344\n","Train Epoch: 10 [0/50000 (0%)]\tLoss: 1.391860\n","Train Epoch: 11 [0/50000 (0%)]\tLoss: 1.378817\n","Train Epoch: 12 [0/50000 (0%)]\tLoss: 1.313257\n","Train Epoch: 13 [0/50000 (0%)]\tLoss: 1.249695\n","Train Epoch: 14 [0/50000 (0%)]\tLoss: 1.405400\n","Train Epoch: 15 [0/50000 (0%)]\tLoss: 1.306638\n","Train Epoch: 16 [0/50000 (0%)]\tLoss: 1.299477\n","Train Epoch: 17 [0/50000 (0%)]\tLoss: 1.331429\n","Train Epoch: 18 [0/50000 (0%)]\tLoss: 1.272717\n","Train Epoch: 19 [0/50000 (0%)]\tLoss: 1.551353\n","Train Epoch: 20 [0/50000 (0%)]\tLoss: 1.324026\n","\n","Train set: Average loss: 0.0194, Accuracy: 28289/50000 (57%)\n","\n","\n","Test set: Average loss: 0.0013, Accuracy: 5608/10000 (56%)\n","\n","Train Epoch: 21 [0/50000 (0%)]\tLoss: 1.344079\n","Train Epoch: 22 [0/50000 (0%)]\tLoss: 1.346261\n","Train Epoch: 23 [0/50000 (0%)]\tLoss: 1.167666\n","Train Epoch: 24 [0/50000 (0%)]\tLoss: 1.182687\n","Train Epoch: 25 [0/50000 (0%)]\tLoss: 1.115317\n","Train Epoch: 26 [0/50000 (0%)]\tLoss: 1.397199\n","Train Epoch: 27 [0/50000 (0%)]\tLoss: 1.089303\n","Freezing SupermaskConv(3, 64, kernel_size=(3, 3), stride=(1, 1), bias=False, sparsity=0.31454342404694857) before epoch 28\n","Train Epoch: 28 [0/50000 (0%)]\tLoss: 1.149110\n","Train Epoch: 29 [0/50000 (0%)]\tLoss: 1.052203\n","Train Epoch: 30 [0/50000 (0%)]\tLoss: 1.128277\n","Train Epoch: 31 [0/50000 (0%)]\tLoss: 0.981286\n","Train Epoch: 32 [0/50000 (0%)]\tLoss: 1.256582\n","Train Epoch: 33 [0/50000 (0%)]\tLoss: 1.505122\n","Train Epoch: 34 [0/50000 (0%)]\tLoss: 1.517303\n","Train Epoch: 35 [0/50000 (0%)]\tLoss: 1.139302\n","Train Epoch: 36 [0/50000 (0%)]\tLoss: 0.918892\n","Train Epoch: 37 [0/50000 (0%)]\tLoss: 1.275290\n","Train Epoch: 38 [0/50000 (0%)]\tLoss: 1.408462\n","Train Epoch: 39 [0/50000 (0%)]\tLoss: 1.297021\n","Train Epoch: 40 [0/50000 (0%)]\tLoss: 1.142547\n","\n","Train set: Average loss: 0.0171, Accuracy: 30611/50000 (61%)\n","\n","\n","Test set: Average loss: 0.0011, Accuracy: 6033/10000 (60%)\n","\n","Freezing SupermaskConv(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False, sparsity=0.2072698755731019) before epoch 41\n","Train Epoch: 41 [0/50000 (0%)]\tLoss: 1.228947\n","Train Epoch: 42 [0/50000 (0%)]\tLoss: 1.205751\n","Train Epoch: 43 [0/50000 (0%)]\tLoss: 1.186335\n","Train Epoch: 44 [0/50000 (0%)]\tLoss: 1.164241\n","Train Epoch: 45 [0/50000 (0%)]\tLoss: 0.879536\n","Train Epoch: 46 [0/50000 (0%)]\tLoss: 1.181566\n","Train Epoch: 47 [0/50000 (0%)]\tLoss: 0.966606\n","Train Epoch: 48 [0/50000 (0%)]\tLoss: 1.211899\n","Train Epoch: 49 [0/50000 (0%)]\tLoss: 1.174230\n","Train Epoch: 50 [0/50000 (0%)]\tLoss: 1.434802\n","Train Epoch: 51 [0/50000 (0%)]\tLoss: 1.092316\n","Train Epoch: 52 [0/50000 (0%)]\tLoss: 1.049908\n","Train Epoch: 53 [0/50000 (0%)]\tLoss: 1.258071\n","Train Epoch: 54 [0/50000 (0%)]\tLoss: 1.217751\n","Train Epoch: 55 [0/50000 (0%)]\tLoss: 1.153007\n","Train Epoch: 56 [0/50000 (0%)]\tLoss: 1.178105\n","Train Epoch: 57 [0/50000 (0%)]\tLoss: 0.976847\n","Train Epoch: 58 [0/50000 (0%)]\tLoss: 1.032636\n","Train Epoch: 59 [0/50000 (0%)]\tLoss: 1.255207\n","Train Epoch: 60 [0/50000 (0%)]\tLoss: 1.376727\n","\n","Train set: Average loss: 0.0172, Accuracy: 30715/50000 (61%)\n","\n","\n","Test set: Average loss: 0.0011, Accuracy: 6027/10000 (60%)\n","\n","Train Epoch: 61 [0/50000 (0%)]\tLoss: 1.292037\n","Train Epoch: 62 [0/50000 (0%)]\tLoss: 1.089778\n","Train Epoch: 63 [0/50000 (0%)]\tLoss: 1.235256\n","Train Epoch: 64 [0/50000 (0%)]\tLoss: 1.178086\n","Train Epoch: 65 [0/50000 (0%)]\tLoss: 0.886657\n","Train Epoch: 66 [0/50000 (0%)]\tLoss: 1.222075\n","Train Epoch: 67 [0/50000 (0%)]\tLoss: 1.247460\n","Train Epoch: 68 [0/50000 (0%)]\tLoss: 1.252803\n","Train Epoch: 69 [0/50000 (0%)]\tLoss: 1.098384\n","Train Epoch: 70 [0/50000 (0%)]\tLoss: 1.211016\n","Train Epoch: 71 [0/50000 (0%)]\tLoss: 0.991071\n","Train Epoch: 72 [0/50000 (0%)]\tLoss: 1.252593\n","Train Epoch: 73 [0/50000 (0%)]\tLoss: 1.168339\n","Train Epoch: 74 [0/50000 (0%)]\tLoss: 1.026817\n","Train Epoch: 75 [0/50000 (0%)]\tLoss: 0.978165\n","Train Epoch: 76 [0/50000 (0%)]\tLoss: 0.907290\n","Train Epoch: 77 [0/50000 (0%)]\tLoss: 1.247657\n","Train Epoch: 78 [0/50000 (0%)]\tLoss: 0.995801\n","Train Epoch: 79 [0/50000 (0%)]\tLoss: 1.054917\n","Train Epoch: 80 [0/50000 (0%)]\tLoss: 0.992763\n","\n","Train set: Average loss: 0.0168, Accuracy: 31284/50000 (63%)\n","\n","\n","Test set: Average loss: 0.0011, Accuracy: 6188/10000 (62%)\n","\n","Train Epoch: 81 [0/50000 (0%)]\tLoss: 1.082426\n","Train Epoch: 82 [0/50000 (0%)]\tLoss: 1.075018\n","Train Epoch: 83 [0/50000 (0%)]\tLoss: 0.991345\n","Train Epoch: 84 [0/50000 (0%)]\tLoss: 0.959021\n","Train Epoch: 85 [0/50000 (0%)]\tLoss: 1.026144\n","Train Epoch: 86 [0/50000 (0%)]\tLoss: 0.968414\n","Train Epoch: 87 [0/50000 (0%)]\tLoss: 0.851097\n","Train Epoch: 88 [0/50000 (0%)]\tLoss: 1.055066\n","Freezing SupermaskLinear(in_features=12544, out_features=256, bias=False, sparsity=0.6154815646358571) before epoch 89\n","Train Epoch: 89 [0/50000 (0%)]\tLoss: 1.284181\n","Train Epoch: 90 [0/50000 (0%)]\tLoss: 1.059777\n","Train Epoch: 91 [0/50000 (0%)]\tLoss: 1.037001\n","Train Epoch: 92 [0/50000 (0%)]\tLoss: 1.038233\n","Train Epoch: 93 [0/50000 (0%)]\tLoss: 1.050584\n","Train Epoch: 94 [0/50000 (0%)]\tLoss: 0.974674\n","Train Epoch: 95 [0/50000 (0%)]\tLoss: 0.915213\n","Train Epoch: 96 [0/50000 (0%)]\tLoss: 1.094688\n","Train Epoch: 97 [0/50000 (0%)]\tLoss: 1.212019\n","Train Epoch: 98 [0/50000 (0%)]\tLoss: 0.986878\n","Train Epoch: 99 [0/50000 (0%)]\tLoss: 0.914997\n","Train Epoch: 100 [0/50000 (0%)]\tLoss: 1.065694\n","\n","Train set: Average loss: 0.0158, Accuracy: 32691/50000 (65%)\n","\n","\n","Test set: Average loss: 0.0010, Accuracy: 6458/10000 (65%)\n","\n","Train Epoch: 101 [0/50000 (0%)]\tLoss: 0.898912\n","Train Epoch: 102 [0/50000 (0%)]\tLoss: 1.043938\n","Train Epoch: 103 [0/50000 (0%)]\tLoss: 1.118098\n","Train Epoch: 104 [0/50000 (0%)]\tLoss: 1.150543\n","Train Epoch: 105 [0/50000 (0%)]\tLoss: 1.153494\n","Train Epoch: 106 [0/50000 (0%)]\tLoss: 0.853021\n","Train Epoch: 107 [0/50000 (0%)]\tLoss: 1.094841\n","Train Epoch: 108 [0/50000 (0%)]\tLoss: 1.042365\n","Train Epoch: 109 [0/50000 (0%)]\tLoss: 1.040938\n","Train Epoch: 110 [0/50000 (0%)]\tLoss: 0.936782\n","Train Epoch: 111 [0/50000 (0%)]\tLoss: 1.130017\n","Train Epoch: 112 [0/50000 (0%)]\tLoss: 0.877850\n","Train Epoch: 113 [0/50000 (0%)]\tLoss: 1.036686\n","Train Epoch: 114 [0/50000 (0%)]\tLoss: 1.056910\n","Train Epoch: 115 [0/50000 (0%)]\tLoss: 1.028018\n","Train Epoch: 116 [0/50000 (0%)]\tLoss: 0.937442\n","Train Epoch: 117 [0/50000 (0%)]\tLoss: 1.030117\n","Train Epoch: 118 [0/50000 (0%)]\tLoss: 0.909310\n","Train Epoch: 119 [0/50000 (0%)]\tLoss: 0.926014\n","\n","Test set: Average loss: 0.0157, Accuracy: 32841/50000 (66%)\n","\n","\n","Test set: Average loss: 0.0010, Accuracy: 6468/10000 (65%)\n","\n","\u001b[32m[I 2022-04-22 04:39:21,003]\u001b[0m Trial 6 finished with value: 64.68 and parameters: {'layer0_epochs': 27, 'layer1_epochs': 40, 'layer2_epochs': 88, 'layer3_epochs': 119, 'learning_rate': 0.1399020037275351, 'momentum': 0.3888026279025666, 'weight_decay': 0.0005905809990740046, 'sparsity_conv1': 0.31454342404694857, 'sparsity_conv2': 0.2072698755731019, 'sparsity_fc1': 0.6154815646358571, 'sparsity_fc2': 0.10747298097678944, 'sparsity_fc3': 0.3127737631716025}. Best is trial 3 with value: 69.8.\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/optuna/progress_bar.py:47: ExperimentalWarning: Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n","  self._init_valid()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4e3ed76b4fc448478a0e1e0c27423ac4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'dataset': 'CIFAR10', 'init': 'signed_constant', 'batch_size': 64, 'epochs': [109, 120, 39, 107], 'optimizer': 'SGD', 'optim_kwargs': {'lr': 0.396271070297724, 'momentum': 0.20034597630563944, 'weight_decay': 0.0004426238750702677}, 'scheduler': True, 'no_cuda': False, 'seed': 500, 'save_name': None, 'sparsity': [{'sparsity': 0.525614171145173}, {'sparsity': 0.3487365468694312}, {'sparsity': 0.9313112995497825}, {'sparsity': 0.25504685488449785}, {'sparsity': 0.6010673282663183}], 'copy_layers': [], 'bias': False}\n","Using device cuda\n","Files already downloaded and verified\n","Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.304213\n","Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.783911\n","Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.484774\n","Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.751179\n","Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.370910\n","Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.376318\n","Train Epoch: 7 [0/50000 (0%)]\tLoss: 1.468095\n","Train Epoch: 8 [0/50000 (0%)]\tLoss: 1.534206\n","Train Epoch: 9 [0/50000 (0%)]\tLoss: 1.557525\n","Train Epoch: 10 [0/50000 (0%)]\tLoss: 1.498185\n","Train Epoch: 11 [0/50000 (0%)]\tLoss: 1.586655\n","Train Epoch: 12 [0/50000 (0%)]\tLoss: 1.534650\n","Train Epoch: 13 [0/50000 (0%)]\tLoss: 1.486599\n","Train Epoch: 14 [0/50000 (0%)]\tLoss: 1.636832\n","Train Epoch: 15 [0/50000 (0%)]\tLoss: 1.614045\n","Train Epoch: 16 [0/50000 (0%)]\tLoss: 1.525096\n","Train Epoch: 17 [0/50000 (0%)]\tLoss: 1.592622\n","Train Epoch: 18 [0/50000 (0%)]\tLoss: 1.600166\n","Train Epoch: 19 [0/50000 (0%)]\tLoss: 1.747072\n","Train Epoch: 20 [0/50000 (0%)]\tLoss: 1.564049\n","\n","Train set: Average loss: 0.0253, Accuracy: 20503/50000 (41%)\n","\n","\n","Test set: Average loss: 0.0016, Accuracy: 4080/10000 (41%)\n","\n","Train Epoch: 21 [0/50000 (0%)]\tLoss: 1.782515\n"]}],"source":["# with open(os.path.join(os.environ[\"HOME_DIR\"], \"results\", \"studies\", \"hp_search_study.pickle\"), \"rb\") as f:\n","#     study = pickle.load(f)\n","study = optuna.create_study(direction='maximize', \n","                            pruner=optuna.pruners.PatientPruner(\n","                                optuna.pruners.MedianPruner(n_startup_trials=15, n_warmup_steps=50, interval_steps=1), \n","                                patience=40\n","                                )\n","                            )\n","for _ in range(100):\n","    study.optimize(objective, n_trials=1, show_progress_bar=True)\n","    save_study(study, \"conv2_augmented_search_rs_500\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iomeoIgeQFW9"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Indirectly Penalizing with Output Scores.ipynb","provenance":[{"file_id":"1LSeUG2USkO0Fi07cYGgH1Cn1JGSdFaJq","timestamp":1650929229167},{"file_id":"1ulel-J7dj6zXXH2paZ5pVQbiScvlbgKQ","timestamp":1650606359020}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"58fd0f238bcf4cb2b676ae599011d870":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_89eec571be8b4cb0846dc43a9ca478d3","IPY_MODEL_a30c3c6950874ab394886c3978652a41","IPY_MODEL_6957175891a54cea9db514da790853ed"],"layout":"IPY_MODEL_5a0feea754854790a0d9f0f9275ef8bf"}},"89eec571be8b4cb0846dc43a9ca478d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1958de9206a34a88bc082841ded5c62f","placeholder":"","style":"IPY_MODEL_07197f2ea89e488593265cc5300a059d","value":""}},"a30c3c6950874ab394886c3978652a41":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_566818e348504dcbbc955f570d09b788","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_02d1f9177ee24e0a9948557c2ac08220","value":170498071}},"6957175891a54cea9db514da790853ed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a6c40bf2ba5446684a407f121a50284","placeholder":"","style":"IPY_MODEL_96dd599b5cdd409991e25453ab7c14ac","value":" 170499072/? [00:03&lt;00:00, 51540519.23it/s]"}},"5a0feea754854790a0d9f0f9275ef8bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1958de9206a34a88bc082841ded5c62f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07197f2ea89e488593265cc5300a059d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"566818e348504dcbbc955f570d09b788":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02d1f9177ee24e0a9948557c2ac08220":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8a6c40bf2ba5446684a407f121a50284":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96dd599b5cdd409991e25453ab7c14ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ec8df6f527248dc8dda33e1902a5b86":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c983ab896b7a44799ecb4b09bdbd2783","IPY_MODEL_69495c28adf843f98c390cbd6fb772c8","IPY_MODEL_17604f4770014a7f8e6f6c00ffe5b023"],"layout":"IPY_MODEL_a40f7009acb04e7d9a537307190fae5d"}},"c983ab896b7a44799ecb4b09bdbd2783":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98888f1c11c342e0938f832893b7421f","placeholder":"","style":"IPY_MODEL_4fd2b913a63f4c4baf9e9f86c115c804","value":""}},"69495c28adf843f98c390cbd6fb772c8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8afc44401aa4fb6a8d08e221ae9cf94","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a464caaf80f64b70b3c610a3c59ec479","value":170498071}},"17604f4770014a7f8e6f6c00ffe5b023":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c10738d574c94da186b76bd925b4c83c","placeholder":"","style":"IPY_MODEL_38b2adb324394ad49280fe25a5b2f644","value":" 170499072/? [00:05&lt;00:00, 31013685.43it/s]"}},"a40f7009acb04e7d9a537307190fae5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98888f1c11c342e0938f832893b7421f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fd2b913a63f4c4baf9e9f86c115c804":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8afc44401aa4fb6a8d08e221ae9cf94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a464caaf80f64b70b3c610a3c59ec479":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c10738d574c94da186b76bd925b4c83c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38b2adb324394ad49280fe25a5b2f644":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e0fcd12e3c7406fab5c94e8e16fe3f1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1f41d16d87ad44b19380725263868741","IPY_MODEL_e66d728c4ab44177a28422d4cfdb950f","IPY_MODEL_004074ae9b8441ad9242238e7c6fa9e7"],"layout":"IPY_MODEL_801e604b329740049688bb77041d98ca"}},"1f41d16d87ad44b19380725263868741":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_05bebc7b6cc444d99ee2976c530f0b39","placeholder":"","style":"IPY_MODEL_83b6a3f0d5a045e4a4606d6fff88984f","value":""}},"e66d728c4ab44177a28422d4cfdb950f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_40326802d5074a04b106b71e7d5bd956","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_22460777b3024231ac41759fa001e8b5","value":170498071}},"004074ae9b8441ad9242238e7c6fa9e7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0bcbbbe19f141dfb4ffa59bb397216b","placeholder":"","style":"IPY_MODEL_41bdb3a5882f4a40b0268b594cee3127","value":" 170499072/? [00:10&lt;00:00, 16997873.62it/s]"}},"801e604b329740049688bb77041d98ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05bebc7b6cc444d99ee2976c530f0b39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83b6a3f0d5a045e4a4606d6fff88984f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40326802d5074a04b106b71e7d5bd956":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22460777b3024231ac41759fa001e8b5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e0bcbbbe19f141dfb4ffa59bb397216b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41bdb3a5882f4a40b0268b594cee3127":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}